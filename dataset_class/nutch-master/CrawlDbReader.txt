Location: CrawlDbReader.java

Content: 

public CrawlDatum get(String crawlDb,String url,Configuration config) throws IOException {

  Text key=new Text(url);

  CrawlDatum val=new CrawlDatum();

  openReaders(crawlDb,config);

  CrawlDatum res=(CrawlDatum)MapFileOutputFormat.getEntry(readers,new HashPartitioner<>(),key,val);

  return res;

}

Location: CrawlDbReader.java

Content: 

private void openReaders(String crawlDb,Configuration config) throws IOException {

  Path crawlDbPath=new Path(crawlDb,CrawlDb.CURRENT_NAME);

  FileStatus stat=crawlDbPath.getFileSystem(config).getFileStatus(crawlDbPath);

  long lastModified=stat.getModificationTime();

synchronized (this) {

    if (readers != null) {

      if (this.lastModified == lastModified) {

        return;

      }

 else {

        hongshuai();

        if (readers == null)         return;

        for (int i=0; i < readers.length; i++) {

          try {

            readers[i].close();

          }

 catch (          Exception e) {

          }

        }

        readers=null;

      }

    }

    this.lastModified=lastModified;

    readers=MapFileOutputFormat.getReaders(crawlDbPath,config);

  }

}

Location: CrawlDbReader.java

Content: 

public void processDumpJob(String crawlDb,String output,Configuration config,String format,String regex,String status,Integer retry,String expr,Float sample) throws IOException, ClassNotFoundException, InterruptedException {

  LOG.info("CrawlDb dump: starting");

  LOG.info("CrawlDb db: {}",crawlDb);

  Path outFolder=new Path(output);

  Job job=NutchJob.getInstance(config);

  job.setJobName("dump " + crawlDb);

  Configuration jobConf=job.getConfiguration();

  FileInputFormat.addInputPath(job,new Path(crawlDb,CrawlDb.CURRENT_NAME));

  job.setInputFormatClass(SequenceFileInputFormat.class);

  FileOutputFormat.setOutputPath(job,outFolder);

  if (format.equals("csv")) {

    job.setOutputFormatClass(CrawlDatumCsvOutputFormat.class);

  }

 else   if (format.equals("crawldb")) {

    job.setOutputFormatClass(MapFileOutputFormat.class);

  }

 else   if (format.equals("json")) {

    job.setOutputFormatClass(CrawlDatumJsonOutputFormat.class);

  }

 else {

    job.setOutputFormatClass(TextOutputFormat.class);

  }

  if (status != null)   jobConf.set("status",status);

  if (regex != null)   jobConf.set("regex",regex);

  if (retry != null)   jobConf.setInt("retry",retry);

  if (expr != null) {

    jobConf.set("expr",expr);

    LOG.info("CrawlDb db: expr: {}",expr);

  }

  if (sample != null) {

    jobConf.setFloat("sample",sample);

  }

  job.setMapperClass(CrawlDbDumpMapper.class);

  job.setOutputKeyClass(Text.class);

  job.setOutputValueClass(CrawlDatum.class);

  job.setJarByClass(CrawlDbReader.class);

  try {

    boolean success=job.waitForCompletion(true);

    if (!success) {

      String message="CrawlDbReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: "+ job.getStatus().getFailureInfo();

      LOG.error(message);

      throw new RuntimeException(message);

    }

  }

 catch (  IOException|InterruptedException|ClassNotFoundException e) {

    LOG.error(StringUtils.stringifyException(e));

    throw e;

  }

  LOG.info("CrawlDb dump: done");

}

Location: CrawlDbReader.java

Content: 

private TreeMap<String,Writable> processStatJobHelper(String crawlDb,Configuration config,boolean sort) throws IOException, InterruptedException, ClassNotFoundException {

  Path tmpFolder=new Path(crawlDb,"stat_tmp" + System.currentTimeMillis());

  Job job=NutchJob.getInstance(config);

  config=job.getConfiguration();

  job.setJobName("stats " + crawlDb);

  config.setBoolean("db.reader.stats.sort",sort);

  FileInputFormat.addInputPath(job,new Path(crawlDb,CrawlDb.CURRENT_NAME));

  job.setInputFormatClass(SequenceFileInputFormat.class);

  job.setJarByClass(CrawlDbReader.class);

  job.setMapperClass(CrawlDbStatMapper.class);

  job.setCombinerClass(CrawlDbStatReducer.class);

  job.setReducerClass(CrawlDbStatReducer.class);

  FileOutputFormat.setOutputPath(job,tmpFolder);

  job.setOutputFormatClass(SequenceFileOutputFormat.class);

  job.setOutputKeyClass(Text.class);

  job.setOutputValueClass(NutchWritable.class);

  config.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs",false);

  FileSystem fileSystem=tmpFolder.getFileSystem(config);

  try {

    boolean success=job.waitForCompletion(true);

    if (!success) {

      String message="CrawlDbReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: "+ job.getStatus().getFailureInfo();

      LOG.error(message);

      fileSystem.delete(tmpFolder,true);

      throw new RuntimeException(message);

    }

  }

 catch (  IOException|InterruptedException|ClassNotFoundException e) {

    LOG.error(StringUtils.stringifyException(e));

    fileSystem.delete(tmpFolder,true);

    throw e;

  }

  SequenceFile.Reader[] readers=SegmentReaderUtil.getReaders(tmpFolder,config);

  Text key=new Text();

  NutchWritable value=new NutchWritable();

  TreeMap<String,Writable> stats=new TreeMap<>();

  for (int i=0; i < readers.length; i++) {

    SequenceFile.Reader reader=readers[i];

    while (reader.next(key,value)) {

      String k=key.toString();

      Writable val=stats.get(k);

      if (val == null) {

        stats.put(k,value.get());

        continue;

      }

      if (k.equals("sc")) {

        float min=Float.MAX_VALUE;

        float max=Float.MIN_VALUE;

        if (stats.containsKey("scn")) {

          min=((FloatWritable)stats.get("scn")).get();

        }

 else {

          min=((FloatWritable)stats.get("sc")).get();

        }

        if (stats.containsKey("scx")) {

          max=((FloatWritable)stats.get("scx")).get();

        }

 else {

          max=((FloatWritable)stats.get("sc")).get();

        }

        float fvalue=((FloatWritable)value.get()).get();

        if (min > fvalue) {

          min=fvalue;

        }

        if (max < fvalue) {

          max=fvalue;

        }

        stats.put("scn",new FloatWritable(min));

        stats.put("scx",new FloatWritable(max));

      }

 else       if (k.equals("ft") || k.equals("fi")) {

        long min=Long.MAX_VALUE;

        long max=Long.MIN_VALUE;

        String minKey=k + "n";

        String maxKey=k + "x";

        if (stats.containsKey(minKey)) {

          min=((LongWritable)stats.get(minKey)).get();

        }

 else         if (stats.containsKey(k)) {

          min=((LongWritable)stats.get(k)).get();

        }

        if (stats.containsKey(maxKey)) {

          max=((LongWritable)stats.get(maxKey)).get();

        }

 else         if (stats.containsKey(k)) {

          max=((LongWritable)stats.get(k)).get();

        }

        long lvalue=((LongWritable)value.get()).get();

        if (min > lvalue) {

          min=lvalue;

        }

        if (max < lvalue) {

          max=lvalue;

        }

        stats.put(k + "n",new LongWritable(min));

        stats.put(k + "x",new LongWritable(max));

      }

 else       if (k.equals("sct")) {

        FloatWritable fvalue=(FloatWritable)value.get();

        ((FloatWritable)val).set(((FloatWritable)val).get() + fvalue.get());

      }

 else       if (k.equals("scd")) {

        MergingDigest tdigest=null;

        MergingDigest tdig=MergingDigest.fromBytes(ByteBuffer.wrap(((BytesWritable)value.get()).getBytes()));

        if (val instanceof BytesWritable) {

          tdigest=MergingDigest.fromBytes(ByteBuffer.wrap(((BytesWritable)val).getBytes()));

          tdigest.add(tdig);

        }

 else {

          tdigest=tdig;

        }

        ByteBuffer tdigestBytes=ByteBuffer.allocate(tdigest.smallByteSize());

        tdigest.asSmallBytes(tdigestBytes);

        stats.put(k,new BytesWritable(tdigestBytes.array()));

      }

 else {

        LongWritable lvalue=(LongWritable)value.get();

        ((LongWritable)val).set(((LongWritable)val).get() + lvalue.get());

      }

    }

    reader.close();

  }

  stats.remove("sc");

  stats.remove("fi");

  stats.remove("ft");

  fileSystem.delete(tmpFolder,true);

  return stats;

}

Location: CrawlDbReader.java

Content: 

public void processStatJob(String crawlDb,Configuration config,boolean sort) throws IOException, InterruptedException, ClassNotFoundException {

  double quantiles[]={.01,.05,.1,.2,.25,.3,.4,.5,.6,.7,.75,.8,.9,.95,.99};

  if (config.get("db.stats.score.quantiles") != null) {

    List<Double> qs=new ArrayList<>();

    for (    String s : config.getStrings("db.stats.score.quantiles")) {

      try {

        double d=Double.parseDouble(s);

        if (d >= 0.0 && d <= 1.0) {

          qs.add(d);

        }

 else {

          LOG.warn("Skipping quantile {} not in range in db.stats.score.quantiles",s);

        }

      }

 catch (      NumberFormatException e) {

        LOG.warn("Skipping bad floating point number {} in db.stats.score.quantiles: {}",s,e.getMessage());

      }

      quantiles=new double[qs.size()];

      int i=0;

      for (      Double q : qs) {

        quantiles[i++]=q;

      }

      Arrays.sort(quantiles);

    }

  }

  LOG.info("CrawlDb statistics start: {}",crawlDb);

  TreeMap<String,Writable> stats=processStatJobHelper(crawlDb,config,sort);

  if (LOG.isInfoEnabled()) {

    LOG.info("Statistics for CrawlDb: {}",crawlDb);

    LongWritable totalCnt=new LongWritable(0);

    if (stats.containsKey("T")) {

      totalCnt=((LongWritable)stats.get("T"));

      stats.remove("T");

    }

    LOG.info("TOTAL urls:\t" + totalCnt.get());

    for (    Map.Entry<String,Writable> entry : stats.entrySet()) {

      String k=entry.getKey();

      long value=0;

      double fvalue=0.0;

      byte[] bytesValue=null;

      Writable val=entry.getValue();

      if (val instanceof LongWritable) {

        value=((LongWritable)val).get();

      }

 else       if (val instanceof FloatWritable) {

        fvalue=((FloatWritable)val).get();

      }

 else       if (val instanceof BytesWritable) {

        bytesValue=((BytesWritable)val).getBytes();

      }

      if (k.equals("scn")) {

        LOG.info("min score:\t" + fvalue);

      }

 else       if (k.equals("scx")) {

        LOG.info("max score:\t" + fvalue);

      }

 else       if (k.equals("sct")) {

        LOG.info("avg score:\t" + (fvalue / totalCnt.get()));

      }

 else       if (k.equals("scNaN")) {

        LOG.info("score == NaN:\t" + value);

      }

 else       if (k.equals("ftn")) {

        LOG.info("earliest fetch time:\t" + new Date(1000 * 60 * value));

      }

 else       if (k.equals("ftx")) {

        LOG.info("latest fetch time:\t" + new Date(1000 * 60 * value));

      }

 else       if (k.equals("ftt")) {

        LOG.info("avg of fetch times:\t" + new Date(1000 * 60 * (value / totalCnt.get())));

      }

 else       if (k.equals("fin")) {

        LOG.info("shortest fetch interval:\t{}",TimingUtil.secondsToDaysHMS(value));

      }

 else       if (k.equals("fix")) {

        LOG.info("longest fetch interval:\t{}",TimingUtil.secondsToDaysHMS(value));

      }

 else       if (k.equals("fit")) {

        LOG.info("avg fetch interval:\t{}",TimingUtil.secondsToDaysHMS(value / totalCnt.get()));

      }

 else       if (k.startsWith("status")) {

        String[] st=k.split(" ");

        int code=Integer.parseInt(st[1]);

        if (st.length > 2)         LOG.info("   " + st[2] + " :\t"+ val);

 else         LOG.info(st[0] + " " + code+ " ("+ CrawlDatum.getStatusName((byte)code)+ "):\t"+ val);

      }

 else       if (k.equals("scd")) {

        MergingDigest tdigest=MergingDigest.fromBytes(ByteBuffer.wrap(bytesValue));

        for (        double q : quantiles) {

          LOG.info("score quantile {}:\t{}",q,tdigest.quantile(q));

        }

      }

 else {

        LOG.info(k + ":\t" + val);

      }

    }

  }

  LOG.info("CrawlDb statistics: done");

}

Location: CrawlDbReader.java

Content: 

public void processTopNJob(String crawlDb,long topN,float min,String output,Configuration config) throws IOException, ClassNotFoundException, InterruptedException {

  if (LOG.isInfoEnabled()) {

    LOG.info("CrawlDb topN: starting (topN=" + topN + ", min="+ min+ ")");

    LOG.info("CrawlDb db: {}",crawlDb);

  }

  Path outFolder=new Path(output);

  Path tempDir=new Path(config.get("mapreduce.cluster.temp.dir",".") + "/readdb-topN-temp-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));

  Job job=NutchJob.getInstance(config);

  job.setJobName("topN prepare " + crawlDb);

  FileInputFormat.addInputPath(job,new Path(crawlDb,CrawlDb.CURRENT_NAME));

  job.setInputFormatClass(SequenceFileInputFormat.class);

  job.setJarByClass(CrawlDbReader.class);

  job.setMapperClass(CrawlDbTopNMapper.class);

  job.setReducerClass(Reducer.class);

  FileOutputFormat.setOutputPath(job,tempDir);

  job.setOutputFormatClass(SequenceFileOutputFormat.class);

  job.setOutputKeyClass(FloatWritable.class);

  job.setOutputValueClass(Text.class);

  job.getConfiguration().setFloat("db.reader.topn.min",min);

  FileSystem fs=tempDir.getFileSystem(config);

  try {

    boolean success=job.waitForCompletion(true);

    if (!success) {

      String message="CrawlDbReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: "+ job.getStatus().getFailureInfo();

      LOG.error(message);

      fs.delete(tempDir,true);

      throw new RuntimeException(message);

    }

  }

 catch (  IOException|InterruptedException|ClassNotFoundException e) {

    LOG.error(StringUtils.stringifyException(e));

    fs.delete(tempDir,true);

    throw e;

  }

  LOG.info("CrawlDb topN: collecting topN scores.");

  job=NutchJob.getInstance(config);

  job.setJobName("topN collect " + crawlDb);

  job.getConfiguration().setLong("db.reader.topn",topN);

  FileInputFormat.addInputPath(job,tempDir);

  job.setInputFormatClass(SequenceFileInputFormat.class);

  job.setMapperClass(Mapper.class);

  job.setReducerClass(CrawlDbTopNReducer.class);

  job.setJarByClass(CrawlDbReader.class);

  FileOutputFormat.setOutputPath(job,outFolder);

  job.setOutputFormatClass(TextOutputFormat.class);

  job.setOutputKeyClass(FloatWritable.class);

  job.setOutputValueClass(Text.class);

  job.setNumReduceTasks(1);

  try {

    boolean success=job.waitForCompletion(true);

    if (!success) {

      String message="CrawlDbReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: "+ job.getStatus().getFailureInfo();

      LOG.error(message);

      fs.delete(tempDir,true);

      throw new RuntimeException(message);

    }

  }

 catch (  IOException|InterruptedException|ClassNotFoundException e) {

    LOG.error(StringUtils.stringifyException(e));

    fs.delete(tempDir,true);

    throw e;

  }

  fs.delete(tempDir,true);

  LOG.info("CrawlDb topN: done");

}

Location: CrawlDbReader.java

Content: 

public Object query(Map<String,String> args,Configuration conf,String type,String crawlId) throws Exception {

  Map<String,Object> results=new HashMap<>();

  String crawlDb=crawlId + "/crawldb";

  if (type.equalsIgnoreCase("stats")) {

    boolean sort=false;

    if (args.containsKey("sort")) {

      if (args.get("sort").equalsIgnoreCase("true"))       sort=true;

    }

    TreeMap<String,Writable> stats=processStatJobHelper(crawlDb,NutchConfiguration.create(),sort);

    LongWritable totalCnt=(LongWritable)stats.get("T");

    stats.remove("T");

    results.put("totalUrls",String.valueOf(totalCnt.get()));

    Map<String,Object> statusMap=new HashMap<>();

    for (    Map.Entry<String,Writable> entry : stats.entrySet()) {

      String k=entry.getKey();

      long val=0L;

      double fval=0.0;

      if (entry.getValue() instanceof LongWritable) {

        val=((LongWritable)entry.getValue()).get();

      }

 else       if (entry.getValue() instanceof FloatWritable) {

        fval=((FloatWritable)entry.getValue()).get();

      }

 else       if (entry.getValue() instanceof BytesWritable) {

        continue;

      }

      if (k.equals("scn")) {

        results.put("minScore",String.valueOf(fval));

      }

 else       if (k.equals("scx")) {

        results.put("maxScore",String.valueOf(fval));

      }

 else       if (k.equals("sct")) {

        results.put("avgScore",String.valueOf((fval / totalCnt.get())));

      }

 else       if (k.startsWith("status")) {

        String[] st=k.split(" ");

        int code=Integer.parseInt(st[1]);

        if (st.length > 2) {

          @SuppressWarnings("unchecked") Map<String,Object> individualStatusInfo=(Map<String,Object>)statusMap.get(String.valueOf(code));

          Map<String,String> hostValues;

          if (individualStatusInfo.containsKey("hostValues")) {

            hostValues=(Map<String,String>)individualStatusInfo.get("hostValues");

          }

 else {

            hostValues=new HashMap<>();

            individualStatusInfo.put("hostValues",hostValues);

          }

          hostValues.put(st[2],String.valueOf(val));

        }

 else {

          Map<String,Object> individualStatusInfo=new HashMap<>();

          individualStatusInfo.put("statusValue",CrawlDatum.getStatusName((byte)code));

          individualStatusInfo.put("count",String.valueOf(val));

          statusMap.put(String.valueOf(code),individualStatusInfo);

        }

      }

 else {

        results.put(k,String.valueOf(val));

      }

    }

    results.put("status",statusMap);

    return results;

  }

  if (type.equalsIgnoreCase("dump")) {

    String output=args.get("out_dir");

    String format="normal";

    String regex=null;

    Integer retry=null;

    String status=null;

    String expr=null;

    Float sample=null;

    if (args.containsKey("format")) {

      format=args.get("format");

    }

    if (args.containsKey("regex")) {

      regex=args.get("regex");

    }

    if (args.containsKey("retry")) {

      retry=Integer.parseInt(args.get("retry"));

    }

    if (args.containsKey("status")) {

      status=args.get("status");

    }

    if (args.containsKey("expr")) {

      expr=args.get("expr");

    }

    if (args.containsKey("sample")) {

      sample=Float.parseFloat(args.get("sample"));

    }

    processDumpJob(crawlDb,output,conf,format,regex,status,retry,expr,sample);

    File dumpFile=new File(output + "/part-00000");

    return dumpFile;

  }

  if (type.equalsIgnoreCase("topN")) {

    String output=args.get("out_dir");

    long topN=Long.parseLong(args.get("nnn"));

    float min=0.0f;

    if (args.containsKey("min")) {

      min=Float.parseFloat(args.get("min"));

    }

    processTopNJob(crawlDb,topN,min,output,conf);

    File dumpFile=new File(output + "/part-00000");

    return dumpFile;

  }

  if (type.equalsIgnoreCase("url")) {

    String url=args.get("url");

    CrawlDatum res=get(crawlDb,url,conf);

    results.put("status",res.getStatus());

    results.put("fetchTime",new Date(res.getFetchTime()));

    results.put("modifiedTime",new Date(res.getModifiedTime()));

    results.put("retriesSinceFetch",res.getRetriesSinceFetch());

    results.put("retryInterval",res.getFetchInterval());

    results.put("score",res.getScore());

    results.put("signature",StringUtil.toHexString(res.getSignature()));

    Map<String,String> metadata=new HashMap<>();

    if (res.getMetaData() != null) {

      for (      Entry<Writable,Writable> e : res.getMetaData().entrySet()) {

        metadata.put(String.valueOf(e.getKey()),String.valueOf(e.getValue()));

      }

    }

    results.put("metadata",metadata);

    return results;

  }

  return results;

}

Location: CrawlDbReader.java

Content: 

public void readUrl(String crawlDb,String url,Configuration config,StringBuilder output) throws IOException {

  CrawlDatum res=get(crawlDb,url,config);

  output.append("URL: " + url + "\n");

  if (res != null) {

    output.append(res);

  }

 else {

    output.append("not found");

  }

  output.append("\n");

}

