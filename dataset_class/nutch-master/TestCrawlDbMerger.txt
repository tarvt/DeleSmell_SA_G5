Location: TestCrawlDbMerger.java

Content: 

private void createCrawlDb(Configuration config,FileSystem fs,Path crawldb,TreeSet<String> init,CrawlDatum cd) throws Exception {

  LOG.debug("* creating crawldb: " + crawldb);

  Path dir=new Path(crawldb,CrawlDb.CURRENT_NAME);

  Option wKeyOpt=MapFile.Writer.keyClass(Text.class);

  org.apache.hadoop.io.SequenceFile.Writer.Option wValueOpt=SequenceFile.Writer.valueClass(CrawlDatum.class);

  MapFile.Writer writer=new MapFile.Writer(config,new Path(dir,"part-r-00000"),wKeyOpt,wValueOpt);

  Iterator<String> it=init.iterator();

  while (it.hasNext()) {

    String key=it.next();

    writer.append(new Text(key),cd);

  }

  writer.close();

}

Location: TestCrawlDbMerger.java

Content: 

/** 

 * Test creates two sample  {@link org.apache.nutch.crawl.CrawlDb}'s populating entries for keys as  {@link org.apache.hadoop.io.Text} e.g. URLs and values as  {@link org.apache.nutch.crawl.CrawlDatum} e.g. record data. It then simulates a merge process for the two CrawlDb's via the  {@link org.apache.nutch.crawl.CrawlDbMerger}tool. The merged CrawlDb is then written to an arbitrary output location and the results read using the  {@link org.apache.nutch.crawl.CrawlDbReader} tool. Test assertions include comparing expected CrawlDb key, value (URL, CrawlDatum) values with actual results based on the merge process. 

 * @throws Exception

 */

@Test public void testMerge() throws Exception {

  Path crawldb1=new Path(testDir,"crawldb1");

  Path crawldb2=new Path(testDir,"crawldb2");

  Path output=new Path(testDir,"output");

  createCrawlDb(conf,fs,crawldb1,init1,cd1);

  createCrawlDb(conf,fs,crawldb2,init2,cd2);

  CrawlDbMerger merger=new CrawlDbMerger(conf);

  LOG.debug("* merging crawldbs to " + output);

  merger.merge(output,new Path[]{crawldb1,crawldb2},false,false);

  LOG.debug("* reading crawldb: " + output);

  reader=new CrawlDbReader();

  String crawlDb=output.toString();

  Iterator<String> it=expected.keySet().iterator();

  while (it.hasNext()) {

    String url=it.next();

    LOG.debug("url=" + url);

    CrawlDatum cd=expected.get(url);

    CrawlDatum res=reader.get(crawlDb,url,conf);

    LOG.debug(" -> " + res);

    System.out.println("url=" + url);

    System.out.println(" cd " + cd);

    System.out.println(" res " + res);

    Assert.assertNotNull(res);

    Assert.assertTrue(cd.equals(res));

  }

  reader.close();

  fs.delete(testDir,true);

}

