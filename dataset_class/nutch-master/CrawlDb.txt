Location: CrawlDb.java

Content: 

public CrawlDb(){

}

Location: CrawlDb.java

Content: 

public CrawlDb(Configuration conf){

  setConf(conf);

}

Location: CrawlDb.java

Content: 

public static Job createJob(Configuration config,Path crawlDb) throws IOException {

  Path newCrawlDb=new Path(crawlDb,Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));

  Job job=NutchJob.getInstance(config);

  job.setJobName("crawldb " + crawlDb);

  Path current=new Path(crawlDb,CURRENT_NAME);

  if (current.getFileSystem(job.getConfiguration()).exists(current)) {

    FileInputFormat.addInputPath(job,current);

  }

  job.setInputFormatClass(SequenceFileInputFormat.class);

  job.setMapperClass(CrawlDbFilter.class);

  job.setReducerClass(CrawlDbReducer.class);

  job.setJarByClass(CrawlDb.class);

  FileOutputFormat.setOutputPath(job,newCrawlDb);

  job.setOutputFormatClass(MapFileOutputFormat.class);

  job.setOutputKeyClass(Text.class);

  job.setOutputValueClass(CrawlDatum.class);

  job.getConfiguration().setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs",false);

  return job;

}

Location: CrawlDb.java

Content: 

private static void install(Configuration conf,Path crawlDb,Path tempCrawlDb) throws IOException {

  boolean preserveBackup=conf.getBoolean("db.preserve.backup",true);

  FileSystem fs=crawlDb.getFileSystem(conf);

  Path old=new Path(crawlDb,"old");

  Path current=new Path(crawlDb,CURRENT_NAME);

  if (fs.exists(current)) {

    FSUtils.replace(fs,old,current,true);

  }

  FSUtils.replace(fs,current,tempCrawlDb,true);

  Path lock=new Path(crawlDb,LOCK_NAME);

  LockUtil.removeLockFile(fs,lock);

  if (!preserveBackup && fs.exists(old)) {

    fs.delete(old,true);

  }

}

Location: CrawlDb.java

Content: 

public static void install(Job job,Path crawlDb) throws IOException {

  Configuration conf=job.getConfiguration();

  Path tempCrawlDb=org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath(job);

  install(conf,crawlDb,tempCrawlDb);

}

Location: CrawlDb.java

Content: 

public static Path lock(Configuration job,Path crawlDb,boolean force) throws IOException {

  Path lock=new Path(crawlDb,LOCK_NAME);

  LockUtil.createLockFile(job,lock,force);

  return lock;

}

Location: CrawlDb.java

Content: 

public void update(Path crawlDb,Path[] segments,boolean normalize,boolean filter) throws IOException, InterruptedException, ClassNotFoundException {

  boolean additionsAllowed=getConf().getBoolean(CRAWLDB_ADDITIONS_ALLOWED,true);

  update(crawlDb,segments,normalize,filter,additionsAllowed,false);

}

Location: CrawlDb.java

Content: 

public void update(Path crawlDb,Path[] segments,boolean normalize,boolean filter,boolean additionsAllowed,boolean force) throws IOException, InterruptedException, ClassNotFoundException {

  Path lock=lock(getConf(),crawlDb,force);

  SimpleDateFormat sdf=new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

  long start=System.currentTimeMillis();

  Job job=CrawlDb.createJob(getConf(),crawlDb);

  Configuration conf=job.getConfiguration();

  conf.setBoolean(CRAWLDB_ADDITIONS_ALLOWED,additionsAllowed);

  conf.setBoolean(CrawlDbFilter.URL_FILTERING,filter);

  conf.setBoolean(CrawlDbFilter.URL_NORMALIZING,normalize);

  boolean url404Purging=conf.getBoolean(CRAWLDB_PURGE_404,false);

  LOG.info("CrawlDb update: starting at {}",sdf.format(start));

  LOG.info("CrawlDb update: db: {}",crawlDb);

  LOG.info("CrawlDb update: segments: {}",Arrays.asList(segments));

  LOG.info("CrawlDb update: additions allowed: {}",additionsAllowed);

  LOG.info("CrawlDb update: URL normalizing: {}",normalize);

  LOG.info("CrawlDb update: URL filtering: {}",filter);

  LOG.info("CrawlDb update: 404 purging: {}",url404Purging);

  for (int i=0; i < segments.length; i++) {

    FileSystem sfs=segments[i].getFileSystem(getConf());

    Path fetch=new Path(segments[i],CrawlDatum.FETCH_DIR_NAME);

    Path parse=new Path(segments[i],CrawlDatum.PARSE_DIR_NAME);

    if (sfs.exists(fetch)) {

      FileInputFormat.addInputPath(job,fetch);

      if (sfs.exists(parse)) {

        FileInputFormat.addInputPath(job,parse);

      }

 else {

        LOG.info(" - adding fetched but unparsed segment {}",segments[i]);

      }

    }

 else {

      LOG.info(" - skipping invalid segment {}",segments[i]);

    }

  }

  LOG.info("CrawlDb update: Merging segment data into db.");

  FileSystem fs=crawlDb.getFileSystem(getConf());

  Path outPath=FileOutputFormat.getOutputPath(job);

  try {

    boolean success=job.waitForCompletion(true);

    if (!success) {

      String message="CrawlDb update job did not succeed, job status:" + job.getStatus().getState() + ", reason: "+ job.getStatus().getFailureInfo();

      LOG.error(message);

      NutchJob.cleanupAfterFailure(outPath,lock,fs);

      throw new RuntimeException(message);

    }

  }

 catch (  IOException|InterruptedException|ClassNotFoundException e) {

    LOG.error("CrawlDb update job failed: {}",e.getMessage());

    NutchJob.cleanupAfterFailure(outPath,lock,fs);

    throw e;

  }

  CrawlDb.install(job,crawlDb);

  if (filter) {

    long urlsFiltered=job.getCounters().findCounter("CrawlDB filter","URLs filtered").getValue();

    LOG.info("CrawlDb update: Total number of existing URLs in CrawlDb rejected by URL filters: {}",urlsFiltered);

  }

  long end=System.currentTimeMillis();

  LOG.info("CrawlDb update: finished at " + sdf.format(end) + ", elapsed: "+ TimingUtil.elapsedTime(start,end));

}

