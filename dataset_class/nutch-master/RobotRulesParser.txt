Location: RobotRulesParser.java

Content: 

/** 

 * Fetch robots.txt (or it's protocol-specific equivalent) which applies to the given URL, parse it and return the set of robot rules applicable for the configured agent name(s).

 * @param protocol {@link Protocol}

 * @param url URL to check

 * @param robotsTxtContent container to store responses when fetching the robots.txt file for debugging or archival purposes. Instead of a robots.txt file, it may include redirects or an error page (404, etc.). Response {@link Content} is appended to the passed list. If null is passednothing is stored.

 * @return robot rules (specific for this URL or default), never null

 */

public BaseRobotRules getRobotRulesSet(Protocol protocol,Text url,List<Content> robotsTxtContent){

  URL u=null;

  try {

    u=new URL(url.toString());

  }

 catch (  Exception e) {

    return EMPTY_RULES;

  }

  return getRobotRulesSet(protocol,u,robotsTxtContent);

}

Location: RobotRulesParser.java

Content: 

/** 

 * Fetch robots.txt (or it's protocol-specific equivalent) which applies to the given URL, parse it and return the set of robot rules applicable for the configured agent name(s).

 * @param protocol {@link Protocol}

 * @param url URL to check

 * @param robotsTxtContent container to store responses when fetching the robots.txt file for debugging or archival purposes. Instead of a robots.txt file, it may include redirects or an error page (404, etc.). Response {@link Content} is appended to the passed list. If null is passednothing is stored.

 * @return robot rules (specific for this URL or default), never null

 */

public abstract BaseRobotRules getRobotRulesSet(Protocol protocol,URL url,List<Content> robotsTxtContent);

Location: RobotRulesParser.java

Content: 

/** 

 * Check whether a URL belongs to a whitelisted host.

 * @param url a {@link java.net.URL} to check against rules

 * @return true if allowed, false otherwise

 */

public boolean isWhiteListed(URL url){

  boolean match=false;

  String urlString=url.getHost();

  if (matcher != null) {

    match=matcher.matches(urlString);

  }

  return match;

}

Location: RobotRulesParser.java

Content: 

/** 

 * Parses the robots content using the  {@link SimpleRobotRulesParser} fromcrawler commons

 * @param url A string containing url

 * @param content Contents of the robots file in a byte array

 * @param contentType The content type of the robots file

 * @param robotName A string containing all the robots agent names used by parser for matching

 * @return BaseRobotRules object

 */

public BaseRobotRules parseRules(String url,byte[] content,String contentType,String robotName){

  return robotParser.parseContent(url,content,contentType,robotName);

}

Location: RobotRulesParser.java

Content: 

public RobotRulesParser(){

}

Location: RobotRulesParser.java

Content: 

public RobotRulesParser(Configuration conf){

  setConf(conf);

}

