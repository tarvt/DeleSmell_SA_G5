Location: TestCrawlDbStates.java

Content: 

/** 

 * Test status db_notmodified detected by <ul> <li>signature comparison</li> <li>or HTTP 304</li> </ul> In addition, test for all available  {@link FetchSchedule} implementationswhether <ul> <li>modified time is set</li> <li>re-fetch is triggered after a certain time to force the fetched content to be in a recent segment (old segments are deleted, see comments in {@link CrawlDbReducer#reduce(Text,Iterator,OutputCollector,Reporter)}</li> </ul>

 */

@Test public void testCrawlDbReducerNotModified(){

  LOG.info("Test state notmodified");

  Context context=CrawlDBTestUtil.createContext();

  Configuration conf=context.getConfiguration();

  ;

  for (  String sched : schedules) {

    String desc="test notmodified by signature comparison + " + sched;

    LOG.info(desc);

    conf.set("db.fetch.schedule.class","org.apache.nutch.crawl." + sched);

    ContinuousCrawlTestUtil crawlUtil=new CrawlTestFetchNotModified(context);

    try {

      if (!crawlUtil.run(20)) {

        fail("failed: " + desc);

      }

    }

 catch (    IOException e) {

      e.printStackTrace();

    }

  }

  for (  String sched : schedules) {

    String desc="test notmodified by HTTP 304 + " + sched;

    LOG.info(desc);

    conf.set("db.fetch.schedule.class","org.apache.nutch.crawl." + sched);

    ContinuousCrawlTestUtil crawlUtil=new CrawlTestFetchNotModifiedHttp304(context);

    try {

      if (!crawlUtil.run(20)) {

        fail("failed: " + desc);

      }

    }

 catch (    IOException e) {

      e.printStackTrace();

    }

  }

}

Location: TestCrawlDbStates.java

Content: 

/** 

 * NUTCH-1245: a fetch_gone should always result in a db_gone. <p> Even in a long-running continuous crawl, when a gone page is re-fetched several times over time. </p>

 */

@Test public void testCrawlDbReducerPageGoneSchedule1(){

  LOG.info("NUTCH-1245: test long running continuous crawl");

  ContinuousCrawlTestUtil crawlUtil=new ContinuousCrawlTestUtil(STATUS_FETCH_GONE,STATUS_DB_GONE);

  try {

    if (!crawlUtil.run(20)) {

      fail("fetch_gone did not result in a db_gone (NUTCH-1245)");

    }

  }

 catch (  IOException e) {

    e.printStackTrace();

  }

}

Location: TestCrawlDbStates.java

Content: 

/** 

 * NUTCH-1245: a fetch_gone should always result in a db_gone. <p> As some kind of misconfiguration set db.fetch.interval.default to a value &gt; (fetchIntervalMax * 1.5). </p>

 */

@Test public void testCrawlDbReducerPageGoneSchedule2(){

  LOG.info("NUTCH-1245 (misconfiguration): test with db.fetch.interval.default > (1.5 * db.fetch.interval.max)");

  Context context=CrawlDBTestUtil.createContext();

  Configuration conf=context.getConfiguration();

  int fetchIntervalMax=conf.getInt("db.fetch.interval.max",0);

  conf.setInt("db.fetch.interval.default",3 + (int)(fetchIntervalMax * 1.5));

  ContinuousCrawlTestUtil crawlUtil=new ContinuousCrawlTestUtil(context,STATUS_FETCH_GONE,STATUS_DB_GONE);

  try {

    if (!crawlUtil.run(0)) {

      fail("fetch_gone did not result in a db_gone (NUTCH-1245)");

    }

  }

 catch (  IOException e) {

    e.printStackTrace();

  }

}

Location: TestCrawlDbStates.java

Content: 

/** 

 * Test the matrix of state transitions: <ul> <li>for all available  {@link FetchSchedule} implementations</li><li>for every possible status in CrawlDb (including "not in CrawlDb")</li> <li>for every possible fetch status</li> <li>and zero or more (0-3) additional in-links</li> </ul> call  {@literal updatedb} and check whether the resulting CrawlDb status isthe expected one.

 */

@Test public void testCrawlDbStateTransitionMatrix(){

  LOG.info("Test CrawlDatum state transitions");

  Reducer<Text,CrawlDatum,Text,CrawlDatum>.Context context=CrawlDBTestUtil.createContext();

  Configuration conf=context.getConfiguration();

  CrawlDbUpdateUtil updateDb=null;

  try {

    updateDb=new CrawlDbUpdateUtil(new CrawlDbReducer(),context);

  }

 catch (  IOException e) {

    e.printStackTrace();

  }

  int retryMax=conf.getInt("db.fetch.retry.max",3);

  for (  String sched : schedules) {

    LOG.info("Testing state transitions with " + sched);

    conf.set("db.fetch.schedule.class","org.apache.nutch.crawl." + sched);

    FetchSchedule schedule=FetchScheduleFactory.getFetchSchedule(conf);

    for (int i=0; i < fetchDbStatusPairs.length; i++) {

      byte fromDbStatus=fetchDbStatusPairs[i][1];

      for (int j=0; j < fetchDbStatusPairs.length; j++) {

        byte fetchStatus=fetchDbStatusPairs[j][0];

        CrawlDatum fromDb=null;

        if (fromDbStatus == -1) {

        }

 else {

          fromDb=new CrawlDatum();

          fromDb.setStatus(fromDbStatus);

          schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL,fromDb);

        }

        byte toDbStatus=fetchDbStatusPairs[j][1];

        if (fetchStatus == -1) {

          if (fromDbStatus == -1) {

            toDbStatus=STATUS_DB_UNFETCHED;

          }

 else {

            toDbStatus=fromDbStatus;

          }

        }

 else         if (fetchStatus == STATUS_FETCH_RETRY) {

          if (fromDb == null || fromDb.getRetriesSinceFetch() < retryMax) {

            toDbStatus=STATUS_DB_UNFETCHED;

          }

 else {

            toDbStatus=STATUS_DB_GONE;

          }

        }

        String fromDbStatusName=(fromDbStatus == -1 ? "<not in CrawlDb>" : getStatusName(fromDbStatus));

        String fetchStatusName=(fetchStatus == -1 ? "<only inlinks>" : CrawlDatum.getStatusName(fetchStatus));

        LOG.info(fromDbStatusName + " + " + fetchStatusName+ " => "+ getStatusName(toDbStatus));

        List<CrawlDatum> values=new ArrayList<CrawlDatum>();

        for (int l=0; l <= 2; l++) {

          CrawlDatum fetch=null;

          if (fetchStatus == -1) {

            if (l == 0)             continue;

          }

 else {

            fetch=new CrawlDatum();

            if (fromDb != null) {

              fetch.set(fromDb);

            }

 else {

              schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL,fetch);

            }

            fetch.setStatus(fetchStatus);

            fetch.setFetchTime(System.currentTimeMillis());

          }

          if (fromDb != null)           values.add(fromDb);

          if (fetch != null)           values.add(fetch);

          for (int n=0; n < l; n++) {

            values.add(linked);

          }

          List<CrawlDatum> res=updateDb.update(values);

          if (res.size() != 1) {

            fail("CrawlDb update didn't result in one single CrawlDatum per URL");

            continue;

          }

          byte status=res.get(0).getStatus();

          if (status != toDbStatus) {

            fail("CrawlDb update for " + fromDbStatusName + " and "+ fetchStatusName+ " and "+ l+ " inlinks results in "+ getStatusName(status)+ " (expected: "+ getStatusName(toDbStatus)+ ")");

          }

          values.clear();

        }

      }

    }

  }

}

Location: TestCrawlDbStates.java

Content: 

/** 

 * Test states after inject: inject must not modify the status of CrawlDatums already in CrawlDb. Newly injected elements have status "db_unfetched". Inject is simulated by calling  {@link Injector.InjectReducer#reduce()}.

 */

@Test public void testCrawlDbStatTransitionInject(){

  LOG.info("Test CrawlDatum states in Injector after inject");

  Configuration conf=CrawlDBTestUtil.createContext().getConfiguration();

  Injector.InjectReducer injector=new Injector.InjectReducer();

  CrawlDbUpdateTestDriver<Injector.InjectReducer> injectDriver=new CrawlDbUpdateTestDriver<Injector.InjectReducer>(injector,conf);

  ScoringFilters scfilters=new ScoringFilters(conf);

  for (  String sched : schedules) {

    LOG.info("Testing inject with " + sched);

    conf.set("db.fetch.schedule.class","org.apache.nutch.crawl." + sched);

    FetchSchedule schedule=FetchScheduleFactory.getFetchSchedule(conf);

    List<CrawlDatum> values=new ArrayList<CrawlDatum>();

    for (int i=0; i < fetchDbStatusPairs.length; i++) {

      byte fromDbStatus=fetchDbStatusPairs[i][1];

      byte toDbStatus=fromDbStatus;

      if (fromDbStatus == -1) {

        toDbStatus=STATUS_DB_UNFETCHED;

      }

 else {

        CrawlDatum fromDb=new CrawlDatum();

        fromDb.setStatus(fromDbStatus);

        schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL,fromDb);

        values.add(fromDb);

      }

      LOG.info("inject " + (fromDbStatus == -1 ? "<not in CrawlDb>" : CrawlDatum.getStatusName(fromDbStatus)) + " + "+ getStatusName(STATUS_INJECTED)+ " => "+ getStatusName(toDbStatus));

      CrawlDatum injected=new CrawlDatum(STATUS_INJECTED,conf.getInt("db.fetch.interval.default",2592000),0.1f);

      schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL,injected);

      try {

        scfilters.injectedScore(CrawlDbUpdateUtil.dummyURL,injected);

      }

 catch (      ScoringFilterException e) {

        LOG.error(StringUtils.stringifyException(e));

      }

      values.add(injected);

      List<CrawlDatum> res=injectDriver.update(values);

      if (res.size() != 1) {

        fail("Inject didn't result in one single CrawlDatum per URL");

        continue;

      }

      byte status=res.get(0).getStatus();

      if (status != toDbStatus) {

        fail("Inject for " + (fromDbStatus == -1 ? "" : getStatusName(fromDbStatus) + " and ") + getStatusName(STATUS_INJECTED)+ " results in "+ getStatusName(status)+ " (expected: "+ getStatusName(toDbStatus)+ ")");

      }

      values.clear();

    }

  }

}

Location: TestCrawlDbStates.java

Content: 

/** 

 * Test whether signatures are reset for "content-less" states (gone, redirect, etc.): otherwise, if this state is temporary and the document appears again with the old content, it may get marked as not_modified in CrawlDb just after the redirect state. In this case we cannot expect content in segments. Cf. NUTCH-1422: reset signature for redirects.

 */

@Test public void testSignatureReset(){

  LOG.info("NUTCH-1422 must reset signature for redirects and similar states");

  Context context=CrawlDBTestUtil.createContext();

  Configuration conf=context.getConfiguration();

  for (  String sched : schedules) {

    LOG.info("Testing reset signature with " + sched);

    conf.set("db.fetch.schedule.class","org.apache.nutch.crawl." + sched);

    ContinuousCrawlTestUtil crawlUtil=new CrawlTestSignatureReset(context);

    try {

      if (!crawlUtil.run(20)) {

        fail("failed: signature not reset");

      }

    }

 catch (    IOException e) {

      e.printStackTrace();

    }

  }

}

