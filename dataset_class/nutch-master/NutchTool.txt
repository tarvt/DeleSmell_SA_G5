Location: NutchTool.java

Content: 

/** 

 * Get relative progress of the tool. Progress is represented as a float in range [0,1] where 1 is complete. 

 * @return a float in range [0,1].

 */

public float getProgress(){

  float res=0;

  if (currentJob != null) {

    try {

      res=(currentJob.mapProgress() + currentJob.reduceProgress()) / 2.0f;

    }

 catch (    IOException e) {

      e.printStackTrace();

      res=0;

    }

catch (    IllegalStateException ile) {

      ile.printStackTrace();

      res=0;

    }

  }

  if (numJobs > 1) {

    res=(currentJobNum + res) / (float)numJobs;

  }

  status.put(Nutch.STAT_PROGRESS,res);

  return res;

}

Location: NutchTool.java

Content: 

/** 

 * Kill the job immediately. Clients should assume that any results that the job produced so far are in an inconsistent state or missing.

 * @return true if succeeded, false otherwise.

 * @throws Exception if there is an error stopping the current{@link org.apache.hadoop.mapreduce.Job}

 */

public boolean killJob() throws Exception {

  if (currentJob != null && !currentJob.isComplete()) {

    try {

      currentJob.killJob();

      return true;

    }

 catch (    Exception e) {

      e.printStackTrace();

      return false;

    }

  }

  return false;

}

Location: NutchTool.java

Content: 

public NutchTool(){

  super(null);

}

Location: NutchTool.java

Content: 

public NutchTool(Configuration conf){

  super(conf);

}

Location: NutchTool.java

Content: 

/** 

 * Runs the tool, using a map of arguments. May return results, or null.

 * @param args a {@link Map} of arguments to be run with the tool

 * @param crawlId a crawl identifier to associate with the tool invocation 

 * @return Map results object if tool executes successfully otherwise null 

 * @throws Exception if there is an error during the tool execution

 */

public abstract Map<String,Object> run(Map<String,Object> args,String crawlId) throws Exception ;

Location: NutchTool.java

Content: 

/** 

 * Stop the job with the possibility to resume. Subclasses should override this, since by default it calls  {@link #killJob()}.

 * @return true if succeeded, false otherwise

 * @throws Exception if there is an error stopping the current{@link org.apache.hadoop.mapreduce.Job}

 */

public boolean stopJob() throws Exception {

  return killJob();

}

