Location: TestJoinDatamerge.java

Content: 

private static void checkOuterConsistency(Job job,Path[] src) throws IOException {

  Path outf=FileOutputFormat.getOutputPath(job);

  FileStatus[] outlist=cluster.getFileSystem().listStatus(outf,new Utils.OutputFileUtils.OutputFilesFilter());

  assertEquals("number of part files is more than 1. It is" + outlist.length,1,outlist.length);

  assertTrue("output file with zero length" + outlist[0].getLen(),0 < outlist[0].getLen());

  SequenceFile.Reader r=new SequenceFile.Reader(cluster.getFileSystem(),outlist[0].getPath(),job.getConfiguration());

  IntWritable k=new IntWritable();

  IntWritable v=new IntWritable();

  while (r.next(k,v)) {

    assertEquals("counts does not match",v.get(),countProduct(k,src,job.getConfiguration()));

  }

  r.close();

}

Location: TestJoinDatamerge.java

Content: 

private static int countProduct(IntWritable key,Path[] src,Configuration conf) throws IOException {

  int product=1;

  for (  Path p : src) {

    int count=0;

    SequenceFile.Reader r=new SequenceFile.Reader(cluster.getFileSystem(),p,conf);

    IntWritable k=new IntWritable();

    IntWritable v=new IntWritable();

    while (r.next(k,v)) {

      if (k.equals(key)) {

        count++;

      }

    }

    r.close();

    if (count != 0) {

      product*=count;

    }

  }

  return product;

}

Location: TestJoinDatamerge.java

Content: 

private static void joinAs(String jointype,Class<? extends SimpleCheckerMapBase<?>> map,Class<? extends SimpleCheckerReduceBase> reduce) throws Exception {

  final int srcs=4;

  Configuration conf=new Configuration();

  Path base=cluster.getFileSystem().makeQualified(new Path("/" + jointype));

  Path[] src=writeSimpleSrc(base,conf,srcs);

  conf.set(CompositeInputFormat.JOIN_EXPR,CompositeInputFormat.compose(jointype,SequenceFileInputFormat.class,src));

  conf.setInt("testdatamerge.sources",srcs);

  Job job=Job.getInstance(conf);

  job.setInputFormatClass(CompositeInputFormat.class);

  FileOutputFormat.setOutputPath(job,new Path(base,"out"));

  job.setMapperClass(map);

  job.setReducerClass(reduce);

  job.setOutputFormatClass(SequenceFileOutputFormat.class);

  job.setOutputKeyClass(IntWritable.class);

  job.setOutputValueClass(IntWritable.class);

  job.waitForCompletion(true);

  assertTrue("Job failed",job.isSuccessful());

  if ("outer".equals(jointype)) {

    checkOuterConsistency(job,src);

  }

  base.getFileSystem(conf).delete(base,true);

}

Location: TestJoinDatamerge.java

Content: 

public void testEmptyJoin() throws Exception {

  Configuration conf=new Configuration();

  Path base=cluster.getFileSystem().makeQualified(new Path("/empty"));

  Path[] src={new Path(base,"i0"),new Path("i1"),new Path("i2")};

  conf.set(CompositeInputFormat.JOIN_EXPR,CompositeInputFormat.compose("outer",MapReduceTestUtil.Fake_IF.class,src));

  MapReduceTestUtil.Fake_IF.setKeyClass(conf,MapReduceTestUtil.IncomparableKey.class);

  Job job=Job.getInstance(conf);

  job.setInputFormatClass(CompositeInputFormat.class);

  FileOutputFormat.setOutputPath(job,new Path(base,"out"));

  job.setMapperClass(Mapper.class);

  job.setReducerClass(Reducer.class);

  job.setOutputKeyClass(MapReduceTestUtil.IncomparableKey.class);

  job.setOutputValueClass(NullWritable.class);

  job.waitForCompletion(true);

  assertTrue(job.isSuccessful());

  base.getFileSystem(conf).delete(base,true);

}

Location: TestJoinDatamerge.java

Content: 

public void testNestedJoin() throws Exception {

  final int SOURCES=3;

  final int ITEMS=(SOURCES + 1) * (SOURCES + 1);

  Configuration conf=new Configuration();

  Path base=cluster.getFileSystem().makeQualified(new Path("/nested"));

  int[][] source=new int[SOURCES][];

  for (int i=0; i < SOURCES; ++i) {

    source[i]=new int[ITEMS];

    for (int j=0; j < ITEMS; ++j) {

      source[i][j]=(i + 2) * (j + 1);

    }

  }

  Path[] src=new Path[SOURCES];

  SequenceFile.Writer out[]=createWriters(base,conf,SOURCES,src);

  IntWritable k=new IntWritable();

  for (int i=0; i < SOURCES; ++i) {

    IntWritable v=new IntWritable();

    v.set(i);

    for (int j=0; j < ITEMS; ++j) {

      k.set(source[i][j]);

      out[i].append(k,v);

    }

    out[i].close();

  }

  out=null;

  StringBuilder sb=new StringBuilder();

  sb.append("outer(inner(");

  for (int i=0; i < SOURCES; ++i) {

    sb.append(CompositeInputFormat.compose(SequenceFileInputFormat.class,src[i].toString()));

    if (i + 1 != SOURCES)     sb.append(",");

  }

  sb.append("),outer(");

  sb.append(CompositeInputFormat.compose(MapReduceTestUtil.Fake_IF.class,"foobar"));

  sb.append(",");

  for (int i=0; i < SOURCES; ++i) {

    sb.append(CompositeInputFormat.compose(SequenceFileInputFormat.class,src[i].toString()));

    sb.append(",");

  }

  sb.append(CompositeInputFormat.compose(MapReduceTestUtil.Fake_IF.class,"raboof") + "))");

  conf.set(CompositeInputFormat.JOIN_EXPR,sb.toString());

  MapReduceTestUtil.Fake_IF.setKeyClass(conf,IntWritable.class);

  MapReduceTestUtil.Fake_IF.setValClass(conf,IntWritable.class);

  Job job=Job.getInstance(conf);

  Path outf=new Path(base,"out");

  FileOutputFormat.setOutputPath(job,outf);

  job.setInputFormatClass(CompositeInputFormat.class);

  job.setMapperClass(Mapper.class);

  job.setReducerClass(Reducer.class);

  job.setNumReduceTasks(0);

  job.setOutputKeyClass(IntWritable.class);

  job.setOutputValueClass(TupleWritable.class);

  job.setOutputFormatClass(SequenceFileOutputFormat.class);

  job.waitForCompletion(true);

  assertTrue("Job failed",job.isSuccessful());

  FileStatus[] outlist=cluster.getFileSystem().listStatus(outf,new Utils.OutputFileUtils.OutputFilesFilter());

  assertEquals(1,outlist.length);

  assertTrue(0 < outlist[0].getLen());

  SequenceFile.Reader r=new SequenceFile.Reader(cluster.getFileSystem(),outlist[0].getPath(),conf);

  TupleWritable v=new TupleWritable();

  while (r.next(k,v)) {

    assertFalse(((TupleWritable)v.get(1)).has(0));

    assertFalse(((TupleWritable)v.get(1)).has(SOURCES + 1));

    boolean chk=true;

    int ki=k.get();

    for (int i=2; i < SOURCES + 2; ++i) {

      if ((ki % i) == 0 && ki <= i * ITEMS) {

        assertEquals(i - 2,((IntWritable)((TupleWritable)v.get(1)).get((i - 1))).get());

      }

 else       chk=false;

    }

    if (chk) {

      assertTrue(v.has(0));

      for (int i=0; i < SOURCES; ++i)       assertTrue(((TupleWritable)v.get(0)).has(i));

    }

 else {

      assertFalse(v.has(0));

    }

  }

  r.close();

  base.getFileSystem(conf).delete(base,true);

}

Location: TestJoinDatamerge.java

Content: 

public void testSimpleInnerJoin() throws Exception {

  joinAs("inner",InnerJoinMapChecker.class,InnerJoinReduceChecker.class);

}

Location: TestJoinDatamerge.java

Content: 

public void testSimpleOuterJoin() throws Exception {

  joinAs("outer",OuterJoinMapChecker.class,OuterJoinReduceChecker.class);

}

Location: TestJoinDatamerge.java

Content: 

public void testSimpleOverride() throws Exception {

  joinAs("override",OverrideMapChecker.class,OverrideReduceChecker.class);

}

Location: TestJoinDatamerge.java

Content: 

private static Path[] writeSimpleSrc(Path testdir,Configuration conf,int srcs) throws IOException {

  SequenceFile.Writer out[]=null;

  Path[] src=new Path[srcs];

  try {

    out=createWriters(testdir,conf,srcs,src);

    final int capacity=srcs * 2 + 1;

    IntWritable key=new IntWritable();

    IntWritable val=new IntWritable();

    for (int k=0; k < capacity; ++k) {

      for (int i=0; i < srcs; ++i) {

        key.set(k % srcs == 0 ? k * srcs : k * srcs + i);

        val.set(10 * k + i);

        out[i].append(key,val);

        if (i == k) {

          out[i].append(key,val);

        }

      }

    }

  }

  finally {

    if (out != null) {

      for (int i=0; i < srcs; ++i) {

        if (out[i] != null)         out[i].close();

      }

    }

  }

  return src;

}

