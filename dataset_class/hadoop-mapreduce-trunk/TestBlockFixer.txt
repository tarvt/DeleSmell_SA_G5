Location: TestBlockFixer.java

Content: 

/** 

 * Tests integrity of generated block. Create a file and delete a block entirely. Wait for the block to be regenerated. Now stop RaidNode and corrupt the generated block. Test that corruption in the generated block can be detected by clients.

 */

protected void generatedBlockTestCommon(String testName,int blockToCorrupt,boolean local) throws Exception {

  LOG.info("Test " + testName + " started.");

  long blockSize=8192L;

  int stripeLength=3;

  mySetup(stripeLength,-1);

  Path file1=new Path("/user/dhruba/raidtest/file1");

  Path destPath=new Path("/destraid/user/dhruba/raidtest");

  long crc1=TestRaidDfs.createTestFile(fileSys,file1,1,7,blockSize);

  long file1Len=fileSys.getFileStatus(file1).getLen();

  LOG.info("Test " + testName + " created test files");

  Configuration localConf=new Configuration(conf);

  localConf.set(RaidNode.RAID_LOCATION_KEY,"/destraid");

  localConf.setInt("raid.blockfix.interval",1000);

  if (local) {

    localConf.set("raid.blockfix.classname","org.apache.hadoop.raid.LocalBlockFixer");

  }

 else {

    localConf.set("raid.blockfix.classname","org.apache.hadoop.raid.DistBlockFixer");

  }

  localConf.setLong("raid.blockfix.filespertask",2L);

  try {

    cnode=RaidNode.createRaidNode(null,localConf);

    TestRaidDfs.waitForFileRaided(LOG,fileSys,file1,destPath);

    cnode.stop();

    cnode.join();

    FileStatus srcStat=fileSys.getFileStatus(file1);

    DistributedFileSystem dfs=(DistributedFileSystem)fileSys;

    LocatedBlocks locs=RaidDFSUtil.getBlockLocations(dfs,file1.toUri().getPath(),0,srcStat.getLen());

    String[] corruptFiles=RaidDFSUtil.getCorruptFiles(dfs);

    assertEquals("no corrupt files expected",0,corruptFiles.length);

    assertEquals("filesFixed() should return 0 before fixing files",0,cnode.blockFixer.filesFixed());

    corruptBlock(locs.get(0).getBlock());

    reportCorruptBlocks(dfs,file1,new int[]{0},blockSize);

    corruptFiles=RaidDFSUtil.getCorruptFiles(dfs);

    assertEquals("file not corrupted",1,corruptFiles.length);

    assertEquals("wrong file corrupted",corruptFiles[0],file1.toUri().getPath());

    cnode=RaidNode.createRaidNode(null,localConf);

    long start=System.currentTimeMillis();

    while (cnode.blockFixer.filesFixed() < 1 && System.currentTimeMillis() - start < 120000) {

      LOG.info("Test " + testName + " waiting for files to be fixed.");

      Thread.sleep(1000);

    }

    assertEquals("file not fixed",1,cnode.blockFixer.filesFixed());

    cnode.stop();

    cnode.join();

    cnode=null;

    dfs=getDFS(conf,dfs);

    assertTrue("file not fixed",TestRaidDfs.validateFile(dfs,file1,file1Len,crc1));

    locs=RaidDFSUtil.getBlockLocations(dfs,file1.toUri().getPath(),0,srcStat.getLen());

    corruptBlock(locs.get(0).getBlock());

    reportCorruptBlocks(dfs,file1,new int[]{0},blockSize);

    try {

      Thread.sleep(5 * 1000);

    }

 catch (    InterruptedException ignore) {

    }

    try {

      TestRaidDfs.validateFile(dfs,file1,file1Len,crc1);

      fail("Expected exception not thrown");

    }

 catch (    org.apache.hadoop.fs.ChecksumException ce) {

    }

catch (    org.apache.hadoop.hdfs.BlockMissingException bme) {

    }

  }

 catch (  Exception e) {

    LOG.info("Test " + testName + " Exception "+ e+ StringUtils.stringifyException(e));

    throw e;

  }

 finally {

    hongshuai();

    if (cnode != null) {

      cnode.stop();

      cnode.join();

    }

    if (mr != null) {

      mr.shutdown();

    }

    if (dfs != null) {

      dfs.shutdown();

    }

  }

  LOG.info("Test " + testName + " completed.");

}

Location: TestBlockFixer.java

Content: 

protected static DistributedFileSystem getDFS(Configuration conf,FileSystem dfs) throws IOException {

  Configuration clientConf=new Configuration(conf);

  clientConf.set("fs.hdfs.impl","org.apache.hadoop.hdfs.DistributedFileSystem");

  clientConf.setBoolean("fs.hdfs.impl.disable.cache",true);

  URI dfsUri=dfs.getUri();

  FileSystem.closeAll();

  return (DistributedFileSystem)FileSystem.get(dfsUri,clientConf);

}

Location: TestBlockFixer.java

Content: 

/** 

 * Create a file with three stripes, corrupt a block each in two stripes, and wait for the the file to be fixed.

 */

protected void implBlockFix(boolean local) throws Exception {

  LOG.info("Test testBlockFix started.");

  long blockSize=8192L;

  int stripeLength=3;

  mySetup(stripeLength,-1);

  Path file1=new Path("/user/dhruba/raidtest/file1");

  Path destPath=new Path("/destraid/user/dhruba/raidtest");

  long crc1=TestRaidDfs.createTestFilePartialLastBlock(fileSys,file1,1,7,blockSize);

  long file1Len=fileSys.getFileStatus(file1).getLen();

  LOG.info("Test testBlockFix created test files");

  Configuration localConf=new Configuration(conf);

  localConf.set(RaidNode.RAID_LOCATION_KEY,"/destraid");

  localConf.setInt("raid.blockfix.interval",1000);

  if (local) {

    localConf.set("raid.blockfix.classname","org.apache.hadoop.raid.LocalBlockFixer");

  }

 else {

    localConf.set("raid.blockfix.classname","org.apache.hadoop.raid.DistBlockFixer");

  }

  localConf.setLong("raid.blockfix.filespertask",2L);

  try {

    cnode=RaidNode.createRaidNode(null,localConf);

    TestRaidDfs.waitForFileRaided(LOG,fileSys,file1,destPath);

    cnode.stop();

    cnode.join();

    FileStatus srcStat=fileSys.getFileStatus(file1);

    DistributedFileSystem dfs=(DistributedFileSystem)fileSys;

    LocatedBlocks locs=RaidDFSUtil.getBlockLocations(dfs,file1.toUri().getPath(),0,srcStat.getLen());

    String[] corruptFiles=RaidDFSUtil.getCorruptFiles(dfs);

    assertEquals("no corrupt files expected",0,corruptFiles.length);

    assertEquals("filesFixed() should return 0 before fixing files",0,cnode.blockFixer.filesFixed());

    int[] corruptBlockIdxs=new int[]{0,4,6};

    for (    int idx : corruptBlockIdxs)     corruptBlock(locs.get(idx).getBlock());

    reportCorruptBlocks(dfs,file1,corruptBlockIdxs,blockSize);

    corruptFiles=RaidDFSUtil.getCorruptFiles(dfs);

    assertEquals("file not corrupted",1,corruptFiles.length);

    assertEquals("wrong file corrupted",corruptFiles[0],file1.toUri().getPath());

    assertEquals("wrong number of corrupt blocks",3,RaidDFSUtil.corruptBlocksInFile(dfs,file1.toUri().getPath(),0,srcStat.getLen()).size());

    cnode=RaidNode.createRaidNode(null,localConf);

    long start=System.currentTimeMillis();

    while (cnode.blockFixer.filesFixed() < 1 && System.currentTimeMillis() - start < 120000) {

      LOG.info("Test testBlockFix waiting for files to be fixed.");

      Thread.sleep(1000);

    }

    assertEquals("file not fixed",1,cnode.blockFixer.filesFixed());

    dfs=getDFS(conf,dfs);

    assertTrue("file not fixed",TestRaidDfs.validateFile(dfs,file1,file1Len,crc1));

  }

 catch (  Exception e) {

    LOG.info("Test testBlockFix Exception " + e + StringUtils.stringifyException(e));

    throw e;

  }

 finally {

    hongshuai();

    if (cnode != null) {

      cnode.stop();

      cnode.join();

    }

    if (mr != null) {

      mr.shutdown();

    }

    if (dfs != null) {

      dfs.shutdown();

    }

  }

  LOG.info("Test testBlockFix completed.");

}

Location: TestBlockFixer.java

Content: 

/** 

 * Corrupt a parity file and wait for it to get fixed.

 */

protected void implParityBlockFix(String testName,boolean local) throws Exception {

  LOG.info("Test " + testName + " started.");

  long blockSize=8192L;

  int stripeLength=3;

  mySetup(stripeLength,-1);

  Path file1=new Path("/user/dhruba/raidtest/file1");

  Path destPath=new Path("/destraid/user/dhruba/raidtest");

  Path parityFile=new Path("/destraid/user/dhruba/raidtest/file1");

  TestRaidDfs.createTestFilePartialLastBlock(fileSys,file1,1,7,blockSize);

  LOG.info("Test " + testName + " created test files");

  Configuration localConf=new Configuration(conf);

  localConf.set(RaidNode.RAID_LOCATION_KEY,"/destraid");

  localConf.setInt("raid.blockfix.interval",1000);

  if (local) {

    localConf.set("raid.blockfix.classname","org.apache.hadoop.raid.LocalBlockFixer");

  }

 else {

    localConf.set("raid.blockfix.classname","org.apache.hadoop.raid.DistBlockFixer");

  }

  localConf.setLong("raid.blockfix.filespertask",2L);

  try {

    cnode=RaidNode.createRaidNode(null,localConf);

    TestRaidDfs.waitForFileRaided(LOG,fileSys,file1,destPath);

    cnode.stop();

    cnode.join();

    long parityCRC=getCRC(fileSys,parityFile);

    FileStatus parityStat=fileSys.getFileStatus(parityFile);

    DistributedFileSystem dfs=(DistributedFileSystem)fileSys;

    LocatedBlocks locs=RaidDFSUtil.getBlockLocations(dfs,parityFile.toUri().getPath(),0,parityStat.getLen());

    String[] corruptFiles=RaidDFSUtil.getCorruptFiles(dfs);

    assertEquals("no corrupt files expected",0,corruptFiles.length);

    assertEquals("filesFixed() should return 0 before fixing files",0,cnode.blockFixer.filesFixed());

    int[] corruptBlockIdxs=new int[]{0,1,2};

    for (    int idx : corruptBlockIdxs)     corruptBlock(locs.get(idx).getBlock());

    reportCorruptBlocks(dfs,parityFile,corruptBlockIdxs,blockSize);

    corruptFiles=RaidDFSUtil.getCorruptFiles(dfs);

    assertEquals("file not corrupted",1,corruptFiles.length);

    assertEquals("wrong file corrupted",corruptFiles[0],parityFile.toUri().getPath());

    cnode=RaidNode.createRaidNode(null,localConf);

    long start=System.currentTimeMillis();

    while (cnode.blockFixer.filesFixed() < 1 && System.currentTimeMillis() - start < 120000) {

      LOG.info("Test " + testName + " waiting for files to be fixed.");

      Thread.sleep(1000);

    }

    assertEquals("file not fixed",1,cnode.blockFixer.filesFixed());

    long checkCRC=getCRC(fileSys,parityFile);

    assertEquals("file not fixed",parityCRC,checkCRC);

  }

 catch (  Exception e) {

    LOG.info("Test " + testName + " Exception "+ e+ StringUtils.stringifyException(e));

    throw e;

  }

 finally {

    hongshuai();

    if (cnode != null) {

      cnode.stop();

      cnode.join();

    }

    if (mr != null) {

      mr.shutdown();

    }

    if (dfs != null) {

      dfs.shutdown();

    }

  }

  LOG.info("Test " + testName + " completed.");

}

Location: TestBlockFixer.java

Content: 

protected void implParityHarBlockFix(String testName,boolean local) throws Exception {

  LOG.info("Test " + testName + " started.");

  long blockSize=8192L;

  int stripeLength=3;

  mySetup(stripeLength,0);

  Path file1=new Path("/user/dhruba/raidtest/file1");

  TestRaidDfs.createTestFilePartialLastBlock(fileSys,file1,1,20,blockSize);

  LOG.info("Test " + testName + " created test files");

  Configuration localConf=new Configuration(conf);

  localConf.setLong("har.block.size",blockSize * 2);

  localConf.set(RaidNode.RAID_LOCATION_KEY,"/destraid");

  localConf.setInt("raid.blockfix.interval",1000);

  if (local) {

    localConf.set("raid.blockfix.classname","org.apache.hadoop.raid.LocalBlockFixer");

  }

 else {

    localConf.set("raid.blockfix.classname","org.apache.hadoop.raid.DistBlockFixer");

  }

  localConf.setLong("raid.blockfix.filespertask",2L);

  try {

    cnode=RaidNode.createRaidNode(null,localConf);

    Path harDirectory=new Path("/destraid/user/dhruba/raidtest/raidtest" + RaidNode.HAR_SUFFIX);

    long start=System.currentTimeMillis();

    while (System.currentTimeMillis() - start < 1000 * 120) {

      if (fileSys.exists(harDirectory)) {

        break;

      }

      LOG.info("Test " + testName + " waiting for har");

      Thread.sleep(1000);

    }

    Path partFile=new Path(harDirectory,"part-0");

    long partCRC=getCRC(fileSys,partFile);

    FileStatus partStat=fileSys.getFileStatus(partFile);

    DistributedFileSystem dfs=(DistributedFileSystem)fileSys;

    LocatedBlocks locs=RaidDFSUtil.getBlockLocations(dfs,partFile.toUri().getPath(),0,partStat.getLen());

    assertEquals("wrong number of har blocks",4,locs.getLocatedBlocks().size());

    cnode.stop();

    cnode.join();

    String[] corruptFiles=RaidDFSUtil.getCorruptFiles(dfs);

    assertEquals("no corrupt files expected",0,corruptFiles.length);

    assertEquals("filesFixed() should return 0 before fixing files",0,cnode.blockFixer.filesFixed());

    int[] corruptBlockIdxs=new int[]{0,3};

    for (    int idx : corruptBlockIdxs)     corruptBlock(locs.get(idx).getBlock());

    reportCorruptBlocks(dfs,partFile,corruptBlockIdxs,partStat.getBlockSize());

    corruptFiles=RaidDFSUtil.getCorruptFiles(dfs);

    assertEquals("file not corrupted",1,corruptFiles.length);

    assertEquals("wrong file corrupted",corruptFiles[0],partFile.toUri().getPath());

    cnode=RaidNode.createRaidNode(null,localConf);

    start=System.currentTimeMillis();

    while (cnode.blockFixer.filesFixed() < 1 && System.currentTimeMillis() - start < 120000) {

      LOG.info("Test " + testName + " waiting for files to be fixed.");

      Thread.sleep(1000);

    }

    assertEquals("file not fixed",1,cnode.blockFixer.filesFixed());

    long checkCRC=getCRC(fileSys,partFile);

    assertEquals("file not fixed",partCRC,checkCRC);

  }

 catch (  Exception e) {

    LOG.info("Test " + testName + " Exception "+ e+ StringUtils.stringifyException(e));

    throw e;

  }

 finally {

    hongshuai();

    if (cnode != null) {

      cnode.stop();

      cnode.join();

    }

    if (mr != null) {

      mr.shutdown();

    }

    if (dfs != null) {

      dfs.shutdown();

    }

  }

  LOG.info("Test " + testName + " completed.");

}

Location: TestBlockFixer.java

Content: 

static void reportCorruptBlocks(FileSystem fs,Path file,int[] idxs,long blockSize) throws IOException {

  FSDataInputStream in=fs.open(file);

  for (  int idx : idxs) {

    long offset=idx * blockSize;

    LOG.info("Reporting corrupt block " + file + ":"+ offset);

    in.seek(offset);

    try {

      in.readFully(new byte[(int)blockSize]);

      fail("Expected exception not thrown for " + file + ":"+ offset);

    }

 catch (    org.apache.hadoop.fs.ChecksumException e) {

    }

catch (    org.apache.hadoop.hdfs.BlockMissingException bme) {

    }

  }

}

Location: TestBlockFixer.java

Content: 

@Test public void testBlockFixLocal() throws Exception {

  implBlockFix(true);

}

Location: TestBlockFixer.java

Content: 

/** 

 * Tests integrity of generated block. Create a file and delete a block entirely. Wait for the block to be regenerated. Now stop RaidNode and corrupt the generated block. Test that corruption in the generated block can be detected by clients.

 */

@Test public void testGeneratedBlockLocal() throws Exception {

  generatedBlockTestCommon("testGeneratedBlock",3,true);

}

Location: TestBlockFixer.java

Content: 

/** 

 * Tests integrity of generated last block. Create a file and delete a block entirely. Wait for the block to be regenerated. Now stop RaidNode and corrupt the generated block. Test that corruption in the generated block can be detected by clients.

 */

@Test public void testGeneratedLastBlockLocal() throws Exception {

  generatedBlockTestCommon("testGeneratedLastBlock",6,true);

}

Location: TestBlockFixer.java

Content: 

/** 

 * Tests isXorParityFile and isRsParityFile

 */

@Test public void testIsParityFile() throws IOException {

  Configuration testConf=new Configuration();

  testConf.set("hdfs.raid.locations","/raid");

  testConf.set("hdfs.raidrs.locations","/raidrs");

  BlockFixer.BlockFixerHelper helper=new BlockFixer.BlockFixerHelper(testConf);

  assertFalse("incorrectly identified rs parity file as xor parity file",helper.isXorParityFile(new Path("/raidrs/test/test")));

  assertTrue("could not identify rs parity file",helper.isRsParityFile(new Path("/raidrs/test/test")));

  assertTrue("could not identify xor parity file",helper.isXorParityFile(new Path("/raid/test/test")));

  assertFalse("incorrectly identified xor parity file as rs parity file",helper.isRsParityFile(new Path("/raid/test/test")));

}

Location: TestBlockFixer.java

Content: 

@Test public void testParityBlockFixLocal() throws Exception {

  implParityBlockFix("testParityBlockFixLocal",true);

}

Location: TestBlockFixer.java

Content: 

@Test public void testParityHarBlockFixLocal() throws Exception {

  implParityHarBlockFix("testParityHarBlockFixLocal",true);

}

Location: TestBlockFixer.java

Content: 

/** 

 * Test the filtering of trash files from the list of corrupt files.

 */

@Test public void testTrashFilter(){

  List<Path> files=new LinkedList<Path>();

  Path p1=new Path("/user/raid/raidtest/f1");

  Path p2=new Path("/user/.Trash/");

  Path p3=new Path("/user/raid/.Trash/raidtest/f1");

  Path p4=new Path("/user/raid/.Trash/");

  files.add(p1);

  files.add(p3);

  files.add(p4);

  files.add(p2);

  Configuration conf=new Configuration();

  RaidUtils.filterTrash(conf,files);

  assertEquals("expected 2 non-trash files but got " + files.size(),2,files.size());

  for (  Path p : files) {

    assertTrue("wrong file returned by filterTrash",p == p1 || p == p2);

  }

}

