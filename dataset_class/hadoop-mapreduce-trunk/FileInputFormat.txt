Location: FileInputFormat.java

Content: 

/** 

 * Add files in the input path recursively into the results.

 * @param result The List to store all files.

 * @param fs The FileSystem.

 * @param path The input path.

 * @param inputFilter The input filter that can be used to filter files/dirs. 

 * @throws IOException

 */

protected void addInputPathRecursively(List<FileStatus> result,FileSystem fs,Path path,PathFilter inputFilter) throws IOException {

  for (  FileStatus stat : fs.listStatus(path,inputFilter)) {

    if (stat.isDirectory()) {

      addInputPathRecursively(result,fs,stat.getPath(),inputFilter);

    }

 else {

      result.add(stat);

    }

  }

}

Location: FileInputFormat.java

Content: 

/** 

 * Add the given comma separated paths to the list of inputs for the map-reduce job.

 * @param conf The configuration of the job 

 * @param commaSeparatedPaths Comma separated paths to be added tothe list of inputs for the map-reduce job.

 */

public static void addInputPaths(JobConf conf,String commaSeparatedPaths){

  for (  String str : getPathStrings(commaSeparatedPaths)) {

    addInputPath(conf,new Path(str));

  }

}

Location: FileInputFormat.java

Content: 

/** 

 * Add the given comma separated paths to the list of inputs for the map-reduce job.

 * @param job The job to modify

 * @param commaSeparatedPaths Comma separated paths to be added tothe list of inputs for the map-reduce job.

 */

public static void addInputPaths(Job job,String commaSeparatedPaths) throws IOException {

  for (  String str : getPathStrings(commaSeparatedPaths)) {

    addInputPath(job,new Path(str));

  }

}

Location: FileInputFormat.java

Content: 

/** 

 * Add a  {@link Path} to the list of inputs for the map-reduce job.

 * @param conf The configuration of the job 

 * @param path {@link Path} to be added to the list of inputs for the map-reduce job.

 */

public static void addInputPath(JobConf conf,Path path){

  path=new Path(conf.getWorkingDirectory(),path);

  String dirStr=StringUtils.escapeString(path.toString());

  String dirs=conf.get(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR);

  conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR,dirs == null ? dirStr : dirs + StringUtils.COMMA_STR + dirStr);

}

Location: FileInputFormat.java

Content: 

/** 

 * Add a  {@link Path} to the list of inputs for the map-reduce job.

 * @param job The {@link Job} to modify

 * @param path {@link Path} to be added to the list of inputs for the map-reduce job.

 */

public static void addInputPath(Job job,Path path) throws IOException {

  Configuration conf=job.getConfiguration();

  path=path.getFileSystem(conf).makeQualified(path);

  String dirStr=StringUtils.escapeString(path.toString());

  String dirs=conf.get(INPUT_DIR);

  conf.set(INPUT_DIR,dirs == null ? dirStr : dirs + "," + dirStr);

}

Location: FileInputFormat.java

Content: 

protected long computeSplitSize(long blockSize,long minSize,long maxSize){

  return Math.max(minSize,Math.min(maxSize,blockSize));

}

Location: FileInputFormat.java

Content: 

private String[] fakeRacks(BlockLocation[] blkLocations,int index) throws IOException {

  String[] allHosts=blkLocations[index].getHosts();

  String[] allTopos=new String[allHosts.length];

  for (int i=0; i < allHosts.length; i++) {

    allTopos[i]=NetworkTopology.DEFAULT_RACK + "/" + allHosts[i];

  }

  return allTopos;

}

Location: FileInputFormat.java

Content: 

protected int getBlockIndex(BlockLocation[] blkLocations,long offset){

  for (int i=0; i < blkLocations.length; i++) {

    if ((blkLocations[i].getOffset() <= offset) && (offset < blkLocations[i].getOffset() + blkLocations[i].getLength())) {

      return i;

    }

  }

  BlockLocation last=blkLocations[blkLocations.length - 1];

  long fileLength=last.getOffset() + last.getLength() - 1;

  throw new IllegalArgumentException("Offset " + offset + " is outside of file (0.."+ fileLength+ ")");

}

Location: FileInputFormat.java

Content: 

/** 

 * Get a PathFilter instance of the filter set for the input paths.

 * @return the PathFilter instance set for the job, NULL if none has been set.

 */

public static PathFilter getInputPathFilter(JobConf conf){

  Class<? extends PathFilter> filterClass=conf.getClass(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.PATHFILTER_CLASS,null,PathFilter.class);

  return (filterClass != null) ? ReflectionUtils.newInstance(filterClass,conf) : null;

}

Location: FileInputFormat.java

Content: 

/** 

 * Get a PathFilter instance of the filter set for the input paths.

 * @return the PathFilter instance set for the job, NULL if none has been set.

 */

public static PathFilter getInputPathFilter(JobContext context){

  Configuration conf=context.getConfiguration();

  Class<?> filterClass=conf.getClass(PATHFILTER_CLASS,null,PathFilter.class);

  return (filterClass != null) ? (PathFilter)ReflectionUtils.newInstance(filterClass,conf) : null;

}

Location: FileInputFormat.java

Content: 

/** 

 * Get the list of input  {@link Path}s for the map-reduce job.

 * @param conf The configuration of the job 

 * @return the list of input {@link Path}s for the map-reduce job.

 */

public static Path[] getInputPaths(JobConf conf){

  String dirs=conf.get(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR,"");

  String[] list=StringUtils.split(dirs);

  Path[] result=new Path[list.length];

  for (int i=0; i < list.length; i++) {

    result[i]=new Path(StringUtils.unEscapeString(list[i]));

  }

  return result;

}

Location: FileInputFormat.java

Content: 

/** 

 * Get the list of input  {@link Path}s for the map-reduce job.

 * @param context The job

 * @return the list of input {@link Path}s for the map-reduce job.

 */

public static Path[] getInputPaths(JobContext context){

  String dirs=context.getConfiguration().get(INPUT_DIR,"");

  String[] list=StringUtils.split(dirs);

  Path[] result=new Path[list.length];

  for (int i=0; i < list.length; i++) {

    result[i]=new Path(StringUtils.unEscapeString(list[i]));

  }

  return result;

}

Location: FileInputFormat.java

Content: 

/** 

 * Get the maximum split size.

 * @param context the job to look at.

 * @return the maximum number of bytes a split can include

 */

public static long getMaxSplitSize(JobContext context){

  return context.getConfiguration().getLong(SPLIT_MAXSIZE,Long.MAX_VALUE);

}

Location: FileInputFormat.java

Content: 

/** 

 * Get the minimum split size

 * @param job the job

 * @return the minimum number of bytes that can be in a split

 */

public static long getMinSplitSize(JobContext job){

  return job.getConfiguration().getLong(SPLIT_MINSIZE,1L);

}

Location: FileInputFormat.java

Content: 

private static String[] getPathStrings(String commaSeparatedPaths){

  int length=commaSeparatedPaths.length();

  int curlyOpen=0;

  int pathStart=0;

  boolean globPattern=false;

  List<String> pathStrings=new ArrayList<String>();

  for (int i=0; i < length; i++) {

    char ch=commaSeparatedPaths.charAt(i);

switch (ch) {

case '{':

{

        curlyOpen++;

        if (!globPattern) {

          globPattern=true;

        }

        break;

      }

case '}':

{

      curlyOpen--;

      if (curlyOpen == 0 && globPattern) {

        globPattern=false;

      }

      break;

    }

case ',':

{

    if (!globPattern) {

      pathStrings.add(commaSeparatedPaths.substring(pathStart,i));

      pathStart=i + 1;

    }

    break;

  }

}

}

pathStrings.add(commaSeparatedPaths.substring(pathStart,length));

return pathStrings.toArray(new String[0]);

}

Location: FileInputFormat.java

Content: 

/** 

 * This function identifies and returns the hosts that contribute  most for a given split. For calculating the contribution, rack locality is treated on par with host locality, so hosts from racks that contribute the most are preferred over hosts on racks that  contribute less

 * @param blkLocations The list of block locations

 * @param offset 

 * @param splitSize 

 * @return array of hosts that contribute most to this split

 * @throws IOException

 */

protected String[] getSplitHosts(BlockLocation[] blkLocations,long offset,long splitSize,NetworkTopology clusterMap) throws IOException {

  int startIndex=getBlockIndex(blkLocations,offset);

  long bytesInThisBlock=blkLocations[startIndex].getOffset() + blkLocations[startIndex].getLength() - offset;

  if (bytesInThisBlock >= splitSize) {

    return blkLocations[startIndex].getHosts();

  }

  long bytesInFirstBlock=bytesInThisBlock;

  int index=startIndex + 1;

  splitSize-=bytesInThisBlock;

  while (splitSize > 0) {

    bytesInThisBlock=Math.min(splitSize,blkLocations[index++].getLength());

    splitSize-=bytesInThisBlock;

  }

  long bytesInLastBlock=bytesInThisBlock;

  int endIndex=index - 1;

  Map<Node,NodeInfo> hostsMap=new IdentityHashMap<Node,NodeInfo>();

  Map<Node,NodeInfo> racksMap=new IdentityHashMap<Node,NodeInfo>();

  String[] allTopos=new String[0];

  for (index=startIndex; index <= endIndex; index++) {

    if (index == startIndex) {

      bytesInThisBlock=bytesInFirstBlock;

    }

 else     if (index == endIndex) {

      bytesInThisBlock=bytesInLastBlock;

    }

 else {

      bytesInThisBlock=blkLocations[index].getLength();

    }

    allTopos=blkLocations[index].getTopologyPaths();

    if (allTopos.length == 0) {

      allTopos=fakeRacks(blkLocations,index);

    }

    for (    String topo : allTopos) {

      Node node, parentNode;

      NodeInfo nodeInfo, parentNodeInfo;

      node=clusterMap.getNode(topo);

      if (node == null) {

        node=new NodeBase(topo);

        clusterMap.add(node);

      }

      nodeInfo=hostsMap.get(node);

      if (nodeInfo == null) {

        nodeInfo=new NodeInfo(node);

        hostsMap.put(node,nodeInfo);

        parentNode=node.getParent();

        parentNodeInfo=racksMap.get(parentNode);

        if (parentNodeInfo == null) {

          parentNodeInfo=new NodeInfo(parentNode);

          racksMap.put(parentNode,parentNodeInfo);

        }

        parentNodeInfo.addLeaf(nodeInfo);

      }

 else {

        nodeInfo=hostsMap.get(node);

        parentNode=node.getParent();

        parentNodeInfo=racksMap.get(parentNode);

      }

      nodeInfo.addValue(index,bytesInThisBlock);

      parentNodeInfo.addValue(index,bytesInThisBlock);

    }

  }

  return identifyHosts(allTopos.length,racksMap);

}

Location: FileInputFormat.java

Content: 

private String[] identifyHosts(int replicationFactor,Map<Node,NodeInfo> racksMap){

  String[] retVal=new String[replicationFactor];

  List<NodeInfo> rackList=new LinkedList<NodeInfo>();

  rackList.addAll(racksMap.values());

  sortInDescendingOrder(rackList);

  boolean done=false;

  int index=0;

  for (  NodeInfo ni : rackList) {

    Set<NodeInfo> hostSet=ni.getLeaves();

    List<NodeInfo> hostList=new LinkedList<NodeInfo>();

    hostList.addAll(hostSet);

    sortInDescendingOrder(hostList);

    for (    NodeInfo host : hostList) {

      retVal[index++]=host.node.getName().split(":")[0];

      if (index == replicationFactor) {

        done=true;

        break;

      }

    }

    if (done == true) {

      break;

    }

  }

  return retVal;

}

Location: FileInputFormat.java

Content: 

/** 

 * A factory that makes the split for this class. It can be overridden by sub-classes to make sub-types

 */

protected FileSplit makeSplit(Path file,long start,long length,String[] hosts){

  return new FileSplit(file,start,length,hosts);

}

Location: FileInputFormat.java

Content: 

/** 

 * Set a PathFilter to be applied to the input paths for the map-reduce job.

 * @param job the job to modify

 * @param filter the PathFilter class use for filtering the input paths.

 */

public static void setInputPathFilter(Job job,Class<? extends PathFilter> filter){

  job.getConfiguration().setClass(PATHFILTER_CLASS,filter,PathFilter.class);

}

Location: FileInputFormat.java

Content: 

/** 

 * Set a PathFilter to be applied to the input paths for the map-reduce job.

 * @param filter the PathFilter class use for filtering the input paths.

 */

public static void setInputPathFilter(JobConf conf,Class<? extends PathFilter> filter){

  conf.setClass(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.PATHFILTER_CLASS,filter,PathFilter.class);

}

Location: FileInputFormat.java

Content: 

/** 

 * Set the array of  {@link Path}s as the list of inputs for the map-reduce job.

 * @param conf Configuration of the job. 

 * @param inputPaths the {@link Path}s of the input directories/files  for the map-reduce job.

 */

public static void setInputPaths(JobConf conf,Path... inputPaths){

  Path path=new Path(conf.getWorkingDirectory(),inputPaths[0]);

  StringBuffer str=new StringBuffer(StringUtils.escapeString(path.toString()));

  for (int i=1; i < inputPaths.length; i++) {

    str.append(StringUtils.COMMA_STR);

    path=new Path(conf.getWorkingDirectory(),inputPaths[i]);

    str.append(StringUtils.escapeString(path.toString()));

  }

  conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR,str.toString());

}

Location: FileInputFormat.java

Content: 

/** 

 * Sets the given comma separated paths as the list of inputs  for the map-reduce job.

 * @param conf Configuration of the job

 * @param commaSeparatedPaths Comma separated paths to be set as the list of inputs for the map-reduce job.

 */

public static void setInputPaths(JobConf conf,String commaSeparatedPaths){

  setInputPaths(conf,StringUtils.stringToPath(getPathStrings(commaSeparatedPaths)));

}

Location: FileInputFormat.java

Content: 

/** 

 * Set the array of  {@link Path}s as the list of inputs for the map-reduce job.

 * @param job The job to modify 

 * @param inputPaths the {@link Path}s of the input directories/files  for the map-reduce job.

 */

public static void setInputPaths(Job job,Path... inputPaths) throws IOException {

  Configuration conf=job.getConfiguration();

  Path path=inputPaths[0].getFileSystem(conf).makeQualified(inputPaths[0]);

  StringBuffer str=new StringBuffer(StringUtils.escapeString(path.toString()));

  for (int i=1; i < inputPaths.length; i++) {

    str.append(StringUtils.COMMA_STR);

    path=inputPaths[i].getFileSystem(conf).makeQualified(inputPaths[i]);

    str.append(StringUtils.escapeString(path.toString()));

  }

  conf.set(INPUT_DIR,str.toString());

}

Location: FileInputFormat.java

Content: 

/** 

 * Sets the given comma separated paths as the list of inputs  for the map-reduce job.

 * @param job the job

 * @param commaSeparatedPaths Comma separated paths to be set as the list of inputs for the map-reduce job.

 */

public static void setInputPaths(Job job,String commaSeparatedPaths) throws IOException {

  setInputPaths(job,StringUtils.stringToPath(getPathStrings(commaSeparatedPaths)));

}

Location: FileInputFormat.java

Content: 

/** 

 * Set the maximum split size

 * @param job the job to modify

 * @param size the maximum split size

 */

public static void setMaxInputSplitSize(Job job,long size){

  job.getConfiguration().setLong(SPLIT_MAXSIZE,size);

}

Location: FileInputFormat.java

Content: 

/** 

 * Set the minimum input split size

 * @param job the job to modify

 * @param size the minimum size

 */

public static void setMinInputSplitSize(Job job,long size){

  job.getConfiguration().setLong(SPLIT_MINSIZE,size);

}

Location: FileInputFormat.java

Content: 

protected void setMinSplitSize(long minSplitSize){

  this.minSplitSize=minSplitSize;

}

Location: FileInputFormat.java

Content: 

private void sortInDescendingOrder(List<NodeInfo> mylist){

  Collections.sort(mylist,new Comparator<NodeInfo>(){

    public int compare(    NodeInfo obj1,    NodeInfo obj2){

      if (obj1 == null || obj2 == null)       return -1;

      if (obj1.getValue() == obj2.getValue()) {

        return 0;

      }

 else {

        return ((obj1.getValue() < obj2.getValue()) ? 1 : -1);

      }

    }

  }

);

}

