Location: ReadingHDFSFilesAsSideEffect.java

Content: 

@Override public double evaluate(JobStatistics job){

  this._job=job;

  double normF=getInputElementDoubleValue("NormalizationFactor",2.0);

  if (job.getLongValue(JobKeys.MAP_INPUT_BYTES) == 0 && job.getLongValue(JobKeys.HDFS_BYTES_READ) != 0) {

    return (double)1;

  }

  if (job.getLongValue(JobKeys.HDFS_BYTES_READ) == 0) {

    return (double)0;

  }

  this._impact=(job.getLongValue(JobKeys.HDFS_BYTES_READ) / job.getLongValue(JobKeys.MAP_INPUT_BYTES));

  if (this._impact >= normF) {

    this._impact=1;

  }

 else {

    this._impact=this._impact / normF;

  }

  return this._impact;

}

Location: ReadingHDFSFilesAsSideEffect.java

Content: 

@Override public String getPrescription(){

  return "Map and/or Reduce tasks are reading application specific files from HDFS. Make sure the replication factor\n" + "of these HDFS files is high enough to avoid the data reading bottleneck. Typically replication factor\n" + "can be square root of map/reduce tasks capacity of the allocated cluster.";

}

Location: ReadingHDFSFilesAsSideEffect.java

Content: 

@Override public String getReferenceDetails(){

  String ref="* Total HDFS Bytes read: " + this._job.getLongValue(JobKeys.HDFS_BYTES_READ) + "\n"+ "* Total Map Input Bytes read: "+ this._job.getLongValue(JobKeys.MAP_INPUT_BYTES)+ "\n"+ "* Impact: "+ truncate(this._impact);

  return ref;

}

Location: ReadingHDFSFilesAsSideEffect.java

Content: 

/** 

 */

public ReadingHDFSFilesAsSideEffect(){

}

