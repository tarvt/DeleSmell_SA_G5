Location: JobSubmitter.java

Content: 

/** 

 * Enqueue the job to be submitted per the deadline associated with it.

 */

public void add(final GridmixJob job) throws InterruptedException {

  final boolean addToQueue=!shutdown;

  if (addToQueue) {

    final SubmitTask task=new SubmitTask(job);

    sem.acquire();

    try {

      sched.execute(task);

    }

 catch (    RejectedExecutionException e) {

      sem.release();

    }

  }

}

Location: JobSubmitter.java

Content: 

private void checkSpecs(Job job) throws ClassNotFoundException, InterruptedException, IOException {

  JobConf jConf=(JobConf)job.getConfiguration();

  if (jConf.getNumReduceTasks() == 0 ? jConf.getUseNewMapper() : jConf.getUseNewReducer()) {

    org.apache.hadoop.mapreduce.OutputFormat<?,?> output=ReflectionUtils.newInstance(job.getOutputFormatClass(),job.getConfiguration());

    output.checkOutputSpecs(job);

  }

 else {

    jConf.getOutputFormat().checkOutputSpecs(jtFs,jConf);

  }

}

Location: JobSubmitter.java

Content: 

private boolean compareFs(FileSystem srcFs,FileSystem destFs){

  URI srcUri=srcFs.getUri();

  URI dstUri=destFs.getUri();

  if (srcUri.getScheme() == null) {

    return false;

  }

  if (!srcUri.getScheme().equals(dstUri.getScheme())) {

    return false;

  }

  String srcHost=srcUri.getHost();

  String dstHost=dstUri.getHost();

  if ((srcHost != null) && (dstHost != null)) {

    try {

      srcHost=InetAddress.getByName(srcHost).getCanonicalHostName();

      dstHost=InetAddress.getByName(dstHost).getCanonicalHostName();

    }

 catch (    UnknownHostException ue) {

      return false;

    }

    if (!srcHost.equals(dstHost)) {

      return false;

    }

  }

 else   if (srcHost == null && dstHost != null) {

    return false;

  }

 else   if (srcHost != null && dstHost == null) {

    return false;

  }

  if (srcUri.getPort() != dstUri.getPort()) {

    return false;

  }

  return true;

}

Location: JobSubmitter.java

Content: 

/** 

 * configure the jobconf of the user with the command line options of  -libjars, -files, -archives.

 * @param conf

 * @throws IOException

 */

private void copyAndConfigureFiles(Job job,Path jobSubmitDir) throws IOException {

  Configuration conf=job.getConfiguration();

  short replication=(short)conf.getInt(Job.SUBMIT_REPLICATION,10);

  copyAndConfigureFiles(job,jobSubmitDir,replication);

  if (job.getWorkingDirectory() == null) {

    job.setWorkingDirectory(jtFs.getWorkingDirectory());

  }

}

Location: JobSubmitter.java

Content: 

private void copyAndConfigureFiles(Job job,Path submitJobDir,short replication) throws IOException {

  Configuration conf=job.getConfiguration();

  if (!(conf.getBoolean(Job.USED_GENERIC_PARSER,false))) {

    LOG.warn("Use GenericOptionsParser for parsing the arguments. " + "Applications should implement Tool for the same.");

  }

  String files=conf.get("tmpfiles");

  String libjars=conf.get("tmpjars");

  String archives=conf.get("tmparchives");

  String jobJar=job.getJar();

  LOG.debug("default FileSystem: " + jtFs.getUri());

  if (jtFs.exists(submitJobDir)) {

    throw new IOException("Not submitting job. Job directory " + submitJobDir + " already exists!! This is unexpected.Please check what's there in"+ " that directory");

  }

  submitJobDir=jtFs.makeQualified(submitJobDir);

  submitJobDir=new Path(submitJobDir.toUri().getPath());

  FsPermission mapredSysPerms=new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);

  FileSystem.mkdirs(jtFs,submitJobDir,mapredSysPerms);

  Path filesDir=JobSubmissionFiles.getJobDistCacheFiles(submitJobDir);

  Path archivesDir=JobSubmissionFiles.getJobDistCacheArchives(submitJobDir);

  Path libjarsDir=JobSubmissionFiles.getJobDistCacheLibjars(submitJobDir);

  if (files != null) {

    FileSystem.mkdirs(jtFs,filesDir,mapredSysPerms);

    String[] fileArr=files.split(",");

    for (    String tmpFile : fileArr) {

      URI tmpURI=null;

      try {

        tmpURI=new URI(tmpFile);

      }

 catch (      URISyntaxException e) {

        throw new IllegalArgumentException(e);

      }

      Path tmp=new Path(tmpURI);

      Path newPath=copyRemoteFiles(filesDir,tmp,conf,replication);

      try {

        URI pathURI=getPathURI(newPath,tmpURI.getFragment());

        DistributedCache.addCacheFile(pathURI,conf);

      }

 catch (      URISyntaxException ue) {

        throw new IOException("Failed to create uri for " + tmpFile,ue);

      }

      DistributedCache.createSymlink(conf);

    }

  }

  if (libjars != null) {

    FileSystem.mkdirs(jtFs,libjarsDir,mapredSysPerms);

    String[] libjarsArr=libjars.split(",");

    for (    String tmpjars : libjarsArr) {

      Path tmp=new Path(tmpjars);

      Path newPath=copyRemoteFiles(libjarsDir,tmp,conf,replication);

      DistributedCache.addFileToClassPath(new Path(newPath.toUri().getPath()),conf);

    }

  }

  if (archives != null) {

    FileSystem.mkdirs(jtFs,archivesDir,mapredSysPerms);

    String[] archivesArr=archives.split(",");

    for (    String tmpArchives : archivesArr) {

      URI tmpURI;

      try {

        tmpURI=new URI(tmpArchives);

      }

 catch (      URISyntaxException e) {

        throw new IllegalArgumentException(e);

      }

      Path tmp=new Path(tmpURI);

      Path newPath=copyRemoteFiles(archivesDir,tmp,conf,replication);

      try {

        URI pathURI=getPathURI(newPath,tmpURI.getFragment());

        DistributedCache.addCacheArchive(pathURI,conf);

      }

 catch (      URISyntaxException ue) {

        throw new IOException("Failed to create uri for " + tmpArchives,ue);

      }

      DistributedCache.createSymlink(conf);

    }

  }

  if (jobJar != null) {

    if ("".equals(job.getJobName())) {

      job.setJobName(new Path(jobJar).getName());

    }

    copyJar(new Path(jobJar),JobSubmissionFiles.getJobJar(submitJobDir),replication);

    job.setJar(JobSubmissionFiles.getJobJar(submitJobDir).toString());

  }

 else {

    LOG.warn("No job jar file set.  User classes may not be found. " + "See Job or Job#setJar(String).");

  }

  ClientDistributedCacheManager.determineTimestamps(conf);

  ClientDistributedCacheManager.determineCacheVisibilities(conf);

  ClientDistributedCacheManager.getDelegationTokens(conf,job.getCredentials());

}

Location: JobSubmitter.java

Content: 

private void copyJar(Path originalJarPath,Path submitJarFile,short replication) throws IOException {

  jtFs.copyFromLocalFile(originalJarPath,submitJarFile);

  jtFs.setReplication(submitJarFile,replication);

  jtFs.setPermission(submitJarFile,new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION));

}

Location: JobSubmitter.java

Content: 

private Path copyRemoteFiles(Path parentDir,Path originalPath,Configuration conf,short replication) throws IOException {

  FileSystem remoteFs=null;

  remoteFs=originalPath.getFileSystem(conf);

  if (compareFs(remoteFs,jtFs)) {

    return originalPath;

  }

  Path newPath=new Path(parentDir,originalPath.getName());

  FileUtil.copy(remoteFs,originalPath,jtFs,newPath,false,conf);

  jtFs.setReplication(newPath,replication);

  return newPath;

}

Location: JobSubmitter.java

Content: 

private URI getPathURI(Path destPath,String fragment) throws URISyntaxException {

  URI pathURI=destPath.toUri();

  if (pathURI.getFragment() == null) {

    if (fragment == null) {

      pathURI=new URI(pathURI.toString() + "#" + destPath.getName());

    }

 else {

      pathURI=new URI(pathURI.toString() + "#" + fragment);

    }

  }

  return pathURI;

}

Location: JobSubmitter.java

Content: 

JobSubmitter(FileSystem submitFs,ClientProtocol submitClient) throws IOException {

  this.submitClient=submitClient;

  this.jtFs=submitFs;

}

Location: JobSubmitter.java

Content: 

/** 

 * Initialize the submission component with downstream monitor and pool of files from which split data may be read.

 * @param monitor Monitor component to which jobs should be passed

 * @param threads Number of submission threadsSee  {@link Gridmix#GRIDMIX_SUB_THR}.

 * @param queueDepth Max depth of pending work queueSee  {@link Gridmix#GRIDMIX_QUE_DEP}.

 * @param inputDir Set of files from which split data may be mined forsynthetic jobs.

 * @param statistics

 */

public JobSubmitter(JobMonitor monitor,int threads,int queueDepth,FilePool inputDir,Statistics statistics){

  sem=new Semaphore(queueDepth);

  sched=new ThreadPoolExecutor(threads,threads,0L,TimeUnit.MILLISECONDS,new LinkedBlockingQueue<Runnable>());

  this.inputDir=inputDir;

  this.monitor=monitor;

  this.statistics=statistics;

}

Location: JobSubmitter.java

Content: 

@SuppressWarnings("unchecked") private void populateTokenCache(Configuration conf,Credentials credentials) throws IOException {

  readTokensFromFiles(conf,credentials);

  String[] nameNodes=conf.getStrings(MRJobConfig.JOB_NAMENODES);

  LOG.debug("adding the following namenodes' delegation tokens:" + Arrays.toString(nameNodes));

  if (nameNodes != null) {

    Path[] ps=new Path[nameNodes.length];

    for (int i=0; i < nameNodes.length; i++) {

      ps[i]=new Path(nameNodes[i]);

    }

    TokenCache.obtainTokensForNamenodes(credentials,ps,conf);

  }

}

Location: JobSubmitter.java

Content: 

@SuppressWarnings("unchecked") private void printTokens(JobID jobId,Credentials credentials) throws IOException {

  if (LOG.isDebugEnabled()) {

    LOG.debug("Printing tokens for job: " + jobId);

    for (    Token<?> token : credentials.getAllTokens()) {

      if (token.getKind().toString().equals("HDFS_DELEGATION_TOKEN")) {

        LOG.debug("Submitting with " + DFSClient.stringifyToken((Token<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier>)token));

      }

    }

  }

}

Location: JobSubmitter.java

Content: 

@SuppressWarnings("unchecked") private void readTokensFromFiles(Configuration conf,Credentials credentials) throws IOException {

  String binaryTokenFilename=conf.get("mapreduce.job.credentials.binary");

  if (binaryTokenFilename != null) {

    Credentials binary=Credentials.readTokenStorageFile(new Path("file:///" + binaryTokenFilename),conf);

    credentials.addAll(binary);

  }

  String tokensFileName=conf.get("mapreduce.job.credentials.json");

  if (tokensFileName != null) {

    LOG.info("loading user's secret keys from " + tokensFileName);

    String localFileName=new Path(tokensFileName).toUri().getPath();

    boolean json_error=false;

    try {

      ObjectMapper mapper=new ObjectMapper();

      Map<String,String> nm=mapper.readValue(new File(localFileName),Map.class);

      for (      Map.Entry<String,String> ent : nm.entrySet()) {

        credentials.addSecretKey(new Text(ent.getKey()),ent.getValue().getBytes());

      }

    }

 catch (    JsonMappingException e) {

      json_error=true;

    }

catch (    JsonParseException e) {

      json_error=true;

    }

    if (json_error)     LOG.warn("couldn't parse Token Cache JSON file with user secret keys");

  }

}

Location: JobSubmitter.java

Content: 

/** 

 * (Re)scan the set of input files from which splits are derived.

 * @throws java.io.IOException

 */

public void refreshFilePool() throws IOException {

  inputDir.refresh();

}

Location: JobSubmitter.java

Content: 

/** 

 * Internal method for submitting jobs to the system. <p>The job submission process involves: <ol> <li> Checking the input and output specifications of the job. </li> <li> Computing the  {@link InputSplit}s for the job. </li> <li> Setup the requisite accounting information for the  {@link DistributedCache} of the job, if necessary.</li> <li> Copying the job's jar and configuration to the map-reduce system directory on the distributed file-system.  </li> <li> Submitting the job to the <code>JobTracker</code> and optionally monitoring it's status. </li> </ol></p>

 * @param job the configuration to submit

 * @param cluster the handle to the Cluster

 * @throws ClassNotFoundException

 * @throws InterruptedException

 * @throws IOException

 */

@SuppressWarnings("unchecked") JobStatus submitJobInternal(Job job,Cluster cluster) throws ClassNotFoundException, InterruptedException, IOException {

  Path jobStagingArea=JobSubmissionFiles.getStagingDir(cluster,job.getConfiguration());

  Configuration conf=job.getConfiguration();

  InetAddress ip=InetAddress.getLocalHost();

  if (ip != null) {

    submitHostAddress=ip.getHostAddress();

    submitHostName=ip.getHostName();

    conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);

    conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);

  }

  JobID jobId=submitClient.getNewJobID();

  job.setJobID(jobId);

  Path submitJobDir=new Path(jobStagingArea,jobId.toString());

  JobStatus status=null;

  try {

    conf.set("mapreduce.job.dir",submitJobDir.toString());

    LOG.debug("Configuring job " + jobId + " with "+ submitJobDir+ " as the submit dir");

    TokenCache.obtainTokensForNamenodes(job.getCredentials(),new Path[]{submitJobDir},conf);

    populateTokenCache(conf,job.getCredentials());

    copyAndConfigureFiles(job,submitJobDir);

    Path submitJobFile=JobSubmissionFiles.getJobConfPath(submitJobDir);

    checkSpecs(job);

    LOG.debug("Creating splits at " + jtFs.makeQualified(submitJobDir));

    int maps=writeSplits(job,submitJobDir);

    conf.setInt(MRJobConfig.NUM_MAPS,maps);

    LOG.info("number of splits:" + maps);

    String queue=conf.get(MRJobConfig.QUEUE_NAME,JobConf.DEFAULT_QUEUE_NAME);

    AccessControlList acl=submitClient.getQueueAdmins(queue);

    conf.set(toFullPropertyName(queue,QueueACL.ADMINISTER_JOBS.getAclName()),acl.getAclString());

    writeConf(conf,submitJobFile);

    printTokens(jobId,job.getCredentials());

    status=submitClient.submitJob(jobId,submitJobDir.toString(),job.getCredentials());

    if (status != null) {

      return status;

    }

 else {

      throw new IOException("Could not launch job");

    }

  }

  finally {

    if (status == null) {

      LOG.info("Cleaning up the staging area " + submitJobDir);

      if (jtFs != null && submitJobDir != null)       jtFs.delete(submitJobDir,true);

    }

  }

}

Location: JobSubmitter.java

Content: 

private void writeConf(Configuration conf,Path jobFile) throws IOException {

  FSDataOutputStream out=FileSystem.create(jtFs,jobFile,new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION));

  try {

    conf.writeXml(out);

  }

  finally {

    out.close();

  }

}

Location: JobSubmitter.java

Content: 

@SuppressWarnings("unchecked") private <T extends InputSplit>int writeNewSplits(JobContext job,Path jobSubmitDir) throws IOException, InterruptedException, ClassNotFoundException {

  Configuration conf=job.getConfiguration();

  InputFormat<?,?> input=ReflectionUtils.newInstance(job.getInputFormatClass(),conf);

  List<InputSplit> splits=input.getSplits(job);

  T[] array=(T[])splits.toArray(new InputSplit[splits.size()]);

  Arrays.sort(array,new SplitComparator());

  JobSplitWriter.createSplitFiles(jobSubmitDir,conf,jobSubmitDir.getFileSystem(conf),array);

  return array.length;

}

Location: JobSubmitter.java

Content: 

private int writeOldSplits(JobConf job,Path jobSubmitDir) throws IOException {

  org.apache.hadoop.mapred.InputSplit[] splits=job.getInputFormat().getSplits(job,job.getNumMapTasks());

  Arrays.sort(splits,new Comparator<org.apache.hadoop.mapred.InputSplit>(){

    public int compare(    org.apache.hadoop.mapred.InputSplit a,    org.apache.hadoop.mapred.InputSplit b){

      try {

        long left=a.getLength();

        long right=b.getLength();

        if (left == right) {

          return 0;

        }

 else         if (left < right) {

          return 1;

        }

 else {

          return -1;

        }

      }

 catch (      IOException ie) {

        throw new RuntimeException("Problem getting input split size",ie);

      }

    }

  }

);

  JobSplitWriter.createSplitFiles(jobSubmitDir,job,jobSubmitDir.getFileSystem(job),splits);

  return splits.length;

}

Location: JobSubmitter.java

Content: 

private int writeSplits(org.apache.hadoop.mapreduce.JobContext job,Path jobSubmitDir) throws IOException, InterruptedException, ClassNotFoundException {

  JobConf jConf=(JobConf)job.getConfiguration();

  int maps;

  if (jConf.getUseNewMapper()) {

    maps=writeNewSplits(job,jobSubmitDir);

  }

 else {

    maps=writeOldSplits(jConf,jobSubmitDir);

  }

  return maps;

}

