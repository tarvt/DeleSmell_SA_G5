Location: FileOutputFormat.java

Content: 

/** 

 * Is the job output compressed?

 * @param conf the {@link JobConf} to look in

 * @return <code>true</code> if the job output should be compressed,<code>false</code> otherwise

 */

public static boolean getCompressOutput(JobConf conf){

  return conf.getBoolean(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS,false);

}

Location: FileOutputFormat.java

Content: 

/** 

 * Is the job output compressed?

 * @param job the Job to look in

 * @return <code>true</code> if the job output should be compressed,<code>false</code> otherwise

 */

public static boolean getCompressOutput(JobContext job){

  return job.getConfiguration().getBoolean(FileOutputFormat.COMPRESS,false);

}

Location: FileOutputFormat.java

Content: 

/** 

 * Get the default path and filename for the output format.

 * @param context the task context

 * @param extension an extension to add to the filename

 * @return a full path $output/_temporary/$taskid/part-[mr]-$id

 * @throws IOException

 */

public Path getDefaultWorkFile(TaskAttemptContext context,String extension) throws IOException {

  FileOutputCommitter committer=(FileOutputCommitter)getOutputCommitter(context);

  return new Path(committer.getWorkPath(),getUniqueFile(context,getOutputName(context),extension));

}

Location: FileOutputFormat.java

Content: 

/** 

 * Get the  {@link CompressionCodec} for compressing the job outputs.

 * @param conf the {@link JobConf} to look in

 * @param defaultValue the {@link CompressionCodec} to return if not set

 * @return the {@link CompressionCodec} to be used to compress the job outputs

 * @throws IllegalArgumentException if the class was specified, but not found

 */

public static Class<? extends CompressionCodec> getOutputCompressorClass(JobConf conf,Class<? extends CompressionCodec> defaultValue){

  Class<? extends CompressionCodec> codecClass=defaultValue;

  String name=conf.get(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS_CODEC);

  if (name != null) {

    try {

      codecClass=conf.getClassByName(name).asSubclass(CompressionCodec.class);

    }

 catch (    ClassNotFoundException e) {

      throw new IllegalArgumentException("Compression codec " + name + " was not found.",e);

    }

  }

  return codecClass;

}

Location: FileOutputFormat.java

Content: 

/** 

 * Get the  {@link CompressionCodec} for compressing the job outputs.

 * @param job the {@link Job} to look in

 * @param defaultValue the {@link CompressionCodec} to return if not set

 * @return the {@link CompressionCodec} to be used to compress the job outputs

 * @throws IllegalArgumentException if the class was specified, but not found

 */

public static Class<? extends CompressionCodec> getOutputCompressorClass(JobContext job,Class<? extends CompressionCodec> defaultValue){

  Class<? extends CompressionCodec> codecClass=defaultValue;

  Configuration conf=job.getConfiguration();

  String name=conf.get(FileOutputFormat.COMPRESS_CODEC);

  if (name != null) {

    try {

      codecClass=conf.getClassByName(name).asSubclass(CompressionCodec.class);

    }

 catch (    ClassNotFoundException e) {

      throw new IllegalArgumentException("Compression codec " + name + " was not found.",e);

    }

  }

  return codecClass;

}

Location: FileOutputFormat.java

Content: 

/** 

 * Get the base output name for the output file.

 */

protected static String getOutputName(JobContext job){

  return job.getConfiguration().get(BASE_OUTPUT_NAME,PART);

}

Location: FileOutputFormat.java

Content: 

/** 

 * Get the  {@link Path} to the output directory for the map-reduce job.

 * @return the {@link Path} to the output directory for the map-reduce job.

 * @see FileOutputFormat#getWorkOutputPath(JobConf)

 */

public static Path getOutputPath(JobConf conf){

  String name=conf.get(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.OUTDIR);

  return name == null ? null : new Path(name);

}

Location: FileOutputFormat.java

Content: 

/** 

 * Get the  {@link Path} to the output directory for the map-reduce job.

 * @return the {@link Path} to the output directory for the map-reduce job.

 * @see FileOutputFormat#getWorkOutputPath(TaskInputOutputContext)

 */

public static Path getOutputPath(JobContext job){

  String name=job.getConfiguration().get(FileOutputFormat.OUTDIR);

  return name == null ? null : new Path(name);

}

Location: FileOutputFormat.java

Content: 

/** 

 * Helper function to generate a  {@link Path} for a file that is unique forthe task within the job output directory. <p>The path can be used to create custom files from within the map and reduce tasks. The path name will be unique for each task. The path parent will be the job output directory.</p>ls <p>This method uses the  {@link #getUniqueName} method to make the file nameunique for the task.</p>

 * @param conf the configuration for the job.

 * @param name the name for the file.

 * @return a unique path accross all tasks of the job.

 */

public static Path getPathForCustomFile(JobConf conf,String name){

  return new Path(getWorkOutputPath(conf),getUniqueName(conf,name));

}

Location: FileOutputFormat.java

Content: 

/** 

 * Helper function to generate a  {@link Path} for a file that is unique forthe task within the job output directory. <p>The path can be used to create custom files from within the map and reduce tasks. The path name will be unique for each task. The path parent will be the job output directory.</p>ls <p>This method uses the  {@link #getUniqueFile} method to make the file nameunique for the task.</p>

 * @param context the context for the task.

 * @param name the name for the file.

 * @param extension the extension for the file

 * @return a unique path accross all tasks of the job.

 */

public static Path getPathForWorkFile(TaskInputOutputContext<?,?,?,?> context,String name,String extension) throws IOException, InterruptedException {

  return new Path(getWorkOutputPath(context),getUniqueFile(context,name,extension));

}

Location: FileOutputFormat.java

Content: 

/** 

 * Helper function to create the task's temporary output directory and  return the path to the task's output file.

 * @param conf job-configuration

 * @param name temporary task-output filename

 * @return path to the task's temporary output file

 * @throws IOException

 */

public static Path getTaskOutputPath(JobConf conf,String name) throws IOException {

  Path outputPath=getOutputPath(conf);

  if (outputPath == null) {

    throw new IOException("Undefined job output-path");

  }

  OutputCommitter committer=conf.getOutputCommitter();

  Path workPath=outputPath;

  TaskAttemptContext context=new TaskAttemptContextImpl(conf,TaskAttemptID.forName(conf.get(JobContext.TASK_ATTEMPT_ID)));

  if (committer instanceof FileOutputCommitter) {

    workPath=((FileOutputCommitter)committer).getWorkPath(context,outputPath);

  }

  return new Path(workPath,name);

}

Location: FileOutputFormat.java

Content: 

/** 

 * Generate a unique filename, based on the task id, name, and extension

 * @param context the task that is calling this

 * @param name the base filename

 * @param extension the filename extension

 * @return a string like $name-[mrsct]-$id$extension

 */

public synchronized static String getUniqueFile(TaskAttemptContext context,String name,String extension){

  TaskID taskId=context.getTaskAttemptID().getTaskID();

  int partition=taskId.getId();

  StringBuilder result=new StringBuilder();

  result.append(name);

  result.append('-');

  result.append(TaskID.getRepresentingCharacter(taskId.getTaskType()));

  result.append('-');

  result.append(NUMBER_FORMAT.format(partition));

  result.append(extension);

  return result.toString();

}

Location: FileOutputFormat.java

Content: 

/** 

 * Helper function to generate a name that is unique for the task. <p>The generated name can be used to create custom files from within the different tasks for the job, the names for different tasks will not collide with each other.</p> <p>The given name is postfixed with the task type, 'm' for maps, 'r' for reduces and the task partition number. For example, give a name 'test' running on the first map o the job the generated name will be 'test-m-00000'.</p>

 * @param conf the configuration for the job.

 * @param name the name to make unique.

 * @return a unique name accross all tasks of the job.

 */

public static String getUniqueName(JobConf conf,String name){

  int partition=conf.getInt(JobContext.TASK_PARTITION,-1);

  if (partition == -1) {

    throw new IllegalArgumentException("This method can only be called from within a Job");

  }

  String taskType=(conf.getBoolean(JobContext.TASK_ISMAP,true)) ? "m" : "r";

  NumberFormat numberFormat=NumberFormat.getInstance();

  numberFormat.setMinimumIntegerDigits(5);

  numberFormat.setGroupingUsed(false);

  return name + "-" + taskType+ "-"+ numberFormat.format(partition);

}

Location: FileOutputFormat.java

Content: 

/** 

 * Get the  {@link Path} to the task's temporary output directory for the map-reduce job <h4 id="SideEffectFiles">Tasks' Side-Effect Files</h4> <p><i>Note:</i> The following is valid only if the  {@link OutputCommitter}is  {@link FileOutputCommitter}. If <code>OutputCommitter</code> is not  a <code>FileOutputCommitter</code>, the task's temporary output directory is same as  {@link #getOutputPath(JobConf)} i.e.<tt>${mapreduce.output.fileoutputformat.outputdir}$</tt></p> <p>Some applications need to create/write-to side-files, which differ from the actual job-outputs. <p>In such cases there could be issues with 2 instances of the same TIP  (running simultaneously e.g. speculative tasks) trying to open/write-to the same file (path) on HDFS. Hence the application-writer will have to pick  unique names per task-attempt (e.g. using the attemptid, say  <tt>attempt_200709221812_0001_m_000000_0</tt>), not just per TIP.</p>  <p>To get around this the Map-Reduce framework helps the application-writer  out by maintaining a special  <tt>${mapreduce.output.fileoutputformat.outputdir}/_temporary/_${taskid}</tt>  sub-directory for each task-attempt on HDFS where the output of the  task-attempt goes. On successful completion of the task-attempt the files  in the <tt>${mapreduce.output.fileoutputformat.outputdir}/_temporary/_${taskid}</tt> (only)  are <i>promoted</i> to <tt>${mapreduce.output.fileoutputformat.outputdir}</tt>. Of course, the  framework discards the sub-directory of unsuccessful task-attempts. This  is completely transparent to the application.</p> <p>The application-writer can take advantage of this by creating any  side-files required in <tt>${mapreduce.task.output.dir}</tt> during execution  of his reduce-task i.e. via  {@link #getWorkOutputPath(JobConf)}, and the  framework will move them out similarly - thus she doesn't have to pick  unique paths per task-attempt.</p> <p><i>Note</i>: the value of <tt>${mapreduce.task.output.dir}</tt> during  execution of a particular task-attempt is actually  <tt>${mapreduce.output.fileoutputformat.outputdir}/_temporary/_{$taskid}</tt>, and this value is  set by the map-reduce framework. So, just create any side-files in the  path  returned by  {@link #getWorkOutputPath(JobConf)} from map/reduce task to take advantage of this feature.</p> <p>The entire discussion holds true for maps of jobs with  reducer=NONE (i.e. 0 reduces) since output of the map, in that case,  goes directly to HDFS.</p> 

 * @return the {@link Path} to the task's temporary output directory for the map-reduce job.

 */

public static Path getWorkOutputPath(JobConf conf){

  String name=conf.get(JobContext.TASK_OUTPUT_DIR);

  return name == null ? null : new Path(name);

}

Location: FileOutputFormat.java

Content: 

/** 

 * Get the  {@link Path} to the task's temporary output directory for the map-reduce job <h4 id="SideEffectFiles">Tasks' Side-Effect Files</h4> <p>Some applications need to create/write-to side-files, which differ from the actual job-outputs. <p>In such cases there could be issues with 2 instances of the same TIP  (running simultaneously e.g. speculative tasks) trying to open/write-to the same file (path) on HDFS. Hence the application-writer will have to pick  unique names per task-attempt (e.g. using the attemptid, say  <tt>attempt_200709221812_0001_m_000000_0</tt>), not just per TIP.</p>  <p>To get around this the Map-Reduce framework helps the application-writer  out by maintaining a special  <tt>${mapreduce.output.fileoutputformat.outputdir}/_temporary/_${taskid}</tt>  sub-directory for each task-attempt on HDFS where the output of the  task-attempt goes. On successful completion of the task-attempt the files  in the <tt>${mapreduce.output.fileoutputformat.outputdir}/_temporary/_${taskid}</tt> (only)  are <i>promoted</i> to <tt>${mapreduce.output.fileoutputformat.outputdir}</tt>. Of course, the  framework discards the sub-directory of unsuccessful task-attempts. This  is completely transparent to the application.</p> <p>The application-writer can take advantage of this by creating any  side-files required in a work directory during execution  of his task i.e. via  {@link #getWorkOutputPath(TaskInputOutputContext)}, and the framework will move them out similarly - thus she doesn't have to pick  unique paths per task-attempt.</p> <p>The entire discussion holds true for maps of jobs with  reducer=NONE (i.e. 0 reduces) since output of the map, in that case,  goes directly to HDFS.</p> 

 * @return the {@link Path} to the task's temporary output directory for the map-reduce job.

 */

public static Path getWorkOutputPath(TaskInputOutputContext<?,?,?,?> context) throws IOException, InterruptedException {

  FileOutputCommitter committer=(FileOutputCommitter)context.getOutputCommitter();

  return committer.getWorkPath();

}

Location: FileOutputFormat.java

Content: 

/** 

 * Set whether the output of the job is compressed.

 * @param job the job to modify

 * @param compress should the output of the job be compressed?

 */

public static void setCompressOutput(Job job,boolean compress){

  job.getConfiguration().setBoolean(FileOutputFormat.COMPRESS,compress);

}

Location: FileOutputFormat.java

Content: 

/** 

 * Set whether the output of the job is compressed.

 * @param conf the {@link JobConf} to modify

 * @param compress should the output of the job be compressed?

 */

public static void setCompressOutput(JobConf conf,boolean compress){

  conf.setBoolean(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS,compress);

}

Location: FileOutputFormat.java

Content: 

/** 

 * Set the  {@link CompressionCodec} to be used to compress job outputs.

 * @param job the job to modify

 * @param codecClass the {@link CompressionCodec} to be used tocompress the job outputs

 */

public static void setOutputCompressorClass(Job job,Class<? extends CompressionCodec> codecClass){

  setCompressOutput(job,true);

  job.getConfiguration().setClass(FileOutputFormat.COMPRESS_CODEC,codecClass,CompressionCodec.class);

}

Location: FileOutputFormat.java

Content: 

/** 

 * Set the  {@link CompressionCodec} to be used to compress job outputs.

 * @param conf the {@link JobConf} to modify

 * @param codecClass the {@link CompressionCodec} to be used tocompress the job outputs

 */

public static void setOutputCompressorClass(JobConf conf,Class<? extends CompressionCodec> codecClass){

  setCompressOutput(conf,true);

  conf.setClass(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS_CODEC,codecClass,CompressionCodec.class);

}

Location: FileOutputFormat.java

Content: 

/** 

 * Set the base output name for output file to be created.

 */

protected static void setOutputName(JobContext job,String name){

  job.getConfiguration().set(BASE_OUTPUT_NAME,name);

}

Location: FileOutputFormat.java

Content: 

/** 

 * Set the  {@link Path} of the output directory for the map-reduce job.

 * @param conf The configuration of the job.

 * @param outputDir the {@link Path} of the output directory for the map-reduce job.

 */

public static void setOutputPath(JobConf conf,Path outputDir){

  outputDir=new Path(conf.getWorkingDirectory(),outputDir);

  conf.set(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.OUTDIR,outputDir.toString());

}

Location: FileOutputFormat.java

Content: 

/** 

 * Set the  {@link Path} of the output directory for the map-reduce job.

 * @param job The job to modify

 * @param outputDir the {@link Path} of the output directory for the map-reduce job.

 */

public static void setOutputPath(Job job,Path outputDir){

  job.getConfiguration().set(FileOutputFormat.OUTDIR,outputDir.toString());

}

Location: FileOutputFormat.java

Content: 

/** 

 * Set the  {@link Path} of the task's temporary output directory for the map-reduce job. <p><i>Note</i>: Task output path is set by the framework. </p>

 * @param conf The configuration of the job.

 * @param outputDir the {@link Path} of the output directory for the map-reduce job.

 */

static void setWorkOutputPath(JobConf conf,Path outputDir){

  outputDir=new Path(conf.getWorkingDirectory(),outputDir);

  conf.set(JobContext.TASK_OUTPUT_DIR,outputDir.toString());

}

