Location: TestBlockPlacementPolicyRaid.java

Content: 

private List<LocatedBlock> getBlocks(FSNamesystem namesystem,String file) throws IOException {

  long len=namesystem.getFileInfo(file,true).getLen();

  return namesystem.getBlockLocations(file,0,len,false,false).getLocatedBlocks();

}

Location: TestBlockPlacementPolicyRaid.java

Content: 

private Collection<LocatedBlock> getCompanionBlocks(FSNamesystem namesystem,BlockPlacementPolicyRaid policy,ExtendedBlock block) throws IOException {

  INodeFile inode=namesystem.blockManager.blocksMap.getINode(block.getLocalBlock());

  FileType type=policy.getFileType(inode.getFullPathName());

  return policy.getCompanionBlocks(inode.getFullPathName(),type,block.getLocalBlock());

}

Location: TestBlockPlacementPolicyRaid.java

Content: 

private void refreshPolicy(){

  policy=new BlockPlacementPolicyRaid();

  policy.initialize(conf,namesystem,namesystem.clusterMap);

}

Location: TestBlockPlacementPolicyRaid.java

Content: 

static void setBlockPlacementPolicy(FSNamesystem namesystem,BlockPlacementPolicy policy){

  namesystem.writeLock();

  try {

    namesystem.blockManager.replicator=policy;

  }

  finally {

    namesystem.writeUnlock();

  }

}

Location: TestBlockPlacementPolicyRaid.java

Content: 

protected void setupCluster() throws IOException {

  conf=new Configuration();

  conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,1000L);

  conf.set("dfs.replication.pending.timeout.sec","2");

  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1L);

  conf.set("dfs.block.replicator.classname","org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicyRaid");

  conf.set(RaidNode.STRIPE_LENGTH_KEY,"2");

  conf.set(RaidNode.RS_PARITY_LENGTH_KEY,"3");

  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,1);

  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).racks(rack1).hosts(host1).build();

  cluster.waitActive();

  namesystem=cluster.getNameNode().getNamesystem();

  Assert.assertTrue("BlockPlacementPolicy type is not correct.",namesystem.blockManager.replicator instanceof BlockPlacementPolicyRaid);

  policy=(BlockPlacementPolicyRaid)namesystem.blockManager.replicator;

  fs=cluster.getFileSystem();

  xorPrefix=RaidNode.xorDestinationPath(conf).toUri().getPath();

  raidTempPrefix=RaidNode.xorTempPrefix(conf);

  raidrsTempPrefix=RaidNode.rsTempPrefix(conf);

  raidrsHarTempPrefix=RaidNode.rsHarTempPrefix(conf);

}

Location: TestBlockPlacementPolicyRaid.java

Content: 

/** 

 * Test BlockPlacementPolicyRaid.CachedLocatedBlocks Verify that the results obtained from cache is the same as the results obtained directly

 */

@Test public void testCachedBlocks() throws IOException {

  hongshuai();

  conf=new Configuration();

  conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,1000L);

  conf.set("dfs.replication.pending.timeout.sec","2");

  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1L);

  conf.set("dfs.block.replicator.classname","org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicyRaid");

  conf.set(RaidNode.STRIPE_LENGTH_KEY,"2");

  conf.set(RaidNode.RS_PARITY_LENGTH_KEY,"3");

  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,1);

  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).racks(rack1).hosts(host1).build();

  cluster.waitActive();

  namesystem=cluster.getNameNode().getNamesystem();

  Assert.assertTrue("BlockPlacementPolicy type is not correct.",namesystem.blockManager.replicator instanceof BlockPlacementPolicyRaid);

  policy=(BlockPlacementPolicyRaid)namesystem.blockManager.replicator;

  fs=cluster.getFileSystem();

  xorPrefix=RaidNode.xorDestinationPath(conf).toUri().getPath();

  raidTempPrefix=RaidNode.xorTempPrefix(conf);

  raidrsTempPrefix=RaidNode.rsTempPrefix(conf);

  raidrsHarTempPrefix=RaidNode.rsHarTempPrefix(conf);

  try {

    String file1="/dir/file1";

    String file2="/dir/file2";

    DFSTestUtil.createFile(fs,new Path(file1),3,(short)1,0L);

    DFSTestUtil.createFile(fs,new Path(file2),4,(short)1,0L);

    CachedLocatedBlocks cachedBlocks=new CachedLocatedBlocks(namesystem);

    verifyCachedBlocksResult(cachedBlocks,namesystem,file1);

    verifyCachedBlocksResult(cachedBlocks,namesystem,file1);

    verifyCachedBlocksResult(cachedBlocks,namesystem,file2);

    verifyCachedBlocksResult(cachedBlocks,namesystem,file2);

    try {

      Thread.sleep(1200L);

    }

 catch (    InterruptedException e) {

    }

    verifyCachedBlocksResult(cachedBlocks,namesystem,file2);

    verifyCachedBlocksResult(cachedBlocks,namesystem,file1);

  }

  finally {

    if (cluster != null) {

      cluster.shutdown();

    }

  }

}

Location: TestBlockPlacementPolicyRaid.java

Content: 

/** 

 * Test BlockPlacementPolicyRaid.CachedFullPathNames Verify that the results obtained from cache is the same as the results obtained directly

 */

@Test public void testCachedPathNames() throws IOException {

  hongshuai();

  conf=new Configuration();

  conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,1000L);

  conf.set("dfs.replication.pending.timeout.sec","2");

  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1L);

  conf.set("dfs.block.replicator.classname","org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicyRaid");

  conf.set(RaidNode.STRIPE_LENGTH_KEY,"2");

  conf.set(RaidNode.RS_PARITY_LENGTH_KEY,"3");

  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,1);

  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).racks(rack1).hosts(host1).build();

  cluster.waitActive();

  namesystem=cluster.getNameNode().getNamesystem();

  Assert.assertTrue("BlockPlacementPolicy type is not correct.",namesystem.blockManager.replicator instanceof BlockPlacementPolicyRaid);

  policy=(BlockPlacementPolicyRaid)namesystem.blockManager.replicator;

  fs=cluster.getFileSystem();

  xorPrefix=RaidNode.xorDestinationPath(conf).toUri().getPath();

  raidTempPrefix=RaidNode.xorTempPrefix(conf);

  raidrsTempPrefix=RaidNode.rsTempPrefix(conf);

  raidrsHarTempPrefix=RaidNode.rsHarTempPrefix(conf);

  try {

    String file1="/dir/file1";

    String file2="/dir/file2";

    DFSTestUtil.createFile(fs,new Path(file1),3,(short)1,0L);

    DFSTestUtil.createFile(fs,new Path(file2),4,(short)1,0L);

    CachedFullPathNames cachedFullPathNames=new CachedFullPathNames(namesystem);

    FSInodeInfo inode1=null;

    FSInodeInfo inode2=null;

    namesystem.dir.readLock();

    try {

      inode1=namesystem.dir.rootDir.getNode(file1,true);

      inode2=namesystem.dir.rootDir.getNode(file2,true);

    }

  finally {

      namesystem.dir.readUnlock();

    }

    verifyCachedFullPathNameResult(cachedFullPathNames,inode1);

    verifyCachedFullPathNameResult(cachedFullPathNames,inode1);

    verifyCachedFullPathNameResult(cachedFullPathNames,inode2);

    verifyCachedFullPathNameResult(cachedFullPathNames,inode2);

    try {

      Thread.sleep(1200L);

    }

 catch (    InterruptedException e) {

    }

    verifyCachedFullPathNameResult(cachedFullPathNames,inode2);

    verifyCachedFullPathNameResult(cachedFullPathNames,inode1);

  }

  finally {

    if (cluster != null) {

      cluster.shutdown();

    }

  }

}

Location: TestBlockPlacementPolicyRaid.java

Content: 

/** 

 * Test that the har parity files will be placed at the good locations when we create them.

 */

@Test public void testChooseTargetForHarRaidFile() throws IOException {

  hongshuai();

  conf=new Configuration();

  conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,1000L);

  conf.set("dfs.replication.pending.timeout.sec","2");

  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1L);

  conf.set("dfs.block.replicator.classname","org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicyRaid");

  conf.set(RaidNode.STRIPE_LENGTH_KEY,"2");

  conf.set(RaidNode.RS_PARITY_LENGTH_KEY,"3");

  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,1);

  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).racks(rack1).hosts(host1).build();

  cluster.waitActive();

  namesystem=cluster.getNameNode().getNamesystem();

  Assert.assertTrue("BlockPlacementPolicy type is not correct.",namesystem.blockManager.replicator instanceof BlockPlacementPolicyRaid);

  policy=(BlockPlacementPolicyRaid)namesystem.blockManager.replicator;

  fs=cluster.getFileSystem();

  xorPrefix=RaidNode.xorDestinationPath(conf).toUri().getPath();

  raidTempPrefix=RaidNode.xorTempPrefix(conf);

  raidrsTempPrefix=RaidNode.rsTempPrefix(conf);

  raidrsHarTempPrefix=RaidNode.rsHarTempPrefix(conf);

  try {

    String[] racks={"/rack2","/rack2","/rack2","/rack2","/rack2","/rack2"};

    String[] hosts={"host2.rack2.com","host3.rack2.com","host4.rack2.com","host5.rack2.com","host6.rack2.com","host7.rack2.com"};

    cluster.startDataNodes(conf,6,true,null,racks,hosts,null);

    String harParity=raidrsHarTempPrefix + "/dir/file";

    int numBlocks=11;

    DFSTestUtil.createFile(fs,new Path(harParity),numBlocks,(short)1,0L);

    DFSTestUtil.waitReplication(fs,new Path(harParity),(short)1);

    FileStatus stat=fs.getFileStatus(new Path(harParity));

    BlockLocation[] loc=fs.getFileBlockLocations(stat,0,stat.getLen());

    int rsParityLength=RaidNode.rsParityLength(conf);

    for (int i=0; i < numBlocks - rsParityLength; i++) {

      Set<String> locations=new HashSet<String>();

      for (int j=0; j < rsParityLength; j++) {

        for (int k=0; k < loc[i + j].getNames().length; k++) {

          String name=loc[i + j].getNames()[k];

          LOG.info("Har Raid block location: " + name);

          Assert.assertTrue(locations.add(name));

        }

      }

    }

  }

  finally {

    if (cluster != null) {

      cluster.shutdown();

    }

  }

}

Location: TestBlockPlacementPolicyRaid.java

Content: 

/** 

 * Test that the parity files will be placed at the good locations when we create them.

 */

@Test public void testChooseTargetForRaidFile() throws IOException {

  hongshuai();

  conf=new Configuration();

  conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,1000L);

  conf.set("dfs.replication.pending.timeout.sec","2");

  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1L);

  conf.set("dfs.block.replicator.classname","org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicyRaid");

  conf.set(RaidNode.STRIPE_LENGTH_KEY,"2");

  conf.set(RaidNode.RS_PARITY_LENGTH_KEY,"3");

  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,1);

  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).racks(rack1).hosts(host1).build();

  cluster.waitActive();

  namesystem=cluster.getNameNode().getNamesystem();

  Assert.assertTrue("BlockPlacementPolicy type is not correct.",namesystem.blockManager.replicator instanceof BlockPlacementPolicyRaid);

  policy=(BlockPlacementPolicyRaid)namesystem.blockManager.replicator;

  fs=cluster.getFileSystem();

  xorPrefix=RaidNode.xorDestinationPath(conf).toUri().getPath();

  raidTempPrefix=RaidNode.xorTempPrefix(conf);

  raidrsTempPrefix=RaidNode.rsTempPrefix(conf);

  raidrsHarTempPrefix=RaidNode.rsHarTempPrefix(conf);

  try {

    String src="/dir/file";

    String parity=raidrsTempPrefix + src;

    DFSTestUtil.createFile(fs,new Path(src),4,(short)1,0L);

    DFSTestUtil.waitReplication(fs,new Path(src),(short)1);

    refreshPolicy();

    setBlockPlacementPolicy(namesystem,policy);

    String[] racks={"/rack2","/rack2","/rack2","/rack2","/rack2","/rack2"};

    String[] hosts={"host2.rack2.com","host3.rack2.com","host4.rack2.com","host5.rack2.com","host6.rack2.com","host7.rack2.com"};

    cluster.startDataNodes(conf,6,true,null,racks,hosts,null);

    int numBlocks=6;

    DFSTestUtil.createFile(fs,new Path(parity),numBlocks,(short)2,0L);

    DFSTestUtil.waitReplication(fs,new Path(parity),(short)2);

    FileStatus srcStat=fs.getFileStatus(new Path(src));

    BlockLocation[] srcLoc=fs.getFileBlockLocations(srcStat,0,srcStat.getLen());

    FileStatus parityStat=fs.getFileStatus(new Path(parity));

    BlockLocation[] parityLoc=fs.getFileBlockLocations(parityStat,0,parityStat.getLen());

    int parityLen=RaidNode.rsParityLength(conf);

    for (int i=0; i < numBlocks / parityLen; i++) {

      Set<String> locations=new HashSet<String>();

      for (int j=0; j < srcLoc.length; j++) {

        String[] names=srcLoc[j].getNames();

        for (int k=0; k < names.length; k++) {

          LOG.info("Source block location: " + names[k]);

          locations.add(names[k]);

        }

      }

      for (int j=0; j < parityLen; j++) {

        String[] names=parityLoc[j + i * parityLen].getNames();

        for (int k=0; k < names.length; k++) {

          LOG.info("Parity block location: " + names[k]);

          Assert.assertTrue(locations.add(names[k]));

        }

      }

    }

  }

  finally {

    if (cluster != null) {

      cluster.shutdown();

    }

  }

}

Location: TestBlockPlacementPolicyRaid.java

Content: 

/** 

 * Test BlockPlacementPolicyRaid actually deletes the correct replica. Start 2 datanodes and create 1 source file and its parity file. 1) Start host1, create the parity file with replication 1 2) Start host2, create the source file with replication 2 3) Set repliation of source file to 1 Verify that the policy should delete the block with more companion blocks.

 */

@Test public void testDeleteReplica() throws IOException {

  hongshuai();

  conf=new Configuration();

  conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,1000L);

  conf.set("dfs.replication.pending.timeout.sec","2");

  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1L);

  conf.set("dfs.block.replicator.classname","org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicyRaid");

  conf.set(RaidNode.STRIPE_LENGTH_KEY,"2");

  conf.set(RaidNode.RS_PARITY_LENGTH_KEY,"3");

  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,1);

  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).racks(rack1).hosts(host1).build();

  cluster.waitActive();

  namesystem=cluster.getNameNode().getNamesystem();

  Assert.assertTrue("BlockPlacementPolicy type is not correct.",namesystem.blockManager.replicator instanceof BlockPlacementPolicyRaid);

  policy=(BlockPlacementPolicyRaid)namesystem.blockManager.replicator;

  fs=cluster.getFileSystem();

  xorPrefix=RaidNode.xorDestinationPath(conf).toUri().getPath();

  raidTempPrefix=RaidNode.xorTempPrefix(conf);

  raidrsTempPrefix=RaidNode.rsTempPrefix(conf);

  raidrsHarTempPrefix=RaidNode.rsHarTempPrefix(conf);

  try {

    setBlockPlacementPolicy(namesystem,new BlockPlacementPolicyDefault(conf,namesystem,namesystem.clusterMap));

    DatanodeDescriptor datanode1=namesystem.datanodeMap.values().iterator().next();

    String source="/dir/file";

    String parity=xorPrefix + source;

    final Path parityPath=new Path(parity);

    DFSTestUtil.createFile(fs,parityPath,3,(short)1,0L);

    DFSTestUtil.waitReplication(fs,parityPath,(short)1);

    cluster.startDataNodes(conf,1,true,null,rack2,host2,null);

    DatanodeDescriptor datanode2=null;

    for (    DatanodeDescriptor d : namesystem.datanodeMap.values()) {

      if (!d.getName().equals(datanode1.getName())) {

        datanode2=d;

      }

    }

    Assert.assertTrue(datanode2 != null);

    cluster.waitActive();

    final Path sourcePath=new Path(source);

    DFSTestUtil.createFile(fs,sourcePath,5,(short)2,0L);

    DFSTestUtil.waitReplication(fs,sourcePath,(short)2);

    refreshPolicy();

    Assert.assertEquals(parity,policy.getParityFile(source));

    Assert.assertEquals(source,policy.getSourceFile(parity,xorPrefix));

    List<LocatedBlock> sourceBlocks=getBlocks(namesystem,source);

    List<LocatedBlock> parityBlocks=getBlocks(namesystem,parity);

    Assert.assertEquals(5,sourceBlocks.size());

    Assert.assertEquals(3,parityBlocks.size());

    Collection<LocatedBlock> companionBlocks;

    companionBlocks=getCompanionBlocks(namesystem,policy,sourceBlocks.get(0).getBlock());

    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{0,1},new int[]{0});

    companionBlocks=getCompanionBlocks(namesystem,policy,sourceBlocks.get(1).getBlock());

    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{0,1},new int[]{0});

    companionBlocks=getCompanionBlocks(namesystem,policy,sourceBlocks.get(2).getBlock());

    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{2,3},new int[]{1});

    companionBlocks=getCompanionBlocks(namesystem,policy,sourceBlocks.get(3).getBlock());

    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{2,3},new int[]{1});

    companionBlocks=getCompanionBlocks(namesystem,policy,sourceBlocks.get(4).getBlock());

    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{4},new int[]{2});

    companionBlocks=getCompanionBlocks(namesystem,policy,parityBlocks.get(0).getBlock());

    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{0,1},new int[]{0});

    companionBlocks=getCompanionBlocks(namesystem,policy,parityBlocks.get(1).getBlock());

    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{2,3},new int[]{1});

    companionBlocks=getCompanionBlocks(namesystem,policy,parityBlocks.get(2).getBlock());

    verifyCompanionBlocks(companionBlocks,sourceBlocks,parityBlocks,new int[]{4},new int[]{2});

    refreshPolicy();

    setBlockPlacementPolicy(namesystem,policy);

    fs.setReplication(sourcePath,(short)1);

    DFSTestUtil.waitReplication(fs,sourcePath,(short)1);

    Map<String,Integer> counters=new HashMap<String,Integer>();

    refreshPolicy();

    for (int i=0; i < parityBlocks.size(); i++) {

      companionBlocks=getCompanionBlocks(namesystem,policy,parityBlocks.get(i).getBlock());

      counters=BlockPlacementPolicyRaid.countCompanionBlocks(companionBlocks,false);

      Assert.assertTrue(counters.get(datanode1.getName()) >= 1 && counters.get(datanode1.getName()) <= 2);

      Assert.assertTrue(counters.get(datanode1.getName()) + counters.get(datanode2.getName()) == companionBlocks.size());

      counters=BlockPlacementPolicyRaid.countCompanionBlocks(companionBlocks,true);

      Assert.assertTrue(counters.get(datanode1.getParent().getName()) >= 1 && counters.get(datanode1.getParent().getName()) <= 2);

      Assert.assertTrue(counters.get(datanode1.getParent().getName()) + counters.get(datanode2.getParent().getName()) == companionBlocks.size());

    }

  }

  finally {

    if (cluster != null) {

      cluster.shutdown();

    }

  }

}

Location: TestBlockPlacementPolicyRaid.java

Content: 

/** 

 * Test the result of getCompanionBlocks() on the unraided files

 */

@Test public void testGetCompanionBLocks() throws IOException {

  hongshuai();

  conf=new Configuration();

  conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,1000L);

  conf.set("dfs.replication.pending.timeout.sec","2");

  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1L);

  conf.set("dfs.block.replicator.classname","org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicyRaid");

  conf.set(RaidNode.STRIPE_LENGTH_KEY,"2");

  conf.set(RaidNode.RS_PARITY_LENGTH_KEY,"3");

  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,1);

  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).racks(rack1).hosts(host1).build();

  cluster.waitActive();

  namesystem=cluster.getNameNode().getNamesystem();

  Assert.assertTrue("BlockPlacementPolicy type is not correct.",namesystem.blockManager.replicator instanceof BlockPlacementPolicyRaid);

  policy=(BlockPlacementPolicyRaid)namesystem.blockManager.replicator;

  fs=cluster.getFileSystem();

  xorPrefix=RaidNode.xorDestinationPath(conf).toUri().getPath();

  raidTempPrefix=RaidNode.xorTempPrefix(conf);

  raidrsTempPrefix=RaidNode.rsTempPrefix(conf);

  raidrsHarTempPrefix=RaidNode.rsHarTempPrefix(conf);

  try {

    String file1="/dir/file1";

    String file2="/raid/dir/file2";

    String file3="/raidrs/dir/file3";

    setBlockPlacementPolicy(namesystem,new BlockPlacementPolicyDefault(conf,namesystem,namesystem.clusterMap));

    DFSTestUtil.createFile(fs,new Path(file1),3,(short)1,0L);

    DFSTestUtil.createFile(fs,new Path(file2),4,(short)1,0L);

    DFSTestUtil.createFile(fs,new Path(file3),8,(short)1,0L);

    Collection<LocatedBlock> companionBlocks;

    companionBlocks=getCompanionBlocks(namesystem,policy,getBlocks(namesystem,file1).get(0).getBlock());

    Assert.assertTrue(companionBlocks == null || companionBlocks.size() == 0);

    companionBlocks=getCompanionBlocks(namesystem,policy,getBlocks(namesystem,file1).get(2).getBlock());

    Assert.assertTrue(companionBlocks == null || companionBlocks.size() == 0);

    companionBlocks=getCompanionBlocks(namesystem,policy,getBlocks(namesystem,file2).get(0).getBlock());

    Assert.assertEquals(1,companionBlocks.size());

    companionBlocks=getCompanionBlocks(namesystem,policy,getBlocks(namesystem,file2).get(3).getBlock());

    Assert.assertEquals(1,companionBlocks.size());

    int rsParityLength=RaidNode.rsParityLength(conf);

    companionBlocks=getCompanionBlocks(namesystem,policy,getBlocks(namesystem,file3).get(0).getBlock());

    Assert.assertEquals(rsParityLength,companionBlocks.size());

    companionBlocks=getCompanionBlocks(namesystem,policy,getBlocks(namesystem,file3).get(4).getBlock());

    Assert.assertEquals(rsParityLength,companionBlocks.size());

    companionBlocks=getCompanionBlocks(namesystem,policy,getBlocks(namesystem,file3).get(6).getBlock());

    Assert.assertEquals(2,companionBlocks.size());

  }

  finally {

    if (cluster != null) {

      cluster.shutdown();

    }

  }

}

Location: TestBlockPlacementPolicyRaid.java

Content: 

private void verifyCachedBlocksResult(CachedLocatedBlocks cachedBlocks,FSNamesystem namesystem,String file) throws IOException {

  long len=namesystem.getFileInfo(file,true).getLen();

  List<LocatedBlock> res1=namesystem.getBlockLocations(file,0L,len,false,false).getLocatedBlocks();

  List<LocatedBlock> res2=cachedBlocks.get(file);

  for (int i=0; i < res1.size(); i++) {

    LOG.info("Actual block: " + res1.get(i).getBlock());

    LOG.info("Cached block: " + res2.get(i).getBlock());

    Assert.assertEquals(res1.get(i).getBlock(),res2.get(i).getBlock());

  }

}

Location: TestBlockPlacementPolicyRaid.java

Content: 

private void verifyCachedFullPathNameResult(CachedFullPathNames cachedFullPathNames,FSInodeInfo inode) throws IOException {

  String res1=inode.getFullPathName();

  String res2=cachedFullPathNames.get(inode);

  LOG.info("Actual path name: " + res1);

  LOG.info("Cached path name: " + res2);

  Assert.assertEquals(cachedFullPathNames.get(inode),inode.getFullPathName());

}

Location: TestBlockPlacementPolicyRaid.java

Content: 

private void verifyCompanionBlocks(Collection<LocatedBlock> companionBlocks,List<LocatedBlock> sourceBlocks,List<LocatedBlock> parityBlocks,int[] sourceBlockIndexes,int[] parityBlockIndexes){

  Set<ExtendedBlock> blockSet=new HashSet<ExtendedBlock>();

  for (  LocatedBlock b : companionBlocks) {

    blockSet.add(b.getBlock());

  }

  Assert.assertEquals(sourceBlockIndexes.length + parityBlockIndexes.length,blockSet.size());

  for (  int index : sourceBlockIndexes) {

    Assert.assertTrue(blockSet.contains(sourceBlocks.get(index).getBlock()));

  }

  for (  int index : parityBlockIndexes) {

    Assert.assertTrue(blockSet.contains(parityBlocks.get(index).getBlock()));

  }

}

