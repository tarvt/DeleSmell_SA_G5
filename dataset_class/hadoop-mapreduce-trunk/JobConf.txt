Location: JobConf.java

Content: 

private void checkAndWarnDeprecation(){

  if (get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) != null) {

    LOG.warn(JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + " Instead use " + JobConf.MAPRED_JOB_MAP_MEMORY_MB_PROPERTY+ " and "+ JobConf.MAPRED_JOB_REDUCE_MEMORY_MB_PROPERTY);

  }

}

Location: JobConf.java

Content: 

/** 

 * Compute the number of slots required to run a single map task-attempt of this job.

 * @param slotSizePerMap cluster-wide value of the amount of memory requiredto run a map-task

 * @return the number of slots required to run a single map task-attempt1 if memory parameters are disabled.

 */

int computeNumSlotsPerMap(long slotSizePerMap){

  if ((slotSizePerMap == DISABLED_MEMORY_LIMIT) || (getMemoryForMapTask() == DISABLED_MEMORY_LIMIT)) {

    return 1;

  }

  return (int)(Math.ceil((float)getMemoryForMapTask() / (float)slotSizePerMap));

}

Location: JobConf.java

Content: 

/** 

 * Compute the number of slots required to run a single reduce task-attempt of this job.

 * @param slotSizePerReduce cluster-wide value of the amount of memory required to run a reduce-task

 * @return the number of slots required to run a single reduce task-attempt1 if memory parameters are disabled

 */

int computeNumSlotsPerReduce(long slotSizePerReduce){

  if ((slotSizePerReduce == DISABLED_MEMORY_LIMIT) || (getMemoryForReduceTask() == DISABLED_MEMORY_LIMIT)) {

    return 1;

  }

  return (int)(Math.ceil((float)getMemoryForReduceTask() / (float)slotSizePerReduce));

}

Location: JobConf.java

Content: 

/** 

 * Use MRAsyncDiskService.moveAndDeleteAllVolumes instead.

 * @see org.apache.hadoop.mapreduce.util.MRAsyncDiskService#cleanupAllVolumes()

 */

@Deprecated public void deleteLocalFiles() throws IOException {

  String[] localDirs=getLocalDirs();

  for (int i=0; i < localDirs.length; i++) {

    FileSystem.getLocal(this).delete(new Path(localDirs[i]),true);

  }

}

Location: JobConf.java

Content: 

public void deleteLocalFiles(String subdir) throws IOException {

  String[] localDirs=getLocalDirs();

  for (int i=0; i < localDirs.length; i++) {

    FileSystem.getLocal(this).delete(new Path(localDirs[i],subdir),true);

  }

}

Location: JobConf.java

Content: 

static String deprecatedString(String key){

  return "The variable " + key + " is no longer used.";

}

Location: JobConf.java

Content: 

/** 

 * Find a jar that contains a class of the same name, if any. It will return a jar file, even if that is not the first thing on the class path that has a class with the same name.

 * @param my_class the class to find.

 * @return a jar file that contains the class, or null.

 * @throws IOException

 */

static String findContainingJar(Class my_class){

  ClassLoader loader=my_class.getClassLoader();

  String class_file=my_class.getName().replaceAll("\\.","/") + ".class";

  try {

    for (Enumeration itr=loader.getResources(class_file); itr.hasMoreElements(); ) {

      URL url=(URL)itr.nextElement();

      if ("jar".equals(url.getProtocol())) {

        String toReturn=url.getPath();

        if (toReturn.startsWith("file:")) {

          toReturn=toReturn.substring("file:".length());

        }

        toReturn=toReturn.replaceAll("\\+","%2B");

        toReturn=URLDecoder.decode(toReturn,"UTF-8");

        return toReturn.replaceAll("!.*$","");

      }

    }

  }

 catch (  IOException e) {

    throw new RuntimeException(e);

  }

  return null;

}

Location: JobConf.java

Content: 

/** 

 * Are the outputs of the maps be compressed?

 * @return <code>true</code> if the outputs of the maps are to be compressed,<code>false</code> otherwise.

 */

public boolean getCompressMapOutput(){

  return getBoolean(JobContext.MAP_OUTPUT_COMPRESS,false);

}

Location: JobConf.java

Content: 

private long getDeprecatedMemoryValue(){

  long oldValue=getLong(MAPRED_TASK_MAXVMEM_PROPERTY,DISABLED_MEMORY_LIMIT);

  oldValue=normalizeMemoryConfigValue(oldValue);

  if (oldValue != DISABLED_MEMORY_LIMIT) {

    oldValue/=(1024 * 1024);

  }

  return oldValue;

}

Location: JobConf.java

Content: 

/** 

 * Get the  {@link InputFormat} implementation for the map-reduce job,defaults to  {@link TextInputFormat} if not specified explicity.

 * @return the {@link InputFormat} implementation for the map-reduce job.

 */

public InputFormat getInputFormat(){

  return ReflectionUtils.newInstance(getClass("mapred.input.format.class",TextInputFormat.class,InputFormat.class),this);

}

Location: JobConf.java

Content: 

/** 

 * Get the pattern for jar contents to unpack on the tasktracker

 */

public Pattern getJarUnpackPattern(){

  return getPattern(JobContext.JAR_UNPACK_PATTERN,UNPACK_JAR_PATTERN_DEFAULT);

}

Location: JobConf.java

Content: 

/** 

 * Get the uri to be invoked in-order to send a notification after the job  has completed (success/failure). 

 * @return the job end notification uri, <code>null</code> if it hasn'tbeen set.

 * @see #setJobEndNotificationURI(String)

 */

public String getJobEndNotificationURI(){

  return get(JobContext.END_NOTIFICATION_URL);

}

Location: JobConf.java

Content: 

/** 

 * Get job-specific shared directory for use as scratch space <p> When a job starts, a shared directory is created at location <code> ${mapreduce.cluster.local.dir}/taskTracker/$user/jobcache/$jobid/work/ </code>. This directory is exposed to the users through  <code>mapreduce.job.local.dir </code>. So, the tasks can use this space  as scratch space and share files among them. </p> This value is available as System property also.

 * @return The localized job specific shared directory

 */

public String getJobLocalDir(){

  return get(JobContext.JOB_LOCAL_DIR);

}

Location: JobConf.java

Content: 

/** 

 * Should the temporary files for failed tasks be kept?

 * @return should the files be kept?

 */

public boolean getKeepFailedTaskFiles(){

  return getBoolean(JobContext.PRESERVE_FAILED_TASK_FILES,false);

}

Location: JobConf.java

Content: 

/** 

 * Get the regular expression that is matched against the task names to see if we need to keep the files.

 * @return the pattern as a string, if it was set, othewise null.

 */

public String getKeepTaskFilesPattern(){

  return get(JobContext.PRESERVE_FILES_PATTERN);

}

Location: JobConf.java

Content: 

/** 

 * Get the  {@link KeyFieldBasedComparator} options

 */

public String getKeyFieldComparatorOption(){

  return get(KeyFieldBasedComparator.COMPARATOR_OPTIONS);

}

Location: JobConf.java

Content: 

/** 

 * Get the  {@link KeyFieldBasedPartitioner} options

 */

public String getKeyFieldPartitionerOption(){

  return get(KeyFieldBasedPartitioner.PARTITIONER_OPTIONS);

}

Location: JobConf.java

Content: 

public String[] getLocalDirs() throws IOException {

  return getTrimmedStrings(MRConfig.LOCAL_DIR);

}

Location: JobConf.java

Content: 

/** 

 * Constructs a local file name. Files are distributed among configured local directories.

 */

public Path getLocalPath(String pathString) throws IOException {

  return getLocalPath(MRConfig.LOCAL_DIR,pathString);

}

Location: JobConf.java

Content: 

/** 

 * Get the map task's debug script.

 * @return the debug Script for the mapred job for failed map tasks.

 * @see #setMapDebugScript(String)

 */

public String getMapDebugScript(){

  return get(JobContext.MAP_DEBUG_SCRIPT);

}

Location: JobConf.java

Content: 

/** 

 * Get the  {@link CompressionCodec} for compressing the map outputs.

 * @param defaultValue the {@link CompressionCodec} to return if not set

 * @return the {@link CompressionCodec} class that should be used to compress the map outputs.

 * @throws IllegalArgumentException if the class was specified, but not found

 */

public Class<? extends CompressionCodec> getMapOutputCompressorClass(Class<? extends CompressionCodec> defaultValue){

  Class<? extends CompressionCodec> codecClass=defaultValue;

  String name=get(JobContext.MAP_OUTPUT_COMPRESS_CODEC);

  if (name != null) {

    try {

      codecClass=getClassByName(name).asSubclass(CompressionCodec.class);

    }

 catch (    ClassNotFoundException e) {

      throw new IllegalArgumentException("Compression codec " + name + " was not found.",e);

    }

  }

  return codecClass;

}

Location: JobConf.java

Content: 

/** 

 * Get the  {@link MapRunnable} class for the job.

 * @return the {@link MapRunnable} class for the job.

 */

public Class<? extends MapRunnable> getMapRunnerClass(){

  return getClass("mapred.map.runner.class",MapRunner.class,MapRunnable.class);

}

Location: JobConf.java

Content: 

/** 

 * Get the maximum percentage of map tasks that can fail without  the job being aborted.  Each map task is executed a minimum of  {@link #getMaxMapAttempts()} attempts before being declared as <i>failed</i>. Defaults to <code>zero</code>, i.e. <i>any</i> failed map-task results in the job being declared as  {@link JobStatus#FAILED}.

 * @return the maximum percentage of map tasks that can fail withoutthe job being aborted.

 */

public int getMaxMapTaskFailuresPercent(){

  return getInt(JobContext.MAP_FAILURES_MAX_PERCENT,0);

}

Location: JobConf.java

Content: 

/** 

 * @deprecated this variable is deprecated and nolonger in use.

 */

@Deprecated public long getMaxPhysicalMemoryForTask(){

  LOG.warn("The API getMaxPhysicalMemoryForTask() is deprecated." + " Refer to the APIs getMemoryForMapTask() and" + " getMemoryForReduceTask() for details.");

  return -1;

}

Location: JobConf.java

Content: 

/** 

 * Get the maximum percentage of reduce tasks that can fail without  the job being aborted.  Each reduce task is executed a minimum of  {@link #getMaxReduceAttempts()} attempts before being declared as <i>failed</i>. Defaults to <code>zero</code>, i.e. <i>any</i> failed reduce-task results  in the job being declared as  {@link JobStatus#FAILED}.

 * @return the maximum percentage of reduce tasks that can fail withoutthe job being aborted.

 */

public int getMaxReduceTaskFailuresPercent(){

  return getInt(JobContext.REDUCE_FAILURES_MAXPERCENT,0);

}

Location: JobConf.java

Content: 

/** 

 * Expert: Get the maximum no. of failures of a given job per tasktracker. If the no. of task failures exceeds this, the tasktracker is <i>blacklisted</i> for this job. 

 * @return the maximum no. of failures of a given job per tasktracker.

 */

public int getMaxTaskFailuresPerTracker(){

  return getInt(JobContext.MAX_TASK_FAILURES_PER_TRACKER,4);

}

Location: JobConf.java

Content: 

/** 

 * Get the memory required to run a task of this job, in bytes. See {@link #MAPRED_TASK_MAXVMEM_PROPERTY}<p/> This method is deprecated. Now, different memory limits can be set for map and reduce tasks of a job, in MB.  <p/> For backward compatibility, if the job configuration sets the key  {@link #MAPRED_TASK_MAXVMEM_PROPERTY} to a value differentfrom  {@link #DISABLED_MEMORY_LIMIT}, that value is returned.  Otherwise, this method will return the larger of the values returned by  {@link #getMemoryForMapTask()} and {@link #getMemoryForReduceTask()}after converting them into bytes.

 * @return Memory required to run a task of this job, in bytes,or  {@link #DISABLED_MEMORY_LIMIT}, if unset.

 * @see #setMaxVirtualMemoryForTask(long)

 * @deprecated Use {@link #getMemoryForMapTask()} and{@link #getMemoryForReduceTask()}

 */

@Deprecated public long getMaxVirtualMemoryForTask(){

  LOG.warn("getMaxVirtualMemoryForTask() is deprecated. " + "Instead use getMemoryForMapTask() and getMemoryForReduceTask()");

  long value=getLong(MAPRED_TASK_MAXVMEM_PROPERTY,DISABLED_MEMORY_LIMIT);

  value=normalizeMemoryConfigValue(value);

  if (value == DISABLED_MEMORY_LIMIT) {

    value=Math.max(getMemoryForMapTask(),getMemoryForReduceTask());

    value=normalizeMemoryConfigValue(value);

    if (value != DISABLED_MEMORY_LIMIT) {

      value*=1024 * 1024;

    }

  }

  return value;

}

Location: JobConf.java

Content: 

/** 

 * Get configured the number of reduce tasks for this job. Defaults to <code>1</code>.

 * @return the number of reduce tasks for this job.

 */

public int getNumMapTasks(){

  return getInt(JobContext.NUM_MAPS,1);

}

Location: JobConf.java

Content: 

/** 

 * Get the number of tasks that a spawned JVM should execute

 */

public int getNumTasksToExecutePerJvm(){

  return getInt(JobContext.JVM_NUMTASKS_TORUN,1);

}

Location: JobConf.java

Content: 

/** 

 * Get the  {@link OutputFormat} implementation for the map-reduce job,defaults to  {@link TextOutputFormat} if not specified explicity.

 * @return the {@link OutputFormat} implementation for the map-reduce job.

 */

public OutputFormat getOutputFormat(){

  return ReflectionUtils.newInstance(getClass("mapred.output.format.class",TextOutputFormat.class,OutputFormat.class),this);

}

Location: JobConf.java

Content: 

/** 

 * Get the  {@link RawComparator} comparator used to compare keys.

 * @return the {@link RawComparator} comparator used to compare keys.

 */

public RawComparator getOutputKeyComparator(){

  Class<? extends RawComparator> theClass=getClass(JobContext.KEY_COMPARATOR,null,RawComparator.class);

  if (theClass != null)   return ReflectionUtils.newInstance(theClass,this);

  return WritableComparator.get(getMapOutputKeyClass().asSubclass(WritableComparable.class));

}

Location: JobConf.java

Content: 

/** 

 * Get the user defined  {@link WritableComparable} comparator for grouping keys of inputs to the reduce.

 * @return comparator set by the user for grouping values.

 * @see #setOutputValueGroupingComparator(Class) for details.  

 */

public RawComparator getOutputValueGroupingComparator(){

  Class<? extends RawComparator> theClass=getClass(JobContext.GROUP_COMPARATOR_CLASS,null,RawComparator.class);

  if (theClass == null) {

    return getOutputKeyComparator();

  }

  return ReflectionUtils.newInstance(theClass,this);

}

Location: JobConf.java

Content: 

/** 

 * Get the reduce task's debug Script

 * @return the debug script for the mapred job for failed reduce tasks.

 * @see #setReduceDebugScript(String)

 */

public String getReduceDebugScript(){

  return get(JobContext.REDUCE_DEBUG_SCRIPT);

}

Location: JobConf.java

Content: 

/** 

 * Get the user-specified session identifier. The default is the empty string. The session identifier is used to tag metric data that is reported to some performance metrics system via the org.apache.hadoop.metrics API.  The  session identifier is intended, in particular, for use by Hadoop-On-Demand  (HOD) which allocates a virtual Hadoop cluster dynamically and transiently.  HOD will set the session identifier by modifying the mapred-site.xml file  before starting the cluster. When not running under HOD, this identifer is expected to remain set to  the empty string.

 * @return the session identifier, defaulting to "".

 */

@Deprecated public String getSessionId(){

  return get("session.id","");

}

Location: JobConf.java

Content: 

/** 

 * Should speculative execution be used for this job?  Defaults to <code>true</code>.

 * @return <code>true</code> if speculative execution be used for this job,<code>false</code> otherwise.

 */

public boolean getSpeculativeExecution(){

  return (getMapSpeculativeExecution() || getReduceSpeculativeExecution());

}

Location: JobConf.java

Content: 

/** 

 * Should the framework use the new context-object code for running the mapper?

 * @return true, if the new api should be used

 */

public boolean getUseNewMapper(){

  return getBoolean("mapred.mapper.new-api",false);

}

Location: JobConf.java

Content: 

/** 

 * Should the framework use the new context-object code for running the reducer?

 * @return true, if the new api should be used

 */

public boolean getUseNewReducer(){

  return getBoolean("mapred.reducer.new-api",false);

}

Location: JobConf.java

Content: 

/** 

 * Construct a map/reduce job configuration.

 */

public JobConf(){

  checkAndWarnDeprecation();

}

Location: JobConf.java

Content: 

/** 

 * A new map/reduce configuration where the behavior of reading from the default resources can be turned off. <p/> If the parameter  {@code loadDefaults} is false, the new instancewill not load resources from the default files.

 * @param loadDefaults specifies whether to load from the default files

 */

public JobConf(boolean loadDefaults){

  super(loadDefaults);

  checkAndWarnDeprecation();

}

Location: JobConf.java

Content: 

/** 

 * Construct a map/reduce job configuration.

 * @param exampleClass a class whose containing jar is used as the job's jar.

 */

public JobConf(Class exampleClass){

  setJarByClass(exampleClass);

  checkAndWarnDeprecation();

}

Location: JobConf.java

Content: 

/** 

 * Construct a map/reduce job configuration.

 * @param conf a Configuration whose settings will be inherited.

 */

public JobConf(Configuration conf){

  super(conf);

  if (conf instanceof JobConf) {

    JobConf that=(JobConf)conf;

    credentials=that.credentials;

  }

  checkAndWarnDeprecation();

}

Location: JobConf.java

Content: 

/** 

 * Construct a map/reduce job configuration.

 * @param conf a Configuration whose settings will be inherited.

 * @param exampleClass a class whose containing jar is used as the job's jar.

 */

public JobConf(Configuration conf,Class exampleClass){

  this(conf);

  setJarByClass(exampleClass);

}

Location: JobConf.java

Content: 

/** 

 * Construct a map/reduce configuration.

 * @param config a Configuration-format XML job description file.

 */

public JobConf(Path config){

  super();

  addResource(config);

  checkAndWarnDeprecation();

}

Location: JobConf.java

Content: 

/** 

 * Construct a map/reduce configuration.

 * @param config a Configuration-format XML job description file.

 */

public JobConf(String config){

  this(new Path(config));

}

Location: JobConf.java

Content: 

/** 

 * Should the map outputs be compressed before transfer? Uses the SequenceFile compression.

 * @param compress should the map outputs be compressed?

 */

public void setCompressMapOutput(boolean compress){

  setBoolean(JobContext.MAP_OUTPUT_COMPRESS,compress);

}

Location: JobConf.java

Content: 

void setCredentials(Credentials credentials){

  this.credentials=credentials;

}

Location: JobConf.java

Content: 

/** 

 * Set the  {@link InputFormat} implementation for the map-reduce job.

 * @param theClass the {@link InputFormat} implementation for the map-reduce job.

 */

public void setInputFormat(Class<? extends InputFormat> theClass){

  setClass("mapred.input.format.class",theClass,InputFormat.class);

}

Location: JobConf.java

Content: 

/** 

 * Set the uri to be invoked in-order to send a notification after the job has completed (success/failure). <p>The uri can contain 2 special parameters: <tt>$jobId</tt> and  <tt>$jobStatus</tt>. Those, if present, are replaced by the job's  identifier and completion-status respectively.</p> <p>This is typically used by application-writers to implement chaining of  Map-Reduce jobs in an <i>asynchronous manner</i>.</p>

 * @param uri the job end notification uri

 * @see JobStatus

 * @see <a href="{@docRoot}/org/apache/hadoop/mapred/JobClient.html# JobCompletionAndChaining">Job Completion and Chaining</a>

 */

public void setJobEndNotificationURI(String uri){

  set(JobContext.END_NOTIFICATION_URL,uri);

}

Location: JobConf.java

Content: 

/** 

 * Set JobSubmitHostAddress for this job.

 * @param hostadd the JobSubmitHostAddress for this job.

 */

void setJobSubmitHostAddress(String hostadd){

  set(MRJobConfig.JOB_SUBMITHOSTADDR,hostadd);

}

Location: JobConf.java

Content: 

/** 

 * Set JobSubmitHostName for this job.

 * @param hostname the JobSubmitHostName for this job.

 */

void setJobSubmitHostName(String hostname){

  set(MRJobConfig.JOB_SUBMITHOST,hostname);

}

Location: JobConf.java

Content: 

/** 

 * Set whether the framework should keep the intermediate files for  failed tasks.

 * @param keep <code>true</code> if framework should keep the intermediate files for failed tasks, <code>false</code> otherwise.

 */

public void setKeepFailedTaskFiles(boolean keep){

  setBoolean(JobContext.PRESERVE_FAILED_TASK_FILES,keep);

}

Location: JobConf.java

Content: 

/** 

 * Set a regular expression for task names that should be kept.  The regular expression ".*_m_000123_0" would keep the files for the first instance of map 123 that ran.

 * @param pattern the java.util.regex.Pattern to match against the task names.

 */

public void setKeepTaskFilesPattern(String pattern){

  set(JobContext.PRESERVE_FILES_PATTERN,pattern);

}

Location: JobConf.java

Content: 

/** 

 * Set the  {@link KeyFieldBasedComparator} options used to compare keys.

 * @param keySpec the key specification of the form -k pos1[,pos2], where,pos is of the form f[.c][opts], where f is the number of the key field to use, and c is the number of the first character from the beginning of the field. Fields and character posns are numbered  starting with 1; a character position of zero in pos2 indicates the field's last character. If '.c' is omitted from pos1, it defaults to 1 (the beginning of the field); if omitted from pos2, it defaults to 0  (the end of the field). opts are ordering options. The supported options are: -n, (Sort numerically) -r, (Reverse the result of comparison)                 

 */

public void setKeyFieldComparatorOptions(String keySpec){

  setOutputKeyComparatorClass(KeyFieldBasedComparator.class);

  set(KeyFieldBasedComparator.COMPARATOR_OPTIONS,keySpec);

}

Location: JobConf.java

Content: 

/** 

 * Set the  {@link KeyFieldBasedPartitioner} options used for {@link Partitioner}

 * @param keySpec the key specification of the form -k pos1[,pos2], where,pos is of the form f[.c][opts], where f is the number of the key field to use, and c is the number of the first character from the beginning of the field. Fields and character posns are numbered  starting with 1; a character position of zero in pos2 indicates the field's last character. If '.c' is omitted from pos1, it defaults to 1 (the beginning of the field); if omitted from pos2, it defaults to 0  (the end of the field).

 */

public void setKeyFieldPartitionerOptions(String keySpec){

  setPartitionerClass(KeyFieldBasedPartitioner.class);

  set(KeyFieldBasedPartitioner.PARTITIONER_OPTIONS,keySpec);

}

Location: JobConf.java

Content: 

/** 

 * Set the debug script to run when the map tasks fail. <p>The debug script can aid debugging of failed map tasks. The script is  given task's stdout, stderr, syslog, jobconf files as arguments.</p> <p>The debug command, run on the node where the map failed, is:</p> <p><pre><blockquote>  $script $stdout $stderr $syslog $jobconf. </blockquote></pre></p> <p> The script file is distributed through  {@link DistributedCache} APIs. The script needs to be symlinked. </p> <p>Here is an example on how to submit a script  <p><blockquote><pre> job.setMapDebugScript("./myscript"); DistributedCache.createSymlink(job); DistributedCache.addCacheFile("/debug/scripts/myscript#myscript"); </pre></blockquote></p>

 * @param mDbgScript the script name

 */

public void setMapDebugScript(String mDbgScript){

  set(JobContext.MAP_DEBUG_SCRIPT,mDbgScript);

}

Location: JobConf.java

Content: 

/** 

 * Set the given class as the   {@link CompressionCodec} for the map outputs.

 * @param codecClass the {@link CompressionCodec} class that will compress  the map outputs.

 */

public void setMapOutputCompressorClass(Class<? extends CompressionCodec> codecClass){

  setCompressMapOutput(true);

  setClass(JobContext.MAP_OUTPUT_COMPRESS_CODEC,codecClass,CompressionCodec.class);

}

Location: JobConf.java

Content: 

/** 

 * Expert: Set the  {@link MapRunnable} class for the job.Typically used to exert greater control on  {@link Mapper}s.

 * @param theClass the {@link MapRunnable} class for the job.

 */

public void setMapRunnerClass(Class<? extends MapRunnable> theClass){

  setClass("mapred.map.runner.class",theClass,MapRunnable.class);

}

Location: JobConf.java

Content: 

/** 

 * Expert: Set the maximum percentage of map tasks that can fail without the job being aborted.  Each map task is executed a minimum of  {@link #getMaxMapAttempts} attempts before being declared as <i>failed</i>.

 * @param percent the maximum percentage of map tasks that can fail without the job being aborted.

 */

public void setMaxMapTaskFailuresPercent(int percent){

  setInt(JobContext.MAP_FAILURES_MAX_PERCENT,percent);

}

Location: JobConf.java

Content: 

@Deprecated public void setMaxPhysicalMemoryForTask(long mem){

  LOG.warn("The API setMaxPhysicalMemoryForTask() is deprecated." + " The value set is ignored. Refer to " + " setMemoryForMapTask() and setMemoryForReduceTask() for details.");

}

Location: JobConf.java

Content: 

/** 

 * Set the maximum percentage of reduce tasks that can fail without the job being aborted. Each reduce task is executed a minimum of  {@link #getMaxReduceAttempts()} attempts before being declared as <i>failed</i>.

 * @param percent the maximum percentage of reduce tasks that can fail without the job being aborted.

 */

public void setMaxReduceTaskFailuresPercent(int percent){

  setInt(JobContext.REDUCE_FAILURES_MAXPERCENT,percent);

}

Location: JobConf.java

Content: 

/** 

 * Set the maximum no. of failures of a given job per tasktracker. If the no. of task failures exceeds <code>noFailures</code>, the  tasktracker is <i>blacklisted</i> for this job. 

 * @param noFailures maximum no. of failures of a given job per tasktracker.

 */

public void setMaxTaskFailuresPerTracker(int noFailures){

  setInt(JobContext.MAX_TASK_FAILURES_PER_TRACKER,noFailures);

}

Location: JobConf.java

Content: 

/** 

 * Set the maximum amount of memory any task of this job can use. See {@link #MAPRED_TASK_MAXVMEM_PROPERTY}<p/> mapred.task.maxvmem is split into mapreduce.map.memory.mb and mapreduce.map.memory.mb,mapred each of the new key are set as mapred.task.maxvmem / 1024 as new values are in MB

 * @param vmem Maximum amount of virtual memory in bytes any task of this jobcan use.

 * @see #getMaxVirtualMemoryForTask()

 * @deprecated Use  {@link #setMemoryForMapTask(long mem)}  andUse  {@link #setMemoryForReduceTask(long mem)}

 */

@Deprecated public void setMaxVirtualMemoryForTask(long vmem){

  LOG.warn("setMaxVirtualMemoryForTask() is deprecated." + "Instead use setMemoryForMapTask() and setMemoryForReduceTask()");

  if (vmem != DISABLED_MEMORY_LIMIT && vmem < 0) {

    setMemoryForMapTask(DISABLED_MEMORY_LIMIT);

    setMemoryForReduceTask(DISABLED_MEMORY_LIMIT);

  }

  if (get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) == null) {

    setMemoryForMapTask(vmem / (1024 * 1024));

    setMemoryForReduceTask(vmem / (1024 * 1024));

  }

 else {

    this.setLong(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY,vmem);

  }

}

Location: JobConf.java

Content: 

public void setMemoryForMapTask(long mem){

  setLong(JobConf.MAPRED_JOB_MAP_MEMORY_MB_PROPERTY,mem);

}

Location: JobConf.java

Content: 

public void setMemoryForReduceTask(long mem){

  setLong(JobConf.MAPRED_JOB_REDUCE_MEMORY_MB_PROPERTY,mem);

}

Location: JobConf.java

Content: 

/** 

 * Set the number of map tasks for this job. <p><i>Note</i>: This is only a <i>hint</i> to the framework. The actual  number of spawned map tasks depends on the number of  {@link InputSplit}s  generated by the job's  {@link InputFormat#getSplits(JobConf,int)}. A custom  {@link InputFormat} is typically used to accurately control the number of map tasks for the job.</p> <h4 id="NoOfMaps">How many maps?</h4> <p>The number of maps is usually driven by the total size of the inputs  i.e. total number of blocks of the input files.</p> <p>The right level of parallelism for maps seems to be around 10-100 maps  per-node, although it has been set up to 300 or so for very cpu-light map  tasks. Task setup takes awhile, so it is best if the maps take at least a  minute to execute.</p> <p>The default behavior of file-based  {@link InputFormat}s is to split the  input into <i>logical</i>  {@link InputSplit}s based on the total size, in  bytes, of input files. However, the  {@link FileSystem} blocksize of the input files is treated as an upper bound for input splits. A lower bound  on the split size can be set via  <a href=" {@docRoot}/../mapred-default.html#mapreduce.input.fileinputformat.split.minsize"> mapreduce.input.fileinputformat.split.minsize</a>.</p> <p>Thus, if you expect 10TB of input data and have a blocksize of 128MB,  you'll end up with 82,000 maps, unless  {@link #setNumMapTasks(int)} is used to set it even higher.</p>

 * @param n the number of map tasks for this job.

 * @see InputFormat#getSplits(JobConf,int)

 * @see FileInputFormat

 * @see FileSystem#getDefaultBlockSize()

 * @see FileStatus#getBlockSize()

 */

public void setNumMapTasks(int n){

  setInt(JobContext.NUM_MAPS,n);

}

Location: JobConf.java

Content: 

/** 

 * Sets the number of tasks that a spawned task JVM should run before it exits

 * @param numTasks the number of tasks to execute; defaults to 1;-1 signifies no limit

 */

public void setNumTasksToExecutePerJvm(int numTasks){

  setInt(JobContext.JVM_NUMTASKS_TORUN,numTasks);

}

Location: JobConf.java

Content: 

/** 

 * Set the  {@link OutputCommitter} implementation for the map-reduce job.

 * @param theClass the {@link OutputCommitter} implementation for the map-reduce job.

 */

public void setOutputCommitter(Class<? extends OutputCommitter> theClass){

  setClass("mapred.output.committer.class",theClass,OutputCommitter.class);

}

Location: JobConf.java

Content: 

/** 

 * Set the  {@link OutputFormat} implementation for the map-reduce job.

 * @param theClass the {@link OutputFormat} implementation for the map-reduce job.

 */

public void setOutputFormat(Class<? extends OutputFormat> theClass){

  setClass("mapred.output.format.class",theClass,OutputFormat.class);

}

Location: JobConf.java

Content: 

/** 

 * Set the  {@link RawComparator} comparator used to compare keys.

 * @param theClass the {@link RawComparator} comparator used to compare keys.

 * @see #setOutputValueGroupingComparator(Class)                 

 */

public void setOutputKeyComparatorClass(Class<? extends RawComparator> theClass){

  setClass(JobContext.KEY_COMPARATOR,theClass,RawComparator.class);

}

Location: JobConf.java

Content: 

/** 

 * Set the user defined  {@link RawComparator} comparator for grouping keys in the input to the reduce. <p>This comparator should be provided if the equivalence rules for keys for sorting the intermediates are different from those for grouping keys before each call to  {@link Reducer#reduce(Object,java.util.Iterator,OutputCollector,Reporter)}.</p> <p>For key-value pairs (K1,V1) and (K2,V2), the values (V1, V2) are passed in a single call to the reduce function if K1 and K2 compare as equal.</p> <p>Since  {@link #setOutputKeyComparatorClass(Class)} can be used to control how keys are sorted, this can be used in conjunction to simulate  <i>secondary sort on values</i>.</p> <p><i>Note</i>: This is not a guarantee of the reduce sort being  <i>stable</i> in any sense. (In any case, with the order of available  map-outputs to the reduce being non-deterministic, it wouldn't make  that much sense.)</p>

 * @param theClass the comparator class to be used for grouping keys. It should implement <code>RawComparator</code>.

 * @see #setOutputKeyComparatorClass(Class)                 

 */

public void setOutputValueGroupingComparator(Class<? extends RawComparator> theClass){

  setClass(JobContext.GROUP_COMPARATOR_CLASS,theClass,RawComparator.class);

}

Location: JobConf.java

Content: 

/** 

 * Set the debug script to run when the reduce tasks fail. <p>The debug script can aid debugging of failed reduce tasks. The script is given task's stdout, stderr, syslog, jobconf files as arguments.</p> <p>The debug command, run on the node where the map failed, is:</p> <p><pre><blockquote>  $script $stdout $stderr $syslog $jobconf. </blockquote></pre></p> <p> The script file is distributed through  {@link DistributedCache} APIs. The script file needs to be symlinked </p> <p>Here is an example on how to submit a script  <p><blockquote><pre> job.setReduceDebugScript("./myscript"); DistributedCache.createSymlink(job); DistributedCache.addCacheFile("/debug/scripts/myscript#myscript"); </pre></blockquote></p>

 * @param rDbgScript the script name

 */

public void setReduceDebugScript(String rDbgScript){

  set(JobContext.REDUCE_DEBUG_SCRIPT,rDbgScript);

}

Location: JobConf.java

Content: 

/** 

 * Set the user-specified session identifier.  

 * @param sessionId the new session id.

 */

@Deprecated public void setSessionId(String sessionId){

  set("session.id",sessionId);

}

Location: JobConf.java

Content: 

/** 

 * Set whether the framework should use the new api for the mapper. This is the default for jobs submitted with the new Job api.

 * @param flag true, if the new api should be used

 */

public void setUseNewMapper(boolean flag){

  setBoolean("mapred.mapper.new-api",flag);

}

Location: JobConf.java

Content: 

/** 

 * Set whether the framework should use the new api for the reducer.  This is the default for jobs submitted with the new Job api.

 * @param flag true, if the new api should be used

 */

public void setUseNewReducer(boolean flag){

  setBoolean("mapred.reducer.new-api",flag);

}

