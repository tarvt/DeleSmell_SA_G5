Location: HadoopArchives.java

Content: 

private void append(SequenceFile.Writer srcWriter,long len,String path,String[] children) throws IOException {

  srcWriter.append(new LongWritable(len),new HarEntry(path,children));

}

Location: HadoopArchives.java

Content: 

/** 

 * archive the given source paths into the dest

 * @param parentPath the parent path of all the source paths

 * @param srcPaths the src paths to be archived

 * @param dest the dest dir that will contain the archive

 */

void archive(Path parentPath,List<Path> srcPaths,String archiveName,Path dest) throws IOException {

  checkPaths(conf,srcPaths);

  int numFiles=0;

  long totalSize=0;

  FileSystem fs=parentPath.getFileSystem(conf);

  this.blockSize=conf.getLong(HAR_BLOCKSIZE_LABEL,blockSize);

  this.partSize=conf.getLong(HAR_PARTSIZE_LABEL,partSize);

  conf.setLong(HAR_BLOCKSIZE_LABEL,blockSize);

  conf.setLong(HAR_PARTSIZE_LABEL,partSize);

  conf.set(DST_HAR_LABEL,archiveName);

  conf.set(SRC_PARENT_LABEL,parentPath.makeQualified(fs).toString());

  Path outputPath=new Path(dest,archiveName);

  FileOutputFormat.setOutputPath(conf,outputPath);

  FileSystem outFs=outputPath.getFileSystem(conf);

  if (outFs.exists(outputPath) || outFs.isFile(dest)) {

    throw new IOException("Invalid Output: " + outputPath);

  }

  conf.set(DST_DIR_LABEL,outputPath.toString());

  Path stagingArea;

  try {

    stagingArea=JobSubmissionFiles.getStagingDir(new Cluster(conf),conf);

  }

 catch (  InterruptedException ie) {

    throw new IOException(ie);

  }

  Path jobDirectory=new Path(stagingArea,NAME + "_" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE),36));

  FsPermission mapredSysPerms=new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);

  FileSystem.mkdirs(jobDirectory.getFileSystem(conf),jobDirectory,mapredSysPerms);

  conf.set(JOB_DIR_LABEL,jobDirectory.toString());

  FileSystem jobfs=jobDirectory.getFileSystem(conf);

  Path srcFiles=new Path(jobDirectory,"_har_src_files");

  conf.set(SRC_LIST_LABEL,srcFiles.toString());

  SequenceFile.Writer srcWriter=SequenceFile.createWriter(jobfs,conf,srcFiles,LongWritable.class,HarEntry.class,SequenceFile.CompressionType.NONE);

  try {

    writeTopLevelDirs(srcWriter,srcPaths,parentPath);

    srcWriter.sync();

    for (    Path src : srcPaths) {

      ArrayList<FileStatusDir> allFiles=new ArrayList<FileStatusDir>();

      FileStatus fstatus=fs.getFileStatus(src);

      FileStatusDir fdir=new FileStatusDir(fstatus,null);

      recursivels(fs,fdir,allFiles);

      for (      FileStatusDir statDir : allFiles) {

        FileStatus stat=statDir.getFileStatus();

        long len=stat.isDirectory() ? 0 : stat.getLen();

        final Path path=relPathToRoot(stat.getPath(),parentPath);

        final String[] children;

        if (stat.isDirectory()) {

          FileStatus[] list=statDir.getChildren();

          children=new String[list.length];

          for (int i=0; i < list.length; i++) {

            children[i]=list[i].getPath().getName();

          }

        }

 else {

          children=null;

        }

        append(srcWriter,len,path.toString(),children);

        srcWriter.sync();

        numFiles++;

        totalSize+=len;

      }

    }

  }

  finally {

    srcWriter.close();

  }

  jobfs.setReplication(srcFiles,(short)10);

  conf.setInt(SRC_COUNT_LABEL,numFiles);

  conf.setLong(TOTAL_SIZE_LABEL,totalSize);

  int numMaps=(int)(totalSize / partSize);

  conf.setNumMapTasks(numMaps == 0 ? 1 : numMaps);

  conf.setNumReduceTasks(1);

  conf.setInputFormat(HArchiveInputFormat.class);

  conf.setOutputFormat(NullOutputFormat.class);

  conf.setMapperClass(HArchivesMapper.class);

  conf.setReducerClass(HArchivesReducer.class);

  conf.setMapOutputKeyClass(IntWritable.class);

  conf.setMapOutputValueClass(Text.class);

  FileInputFormat.addInputPath(conf,jobDirectory);

  conf.setSpeculativeExecution(false);

  JobClient.runJob(conf);

  try {

    jobfs.delete(jobDirectory,true);

  }

 catch (  IOException ie) {

    LOG.info("Unable to clean tmp directory " + jobDirectory);

  }

}

Location: HadoopArchives.java

Content: 

private static void checkPaths(Configuration conf,List<Path> paths) throws IOException {

  for (  Path p : paths) {

    FileSystem fs=p.getFileSystem(conf);

    if (!fs.exists(p)) {

      throw new FileNotFoundException("Source " + p + " does not exist.");

    }

  }

}

Location: HadoopArchives.java

Content: 

private boolean checkValidName(String name){

  Path tmp=new Path(name);

  if (tmp.depth() != 1) {

    return false;

  }

  if (name.endsWith(".har"))   return true;

  return false;

}

Location: HadoopArchives.java

Content: 

public HadoopArchives(Configuration conf){

  setConf(conf);

}

Location: HadoopArchives.java

Content: 

private Path largestDepth(List<Path> paths){

  Path deepest=paths.get(0);

  for (  Path p : paths) {

    if (p.depth() > deepest.depth()) {

      deepest=p;

    }

  }

  return deepest;

}

Location: HadoopArchives.java

Content: 

/** 

 * this assumes that there are two types of files file/dir

 * @param fs the input filesystem

 * @param fdir the filestatusdir of the path  

 * @param out the list of paths output of recursive ls

 * @throws IOException

 */

private void recursivels(FileSystem fs,FileStatusDir fdir,List<FileStatusDir> out) throws IOException {

  if (fdir.getFileStatus().isFile()) {

    out.add(fdir);

    return;

  }

 else {

    out.add(fdir);

    FileStatus[] listStatus=fs.listStatus(fdir.getFileStatus().getPath());

    fdir.setChildren(listStatus);

    for (    FileStatus stat : listStatus) {

      FileStatusDir fstatDir=new FileStatusDir(stat,null);

      recursivels(fs,fstatDir,out);

    }

  }

}

Location: HadoopArchives.java

Content: 

/** 

 * truncate the prefix root from the full path

 * @param fullPath the full path

 * @param root the prefix root to be truncated

 * @return the relative path

 */

private Path relPathToRoot(Path fullPath,Path root){

  final Path justRoot=new Path(Path.SEPARATOR);

  if (fullPath.depth() == root.depth()) {

    return justRoot;

  }

 else   if (fullPath.depth() > root.depth()) {

    Path retPath=new Path(fullPath.getName());

    Path parent=fullPath.getParent();

    for (int i=0; i < (fullPath.depth() - root.depth() - 1); i++) {

      retPath=new Path(parent.getName(),retPath);

      parent=parent.getParent();

    }

    return new Path(justRoot,retPath);

  }

  return null;

}

Location: HadoopArchives.java

Content: 

/** 

 * this method writes all the valid top level directories  into the srcWriter for indexing. This method is a little tricky. example-  for an input with parent path /home/user/ and sources  as /home/user/source/dir1, /home/user/source/dir2 - this  will output <source, dir, dir1, dir2> (dir means that source is a dir with dir1 and dir2 as children) and <source/dir1, file, null> and <source/dir2, file, null>

 * @param srcWriter the sequence file writer to write thedirectories to

 * @param paths the source paths provided by the user. Theyare glob free and have full path (not relative paths)

 * @param parentPath the parent path that you wnat the archivesto be relative to. example - /home/user/dir1 can be archived with parent as /home or /home/user.

 * @throws IOException

 */

private void writeTopLevelDirs(SequenceFile.Writer srcWriter,List<Path> paths,Path parentPath) throws IOException {

  List<Path> justDirs=new ArrayList<Path>();

  for (  Path p : paths) {

    if (!p.getFileSystem(getConf()).isFile(p)) {

      justDirs.add(new Path(p.toUri().getPath()));

    }

 else {

      justDirs.add(new Path(p.getParent().toUri().getPath()));

    }

  }

  TreeMap<String,HashSet<String>> allpaths=new TreeMap<String,HashSet<String>>();

  Path deepest=largestDepth(paths);

  Path root=new Path(Path.SEPARATOR);

  for (int i=parentPath.depth(); i < deepest.depth(); i++) {

    List<Path> parents=new ArrayList<Path>();

    for (    Path p : justDirs) {

      if (p.compareTo(root) == 0) {

      }

 else {

        Path parent=p.getParent();

        if (null != parent) {

          if (allpaths.containsKey(parent.toString())) {

            HashSet<String> children=allpaths.get(parent.toString());

            children.add(p.getName());

          }

 else {

            HashSet<String> children=new HashSet<String>();

            children.add(p.getName());

            allpaths.put(parent.toString(),children);

          }

          parents.add(parent);

        }

      }

    }

    justDirs=parents;

  }

  Set<Map.Entry<String,HashSet<String>>> keyVals=allpaths.entrySet();

  for (  Map.Entry<String,HashSet<String>> entry : keyVals) {

    final Path relPath=relPathToRoot(new Path(entry.getKey()),parentPath);

    if (relPath != null) {

      final String[] children=new String[entry.getValue().size()];

      int i=0;

      for (      String child : entry.getValue()) {

        children[i++]=child;

      }

      append(srcWriter,0L,relPath.toString(),children);

    }

  }

}

