Location: CqlInputFormat.java

Content: 

@Override public org.apache.hadoop.mapreduce.RecordReader<Long,Row> createRecordReader(org.apache.hadoop.mapreduce.InputSplit arg0,TaskAttemptContext arg1) throws IOException, InterruptedException {

  return new CqlRecordReader();

}

Location: CqlInputFormat.java

Content: 

private Map<TokenRange,Long> describeSplits(String keyspace,String table,TokenRange tokenRange,Host host,int splitSize,int splitSizeMb,Session session){

  ResultSet resultSet=queryTableEstimates(session,host,keyspace,table,tokenRange);

  Row row=resultSet.one();

  long meanPartitionSize=0;

  long partitionCount=0;

  int splitCount=0;

  if (row != null) {

    meanPartitionSize=row.getLong("mean_partition_size");

    partitionCount=row.getLong("partitions_count");

    splitCount=splitSizeMb > 0 ? (int)(meanPartitionSize * partitionCount / splitSizeMb / 1024 / 1024) : (int)(partitionCount / splitSize);

  }

  if (splitCount == 0) {

    Map<TokenRange,Long> wrappedTokenRange=new HashMap<>();

    wrappedTokenRange.put(tokenRange,partitionCount == 0 ? 128L : partitionCount);

    return wrappedTokenRange;

  }

  return splitTokenRange(tokenRange,splitCount,partitionCount / splitCount);

}

Location: CqlInputFormat.java

Content: 

private static Map<TokenRange,List<Host>> getRangeMap(String keyspace,Metadata metadata,String targetDC){

  return CqlClientHelper.getLocalPrimaryRangeForDC(keyspace,metadata,targetDC);

}

Location: CqlInputFormat.java

Content: 

public RecordReader<Long,Row> getRecordReader(InputSplit split,JobConf jobConf,final Reporter reporter) throws IOException {

  TaskAttemptContext tac=HadoopCompat.newMapContext(jobConf,TaskAttemptID.forName(jobConf.get(MAPRED_TASK_ID)),null,null,null,new ReporterWrapper(reporter),null);

  CqlRecordReader recordReader=new CqlRecordReader();

  recordReader.initialize((org.apache.hadoop.mapreduce.InputSplit)split,tac);

  return recordReader;

}

Location: CqlInputFormat.java

Content: 

public InputSplit[] getSplits(JobConf jobConf,int numSplits) throws IOException {

  TaskAttemptContext tac=HadoopCompat.newTaskAttemptContext(jobConf,new TaskAttemptID());

  List<org.apache.hadoop.mapreduce.InputSplit> newInputSplits=this.getSplits(tac);

  InputSplit[] oldInputSplits=new InputSplit[newInputSplits.size()];

  for (int i=0; i < newInputSplits.size(); i++)   oldInputSplits[i]=(ColumnFamilySplit)newInputSplits.get(i);

  return oldInputSplits;

}

Location: CqlInputFormat.java

Content: 

public List<org.apache.hadoop.mapreduce.InputSplit> getSplits(JobContext context) throws IOException {

  Configuration conf=HadoopCompat.getConfiguration(context);

  validateConfiguration(conf);

  keyspace=ConfigHelper.getInputKeyspace(conf);

  cfName=ConfigHelper.getInputColumnFamily(conf);

  partitioner=ConfigHelper.getInputPartitioner(conf);

  logger.trace("partitioner is {}",partitioner);

  ExecutorService executor=new ThreadPoolExecutor(0,128,60L,TimeUnit.SECONDS,new LinkedBlockingQueue<Runnable>());

  List<org.apache.hadoop.mapreduce.InputSplit> splits=new ArrayList<>();

  String[] inputInitialAddress=ConfigHelper.getInputInitialAddress(conf).split(",");

  try (Cluster cluster=CqlConfigHelper.getInputCluster(inputInitialAddress,conf);Session session=cluster.connect()){

    List<SplitFuture> splitfutures=new ArrayList<>();

    Pair<String,String> jobKeyRange=ConfigHelper.getInputKeyRange(conf);

    Range<Token> jobRange=null;

    if (jobKeyRange != null) {

      jobRange=new Range<>(partitioner.getTokenFactory().fromString(jobKeyRange.left),partitioner.getTokenFactory().fromString(jobKeyRange.right));

    }

    Metadata metadata=cluster.getMetadata();

    Map<TokenRange,List<Host>> masterRangeNodes=getRangeMap(keyspace,metadata,getTargetDC(metadata,inputInitialAddress));

    for (    TokenRange range : masterRangeNodes.keySet()) {

      if (jobRange == null) {

        for (        TokenRange unwrapped : range.unwrap()) {

          SplitFuture task=new SplitFuture(new SplitCallable(unwrapped,masterRangeNodes.get(range),conf,session));

          executor.submit(task);

          splitfutures.add(task);

        }

      }

 else {

        TokenRange jobTokenRange=rangeToTokenRange(metadata,jobRange);

        if (range.intersects(jobTokenRange)) {

          for (          TokenRange intersection : range.intersectWith(jobTokenRange)) {

            for (            TokenRange unwrapped : intersection.unwrap()) {

              SplitFuture task=new SplitFuture(new SplitCallable(unwrapped,masterRangeNodes.get(range),conf,session));

              executor.submit(task);

              splitfutures.add(task);

            }

          }

        }

      }

    }

    List<SplitFuture> failedTasks=new ArrayList<>();

    int maxSplits=0;

    long expectedPartionsForFailedRanges=0;

    for (    SplitFuture task : splitfutures) {

      try {

        List<ColumnFamilySplit> tokenRangeSplits=task.get();

        if (tokenRangeSplits.size() > maxSplits) {

          maxSplits=tokenRangeSplits.size();

          expectedPartionsForFailedRanges=tokenRangeSplits.get(0).getLength();

        }

        splits.addAll(tokenRangeSplits);

      }

 catch (      Exception e) {

        failedTasks.add(task);

      }

    }

    if (!failedTasks.isEmpty()) {

      if (maxSplits == 0)       throwAllSplitsFailed(failedTasks);

      for (      SplitFuture task : failedTasks) {

        try {

          task.get();

        }

 catch (        Exception cause) {

          logger.warn("Unable to get estimate for {}, the host {} had a exception; falling back to default estimate",task.splitCallable.tokenRange,task.splitCallable.hosts.get(0),cause);

        }

      }

      for (      SplitFuture task : failedTasks)       splits.addAll(toSplit(task.splitCallable.hosts,splitTokenRange(task.splitCallable.tokenRange,maxSplits,expectedPartionsForFailedRanges)));

    }

  }

  finally {

    executor.shutdownNow();

  }

  assert splits.size() > 0;

  Collections.shuffle(splits,new Random(System.nanoTime()));

  return splits;

}

Location: CqlInputFormat.java

Content: 

private Map<TokenRange,Long> getSubSplits(String keyspace,String cfName,TokenRange range,Host host,Configuration conf,Session session){

  int splitSize=ConfigHelper.getInputSplitSize(conf);

  int splitSizeMb=ConfigHelper.getInputSplitSizeInMb(conf);

  return describeSplits(keyspace,cfName,range,host,splitSize,splitSizeMb,session);

}

Location: CqlInputFormat.java

Content: 

private static String getTargetDC(Metadata metadata,String[] inputInitialAddress){

  BiMultiValMap<InetAddress,String> addressToDc=new BiMultiValMap<>();

  Multimap<String,InetAddress> dcToAddresses=addressToDc.inverse();

  Set<InetAddress> addresses=new HashSet<>(inputInitialAddress.length);

  for (  String inputAddress : inputInitialAddress)   addresses.addAll(parseAddress(inputAddress));

  for (  Host host : metadata.getAllHosts()) {

    InetAddress address=host.getBroadcastAddress();

    if (addresses.contains(address))     addressToDc.put(address,host.getDatacenter());

  }

switch (dcToAddresses.keySet().size()) {

case 1:

    return Iterables.getOnlyElement(dcToAddresses.keySet());

case 0:

  throw new IllegalStateException("Input addresses could not be used to find DC; non match client metadata");

default :

for (String inputAddress : inputInitialAddress) {

  for (  InetAddress add : parseAddress(inputAddress)) {

    String dc=addressToDc.get(add);

    if (dc != null)     return dc;

  }

}

throw new AssertionError("Unable to infer datacenter from initial addresses; multiple datacenters found " + dcToAddresses.keySet() + ", should only use addresses from one datacenter");

}

}

Location: CqlInputFormat.java

Content: 

private static List<InetAddress> parseAddress(String str){

  try {

    return Arrays.asList(InetAddress.getAllByName(str));

  }

 catch (  Exception e) {

    throw new RuntimeException(e);

  }

}

Location: CqlInputFormat.java

Content: 

private static ResultSet queryTableEstimates(Session session,Host host,String keyspace,String table,TokenRange tokenRange){

  try {

    String query=String.format("SELECT mean_partition_size, partitions_count " + "FROM %s.%s " + "WHERE keyspace_name = ? AND table_name = ? AND range_type = '%s' AND range_start = ? AND range_end = ?",SchemaConstants.SYSTEM_KEYSPACE_NAME,SystemKeyspace.TABLE_ESTIMATES,SystemKeyspace.TABLE_ESTIMATES_TYPE_LOCAL_PRIMARY);

    Statement stmt=new SimpleStatement(query,keyspace,table,tokenRange.getStart().toString(),tokenRange.getEnd().toString()).setHost(host);

    return session.execute(stmt);

  }

 catch (  InvalidQueryException e) {

    String query=String.format("SELECT mean_partition_size, partitions_count " + "FROM %s.%s " + "WHERE keyspace_name = ? AND table_name = ? AND range_start = ? AND range_end = ?",SchemaConstants.SYSTEM_KEYSPACE_NAME,SystemKeyspace.LEGACY_SIZE_ESTIMATES);

    Statement stmt=new SimpleStatement(query,keyspace,table,tokenRange.getStart().toString(),tokenRange.getEnd().toString()).setHost(host);

    return session.execute(stmt);

  }

}

Location: CqlInputFormat.java

Content: 

private TokenRange rangeToTokenRange(Metadata metadata,Range<Token> range){

  return metadata.newTokenRange(metadata.newToken(partitioner.getTokenFactory().toString(range.left)),metadata.newToken(partitioner.getTokenFactory().toString(range.right)));

}

Location: CqlInputFormat.java

Content: 

private static Map<TokenRange,Long> splitTokenRange(TokenRange tokenRange,int splitCount,long partitionCount){

  List<TokenRange> splitRanges=tokenRange.splitEvenly(splitCount);

  Map<TokenRange,Long> rangesWithLength=Maps.newHashMapWithExpectedSize(splitRanges.size());

  for (  TokenRange range : splitRanges)   rangesWithLength.put(range,partitionCount);

  return rangesWithLength;

}

Location: CqlInputFormat.java

Content: 

private static IllegalStateException throwAllSplitsFailed(List<SplitFuture> failedTasks){

  IllegalStateException exception=new IllegalStateException("No successful tasks found");

  for (  SplitFuture task : failedTasks) {

    try {

      task.get();

    }

 catch (    Exception cause) {

      exception.addSuppressed(cause);

    }

  }

  throw exception;

}

Location: CqlInputFormat.java

Content: 

private List<ColumnFamilySplit> toSplit(List<Host> hosts,Map<TokenRange,Long> subSplits){

  String[] endpoints=new String[hosts.size()];

  int endpointIndex=0;

  for (  Host endpoint : hosts)   endpoints[endpointIndex++]=endpoint.getAddress().getHostName();

  boolean partitionerIsOpp=partitioner instanceof OrderPreservingPartitioner || partitioner instanceof ByteOrderedPartitioner;

  ArrayList<ColumnFamilySplit> splits=new ArrayList<>();

  for (  Map.Entry<TokenRange,Long> subSplitEntry : subSplits.entrySet()) {

    TokenRange subrange=subSplitEntry.getKey();

    ColumnFamilySplit split=new ColumnFamilySplit(partitionerIsOpp ? subrange.getStart().toString().substring(2) : subrange.getStart().toString(),partitionerIsOpp ? subrange.getEnd().toString().substring(2) : subrange.getEnd().toString(),subSplitEntry.getValue(),endpoints);

    logger.trace("adding {}",split);

    splits.add(split);

  }

  return splits;

}

Location: CqlInputFormat.java

Content: 

protected void validateConfiguration(Configuration conf){

  if (ConfigHelper.getInputKeyspace(conf) == null || ConfigHelper.getInputColumnFamily(conf) == null) {

    throw new UnsupportedOperationException("you must set the keyspace and table with setInputColumnFamily()");

  }

  if (ConfigHelper.getInputInitialAddress(conf) == null)   throw new UnsupportedOperationException("You must set the initial output address to a Cassandra node with setInputInitialAddress");

  if (ConfigHelper.getInputPartitioner(conf) == null)   throw new UnsupportedOperationException("You must set the Cassandra partitioner class with setInputPartitioner");

}

