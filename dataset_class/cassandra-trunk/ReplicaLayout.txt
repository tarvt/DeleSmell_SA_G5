Location: ReplicaLayout.java

Content: 

/** 

 * All relevant owners of the ring position(s) for this operation, as implied by the current ring layout. For writes, this will include pending owners, and for reads it will be equivalent to natural()

 */

public E all(){

  return natural;

}

Location: ReplicaLayout.java

Content: 

/** 

 * TODO: we should really double check that the provided range does not overlap multiple token ring regions

 * @return the read layout for a range - this includes only live natural replicas, i.e. those that are not pendingand not marked down by the failure detector. these are reverse sorted by the badness score of the configured snitch

 */

static ReplicaLayout.ForRangeRead forRangeReadLiveSorted(Keyspace keyspace,AbstractBounds<PartitionPosition> range){

  EndpointsForRange replicas=keyspace.getReplicationStrategy().getNaturalReplicas(range.right);

  replicas=DatabaseDescriptor.getEndpointSnitch().sortedByProximity(FBUtilities.getBroadcastAddressAndPort(),replicas);

  replicas=replicas.filter(FailureDetector.isReplicaAlive);

  return new ReplicaLayout.ForRangeRead(range,replicas);

}

Location: ReplicaLayout.java

Content: 

/** 

 * @return the read layout for a token - this includes only live natural replicas, i.e. those that are not pendingand not marked down by the failure detector. these are reverse sorted by the badness score of the configured snitch

 */

static ReplicaLayout.ForTokenRead forTokenReadLiveSorted(Keyspace keyspace,Token token){

  EndpointsForToken replicas=keyspace.getReplicationStrategy().getNaturalReplicasForToken(token);

  replicas=DatabaseDescriptor.getEndpointSnitch().sortedByProximity(FBUtilities.getBroadcastAddressAndPort(),replicas);

  replicas=replicas.filter(FailureDetector.isReplicaAlive);

  return new ReplicaLayout.ForTokenRead(replicas);

}

Location: ReplicaLayout.java

Content: 

/** 

 * Gets the 'natural' and 'pending' replicas that own a given token, with no filtering or processing. Since a write is intended for all nodes (except, unless necessary, transient replicas), this method's only responsibility is to fetch the 'natural' and 'pending' replicas, then resolve any conflicts {@link ReplicaLayout#haveWriteConflicts(Endpoints,Endpoints)}

 */

public static ReplicaLayout.ForTokenWrite forTokenWriteLiveAndDown(Keyspace keyspace,Token token){

  EndpointsForToken natural=keyspace.getReplicationStrategy().getNaturalReplicasForToken(token);

  EndpointsForToken pending=StorageService.instance.getTokenMetadata().pendingEndpointsForToken(token,keyspace.getName());

  return forTokenWrite(natural,pending);

}

Location: ReplicaLayout.java

Content: 

public static ReplicaLayout.ForTokenWrite forTokenWrite(EndpointsForToken natural,EndpointsForToken pending){

  if (haveWriteConflicts(natural,pending)) {

    natural=resolveWriteConflictsInNatural(natural,pending);

    pending=resolveWriteConflictsInPending(natural,pending);

  }

  return new ReplicaLayout.ForTokenWrite(natural,pending);

}

Location: ReplicaLayout.java

Content: 

/** 

 * Detect if we have any endpoint in both pending and full; this can occur either due to races (there is no isolation) or because an endpoint is transitioning between full and transient replication status. We essentially always prefer the full version for writes, because this is stricter. For transient->full transitions: Since we always write to any pending transient replica, effectively upgrading it to full for the transition duration, it might at first seem to be OK to continue treating the conflict replica as its 'natural' transient form, as there is always a quorum of nodes receiving the write.  However, ring ownership changes are not atomic or consistent across the cluster, and it is possible for writers to see different ring states. Furthermore, an operator would expect that the full node has received all writes, with no extra need for repair (as the normal contract dictates) when it completes its transition. While we cannot completely eliminate risks due to ring inconsistencies, this approach is the most conservative available to us today to mitigate, and (we think) the easiest to reason about. For full->transient transitions: In this case, things are dicier, because in theory we can trigger this change instantly.  All we need to do is drop some data, surely? Ring movements can put us in a pickle; any other node could believe us to be full when we have become transient, and perform a full data request to us that we believe ourselves capable of answering, but that we are not. If the ring is inconsistent, it's even feasible that a transient request would be made to the node that is losing its transient status, that also does not know it has yet done so, resulting in all involved nodes being unaware of the data inconsistency. This happens because ring ownership changes are implied by a single node; not all owning nodes get a say in when the transition takes effect.  As such, a node can hold an incorrect belief about its own ownership ranges. This race condition is somewhat inherent in present day Cassandra, and there's actually a limit to what we can do about it. It is a little more dangerous with transient replication, however, because we can completely answer a request without ever touching a digest, meaning we are less likely to attempt to repair any inconsistency. We aren't guaranteed to contact any different nodes for the data requests, of course, though we at least have a chance. Note: If we have any pending transient->full movement, we need to move the full replica to our 'natural' bucket to avoid corrupting our count.  This is fine for writes, all we're doing is ensuring we always write to the node, instead of selectively.

 * @param natural

 * @param pending

 * @param < E >

 * @return

 */

static <E extends Endpoints<E>>boolean haveWriteConflicts(E natural,E pending){

  Set<InetAddressAndPort> naturalEndpoints=natural.endpoints();

  for (  InetAddressAndPort pendingEndpoint : pending.endpoints()) {

    if (naturalEndpoints.contains(pendingEndpoint))     return true;

  }

  return false;

}

Location: ReplicaLayout.java

Content: 

/** 

 * The 'natural' owners of the ring position(s), as implied by the current ring layout. This excludes any pending owners, i.e. those that are in the process of taking ownership of a range, but have not yet finished obtaining their view of the range.

 */

public final E natural(){

  return natural;

}

Location: ReplicaLayout.java

Content: 

ReplicaLayout(E natural){

  this.natural=natural;

}

Location: ReplicaLayout.java

Content: 

/** 

 * MUST APPLY FIRST See  {@link ReplicaLayout#haveWriteConflicts}

 * @return a 'natural' replica collection, that has had its conflicts with pending repaired

 */

@VisibleForTesting static EndpointsForToken resolveWriteConflictsInNatural(EndpointsForToken natural,EndpointsForToken pending){

  EndpointsForToken.Builder resolved=natural.newBuilder(natural.size());

  for (  Replica replica : natural) {

    if (replica.isTransient()) {

      Replica conflict=pending.byEndpoint().get(replica.endpoint());

      if (conflict != null) {

        assert conflict.isFull();

        resolved.add(conflict);

        continue;

      }

    }

    resolved.add(replica);

  }

  return resolved.build();

}

Location: ReplicaLayout.java

Content: 

/** 

 * MUST APPLY SECOND See  {@link ReplicaLayout#haveWriteConflicts}

 * @return a 'pending' replica collection, that has had its conflicts with natural repaired

 */

@VisibleForTesting static EndpointsForToken resolveWriteConflictsInPending(EndpointsForToken natural,EndpointsForToken pending){

  return pending.without(natural.endpoints());

}

